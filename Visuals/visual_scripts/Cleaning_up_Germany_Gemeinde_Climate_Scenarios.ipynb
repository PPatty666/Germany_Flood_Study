{
 "cells": [
  {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](",
        "https://colab.research.google.com/github/PPatty666/Germany_Flood_Study/blob/main/Visuals/visual_scripts/Cleaning_up_Germany_Gemeinde_Climate_Scenarios.ipynb",
        ")"
    ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "srppXBKbvU2m"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install fiona\n",
    "!pip install folium mapclassify\n",
    "!pip install ipyleaflet\n",
    "!pip install -U kaleido\n",
    "!pip install contextily\n",
    "!pip install git+https://github.com/pmdscully/geo_northarrow.git\n",
    "!pip install matplotlib_scalebar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7vQ9WiNQxEpQ",
    "outputId": "76ee32ea-35fb-4bf9-9d9f-c31e3e7aa42d"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-WTkziPtBTOF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib.legend import Legend\n",
    "from matplotlib.colors import to_hex\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "from PIL import Image\n",
    "from IPython.display import Video\n",
    "\n",
    "from ipyleaflet import Map, GeoJSON, Popup\n",
    "from ipywidgets import HTML\n",
    "import json\n",
    "import folium\n",
    "from folium import IFrame\n",
    "\n",
    "import re\n",
    "import math\n",
    "from numpy.random import default_rng\n",
    "import scipy\n",
    "from scipy.stats import gamma\n",
    "import statsmodels.api as sm\n",
    "import arviz as az\n",
    "import pymc as pm\n",
    "import xarray as xr\n",
    "\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "from shapely.geometry import MultiLineString, LineString\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "import contextily as ctx\n",
    "from geo_northarrow import add_north_arrow\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import mapclassify\n",
    "from mapclassify import NaturalBreaks\n",
    "from mapclassify import Quantiles\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_nOA6O2-ZZP"
   },
   "source": [
    "## Find the flooding exposure scenario for the year 2020 (based on a 100-year return period) to use as the baseline for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QKU_ARsgV1p8",
    "outputId": "7012c4e7-a364-412a-ef29-6a95b088cdd9"
   },
   "outputs": [],
   "source": [
    "!unzip \"/content/drive/MyDrive/Germany_Flood_Study/Gemeinde_all_return_periods.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5e9iGJrh_LK"
   },
   "source": [
    "# SSP1-RCP2.6 (Most Optimistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2PYbfOmZWlF"
   },
   "source": [
    "### Read files, pre-processing and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BT9XT45Fh_LK",
    "outputId": "2ded3345-2485-4b40-e82b-fdb64cd4b481"
   },
   "outputs": [],
   "source": [
    "!unzip \"/content/most_optimistic.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hT9s5kah_LL"
   },
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    '/content/Germany_Gemeinde_2020_100.xlsx',\n",
    "    '/content/most_optimistic/Germany_Gemeinde_2030_100_26.xlsx',\n",
    "    '/content/most_optimistic/Germany_Gemeinde_2050_100_26.xlsx',\n",
    "    '/content/most_optimistic/Germany_Gemeinde_2080_100_26.xlsx',\n",
    "]\n",
    "\n",
    "dataframes = []\n",
    "for path in file_paths:\n",
    "    df = pd.read_excel(path)\n",
    "    df = df.round(6)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Optionally unpack into variables\n",
    "(Gemeinde_2020_100_raw, Gemeinde_2030_100_26_raw, Gemeinde_2050_100_26_raw, Gemeinde_2080_100_26_raw) = dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "id": "HpcYMzCj_vW7",
    "outputId": "ad804bea-9f08-4bf8-ebd5-f986eb9cbac5"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "id": "ifzQu6euh_LL",
    "outputId": "96e19abb-4713-4696-93a8-81b8e5eab466"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2030_100_26_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tjBF0H4Oh_LL"
   },
   "outputs": [],
   "source": [
    "# Date range: every 6 months\n",
    "dates = pd.date_range(start='2016-07-01', end='2025-01-01', freq='6MS')\n",
    "\n",
    "# Prefixes and suffixes\n",
    "prefixes = ['CU', 'FU', 'PD', 'MAX']\n",
    "suffixes = ['P0', 'lt0.15', 'lt0.5', 'lt1.5', 'gt1.5']\n",
    "\n",
    "# Correct order: by prefix, then suffix, then date\n",
    "all_columns = [\n",
    "    f\"{prefix}-{date.strftime('%b-%y')}-{suffix}\"\n",
    "    for prefix in prefixes\n",
    "    for suffix in suffixes\n",
    "    for date in dates\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Njs_MkM9h_LL"
   },
   "outputs": [],
   "source": [
    "# Check if number of replacement columns matches the shape\n",
    "for df in [Gemeinde_2020_100_raw, Gemeinde_2030_100_26_raw, Gemeinde_2050_100_26_raw, Gemeinde_2080_100_26_raw]:\n",
    "    if len(all_columns) == df.shape[1] - 3:\n",
    "      df.columns = list(df.columns[:3]) + all_columns\n",
    "      df.reset_index(drop=True, inplace=True)\n",
    "      df['id'] = df.index\n",
    "    else:\n",
    "      raise ValueError(\"Length of generated column names does not match number of columns (excluding the first one).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "id": "v3n-wAYAQXtf",
    "outputId": "99ec013e-7929-4ad5-c706-9a51425b8fd6"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2030_100_26_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMLSl9evl6wa"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_raw = Gemeinde_2020_100_raw[1:]\n",
    "Gemeinde_2030_100_26_raw = Gemeinde_2030_100_26_raw[1:]\n",
    "Gemeinde_2050_100_26_raw = Gemeinde_2050_100_26_raw[1:]\n",
    "Gemeinde_2080_100_26_raw = Gemeinde_2080_100_26_raw[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_hte2e1h_LL"
   },
   "outputs": [],
   "source": [
    "# Pre-processed files should be stored in pickle format to enable direct reloading, particularly in cases where the session must be restarted due to memory limitations.\n",
    "save_path = \"/content/\"\n",
    "\n",
    "# Make sure the directory exists\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "gemeinde_dfs = [Gemeinde_2020_100_raw, Gemeinde_2030_100_26_raw, Gemeinde_2050_100_26_raw, Gemeinde_2080_100_26_raw]\n",
    "\n",
    "filenames = [\n",
    "    \"Gemeinde_2020_100_raw.pkl\", \"Gemeinde_2030_100_26_raw.pkl\", \"Gemeinde_2050_100_26_raw.pkl\", \"Gemeinde_2080_100_26_raw.pkl\"\n",
    "]\n",
    "\n",
    "for df, filename in zip(gemeinde_dfs, filenames):\n",
    "    df.to_pickle(os.path.join(save_path, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYcX5X63Y7Wg"
   },
   "source": [
    "### **PLEASE re-run if the session restarts before you finish processing this climate scenario:** Read and load file from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZ_iM_7bh_LL"
   },
   "outputs": [],
   "source": [
    "# Define the path where the files are stored\n",
    "save_path = \"/content/\"\n",
    "\n",
    "# List of filenames\n",
    "filenames = [\n",
    "    \"Gemeinde_2020_100_raw.pkl\", \"Gemeinde_2030_100_26_raw.pkl\", \"Gemeinde_2050_100_26_raw.pkl\", \"Gemeinde_2080_100_26_raw.pkl\"\n",
    "]\n",
    "\n",
    "# Load all DataFrames into a list\n",
    "gemeinde_dfs = [pd.read_pickle(os.path.join(save_path, filename)) for filename in filenames]\n",
    "\n",
    "cols_to_convert = [col for col in gemeinde_dfs[0].columns if col not in ['Gemeinde', 'Kreis', 'Land', 'id']]\n",
    "\n",
    "def convert_and_round(df):\n",
    "    df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, errors='coerce').round(6)\n",
    "    return df\n",
    "\n",
    "gemeinde_dfs = [convert_and_round(df) for df in gemeinde_dfs]\n",
    "\n",
    "# Unpack if needed\n",
    "(\n",
    "  Gemeinde_2020_100_raw,\n",
    "  Gemeinde_2030_100_26_raw,\n",
    "  Gemeinde_2050_100_26_raw,\n",
    "  Gemeinde_2080_100_26_raw\n",
    ") = gemeinde_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61VkcVgffJKa"
   },
   "outputs": [],
   "source": [
    "# Date range: every 6 months\n",
    "dates = pd.date_range(start='2016-07-01', end='2025-01-01', freq='6MS')\n",
    "\n",
    "# Prefixes and suffixes\n",
    "prefixes = ['CU', 'FU', 'PD', 'MAX']\n",
    "suffixes = ['P0', 'lt0.15', 'lt0.5', 'lt1.5', 'gt1.5']\n",
    "\n",
    "# Correct order: by prefix, then suffix, then date\n",
    "all_columns = [\n",
    "    f\"{prefix}-{date.strftime('%b-%y')}-{suffix}\"\n",
    "    for prefix in prefixes\n",
    "    for suffix in suffixes\n",
    "    for date in dates\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0pTC-M4h_LL"
   },
   "source": [
    "## Combined flooding (max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOxKGiWjh_LL"
   },
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOCPmpdNh_LL"
   },
   "outputs": [],
   "source": [
    "# List of years to process\n",
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "# Columns you want to select (CU columns and Land)\n",
    "max_columns = ['id', 'Gemeinde', 'Kreis', 'Land'] + [col for col in all_columns if col.startswith('MAX') or col == 'Land']\n",
    "\n",
    "# Loop through each year and modify the corresponding DataFrame\n",
    "for year in years:\n",
    "    if year == 2020:\n",
    "        df_name = f'Gemeinde_{year}_100_raw'  # 2020 uses '_100_raw'\n",
    "    else:\n",
    "        df_name = f'Gemeinde_{year}_100_26_raw'  # 2030, 2050, and 2080 use '_100_85_raw'\n",
    "\n",
    "    # Check if the DataFrame exists before proceeding\n",
    "    if df_name in globals():\n",
    "        # Get the raw DataFrame and select the relevant columns for max flooding data\n",
    "        df_raw = globals()[df_name]\n",
    "        globals()[f'Gemeinde_{year}_100_26_MAX'] = df_raw[max_columns]\n",
    "    else:\n",
    "        print(f'{df_name} does not exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QSLtfVbNh_LL",
    "outputId": "612af375-1786-402c-fa11-f55e096e2b78"
   },
   "outputs": [],
   "source": [
    "len(Gemeinde_2020_100_26_MAX) == len(Gemeinde_2030_100_26_MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KB8viTDth_LL"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    globals()[f'Gemeinde_{y}_100_26_MAX_long'] = globals()[f'Gemeinde_{y}_100_26_MAX'].melt(\n",
    "        id_vars=['id', 'Gemeinde', 'Kreis', 'Land'], var_name='month_year_depth', value_name='MAX_area'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wGokkysMh_LM"
   },
   "outputs": [],
   "source": [
    "for df in [Gemeinde_2020_100_26_MAX_long, Gemeinde_2030_100_26_MAX_long, Gemeinde_2050_100_26_MAX_long, Gemeinde_2080_100_26_MAX_long]:\n",
    "\n",
    "    # Replace 'CU' with 'MAX' if needed (optional step if not already 'MAX')\n",
    "    # df['month_year_depth'] = df['month_year_depth'].str.replace('CU', 'MAX')\n",
    "\n",
    "    # Split the 'month_year_depth' column into parts\n",
    "    df[['prefix', 'month_year', 'depth_cat']] = df['month_year_depth'].str.extract(r'^(MAX)-([A-Za-z]{3}-\\d{2})-(.+)$')\n",
    "\n",
    "    # Drop 'prefix' if not needed\n",
    "    df.drop(columns='prefix', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_vdUUpjh_LM"
   },
   "outputs": [],
   "source": [
    "# Define your DataFrames with their corresponding years\n",
    "year_df_map = {\n",
    "    2020: Gemeinde_2020_100_26_MAX_long,\n",
    "    2030: Gemeinde_2030_100_26_MAX_long,\n",
    "    2050: Gemeinde_2050_100_26_MAX_long,\n",
    "    2080: Gemeinde_2080_100_26_MAX_long\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame\n",
    "for year, df in year_df_map.items():\n",
    "\n",
    "    # Filter to only Jan-25\n",
    "    jan25_df = df[df['month_year'] == 'Jan-25'].reset_index(drop=True)\n",
    "\n",
    "    # Set the new global variable\n",
    "    globals()[f'Gemeinde_{year}_100_26_MAX_Jan25'] = jan25_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmqkTRD2h_LM"
   },
   "outputs": [],
   "source": [
    "# Second part: group and create *_settle DataFrames\n",
    "for year in [2020, 2030, 2050, 2080]:\n",
    "    jan25_var_name = f'Gemeinde_{year}_100_26_MAX_Jan25'\n",
    "\n",
    "    if jan25_var_name in globals():\n",
    "        jan25_df = globals()[jan25_var_name]\n",
    "\n",
    "        # Group and sum by 'id' and 'month_year'\n",
    "        settle_df = jan25_df.groupby(['id', 'month_year'], as_index=False)['MAX_area'].sum()\n",
    "        settle_df.columns = ['id', 'month_year', 'settle_area']\n",
    "        settle_df['settle_area'] = settle_df['settle_area'].astype(float)\n",
    "\n",
    "        # Save to a new global variable\n",
    "        globals()[f'{jan25_var_name}_settle'] = settle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQ6zy8YWh_LM"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    long_df = globals()[f'Gemeinde_{y}_100_26_MAX_Jan25']\n",
    "    settle_df = globals()[f'Gemeinde_{y}_100_26_MAX_Jan25_settle']\n",
    "\n",
    "    merged_df = pd.merge(long_df, settle_df, on=['id', 'month_year'], how='left')\n",
    "\n",
    "    globals()[f'Gemeinde_{y}_100_26_MAX_Jan25'] = merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TxXbZpsakWj5",
    "outputId": "a5f8e5e0-0781-4d65-8c29-849ac34843aa"
   },
   "outputs": [],
   "source": [
    "# Filter out rows where Gemeinde == 'Germany'\n",
    "filtered = Gemeinde_2020_100_26_MAX_Jan25[Gemeinde_2020_100_26_MAX_Jan25.Gemeinde != 'Germany']\n",
    "filtered.settle_area = filtered.settle_area.round(3)\n",
    "\n",
    "# Group and count\n",
    "grouped = filtered.groupby(['Gemeinde', 'Kreis', 'Land', 'settle_area']).size().reset_index(name='count')\n",
    "\n",
    "# Check for duplicates (count > 1)\n",
    "duplicates = grouped[grouped['count'] > 5]\n",
    "\n",
    "print(f\"Number of duplicate combinations: {len(duplicates)}\")\n",
    "print(duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2uJwOYhtnRqy",
    "outputId": "83ef9f80-fb3b-47cf-8cf4-f8abf48d7940"
   },
   "outputs": [],
   "source": [
    "duplicate_rows = filtered[\n",
    "    (filtered['Gemeinde'] == 'Helgoland') &\n",
    "    (filtered['Kreis'] == 'Pinneberg') &\n",
    "    (filtered['Land'] == 'Schleswig-Holstein')\n",
    "]\n",
    "\n",
    "print(duplicate_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5H-Hq1IeFM3v"
   },
   "outputs": [],
   "source": [
    "# Add suffix based on id\n",
    "Gemeinde_2020_100_26_MAX_Jan25.loc[\n",
    "    Gemeinde_2020_100_26_MAX_Jan25['id'] == 9799, 'Gemeinde'\n",
    "] = Gemeinde_2020_100_26_MAX_Jan25.loc[\n",
    "    Gemeinde_2020_100_26_MAX_Jan25['id'] == 9799, 'Gemeinde'\n",
    "] + '-1'\n",
    "\n",
    "Gemeinde_2020_100_26_MAX_Jan25.loc[\n",
    "    Gemeinde_2020_100_26_MAX_Jan25['id'] == 9800, 'Gemeinde'\n",
    "] = Gemeinde_2020_100_26_MAX_Jan25.loc[\n",
    "    Gemeinde_2020_100_26_MAX_Jan25['id'] == 9800, 'Gemeinde'\n",
    "] + '-2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "VzBSvPdGFgAw",
    "outputId": "4f9070ea-783b-4873-91a2-2f82ff92ba17"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_26_MAX_Jan25[\n",
    "    (Gemeinde_2020_100_26_MAX_Jan25['Gemeinde'] == 'Helgoland-2') &\n",
    "    (Gemeinde_2020_100_26_MAX_Jan25['Kreis'] == 'Pinneberg') &\n",
    "    (Gemeinde_2020_100_26_MAX_Jan25['Land'] == 'Schleswig-Holstein')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BGGkcn0gitE"
   },
   "source": [
    "### Check admin boundary matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVgsnZYxYOyz"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!unzip /content/drive/MyDrive/Germany_Flood_Study/vg250_01-01.gk3.shape.ebenen.zip -d admin-GIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQ7wmpzyYfLE"
   },
   "outputs": [],
   "source": [
    "admin3_boundary = gpd.read_file('/content/admin-GIS/vg250_01-01.gk3.shape.ebenen/vg250_ebenen_0101/VG250_GEM.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QWqa_oD6Xy4Q",
    "outputId": "c0540935-3b0d-4ad7-8b7d-5f04c7e9c46d"
   },
   "outputs": [],
   "source": [
    "!unzip \"/content/drive/MyDrive/Germany_Flood_Study/Gemeinde_Stats_all_return_periods.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29vCKB7IYNMo"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_raw_exposure = pd.read_csv('/content/GermanyStats_2020_100.csv', encoding='ISO-8859-1').round(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "s_xlmgsVZIHb",
    "outputId": "d3d42e3a-c2ec-4e9f-fc5f-7af411dcf6b2"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_raw_exposure.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M001PO0CdAe9",
    "outputId": "c3ff1fbb-4e2d-4f7a-8b4f-609e202c52fb"
   },
   "outputs": [],
   "source": [
    "# Create the list of new month_year labels (6-month intervals)\n",
    "month_years = pd.date_range('2016-07-01', '2025-01-01', freq='6MS').strftime('%b-%y').tolist()\n",
    "print(month_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPYbgO4bfoLg"
   },
   "outputs": [],
   "source": [
    "# Get current column names\n",
    "cols = Gemeinde_2020_100_raw_exposure.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKFUKxQYfwox"
   },
   "outputs": [],
   "source": [
    "# Update column names only if they end with _01 to _018\n",
    "new_cols = []\n",
    "for col in cols:\n",
    "    match = re.search(r'_(\\d{2})$', col)\n",
    "    if match:\n",
    "        suffix_num = int(match.group(1))\n",
    "        if 1 <= suffix_num <= len(month_years):\n",
    "            new_suffix = month_years[suffix_num - 1]  # map _01 to Jul-16, etc.\n",
    "            col = re.sub(r'_(\\d{2})$', f'_{new_suffix}', col)\n",
    "    new_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVYY1Xu3f9gq"
   },
   "outputs": [],
   "source": [
    "# Assign new column names back to the DataFrame\n",
    "Gemeinde_2020_100_raw_exposure.columns = new_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CU_AriicnP7A"
   },
   "source": [
    "### Settlement area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZafPAZGm4Y9q",
    "outputId": "0bfd0323-09aa-4ace-db2c-c655ed90db66"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_raw_exposure.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTfv-IFd460D"
   },
   "outputs": [],
   "source": [
    "# Select columns that start with 'CU' or are exactly 'Land'\n",
    "max_columns = new_cols[:7] + [col for col in new_cols if col.startswith('SA_maxRisk')]\n",
    "\n",
    "\n",
    "Gemeinde_2020_100_MAX_exposure = Gemeinde_2020_100_raw_exposure[max_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LixZ45C25EIe"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_MAX_exposure_long = Gemeinde_2020_100_MAX_exposure.melt(\n",
    "        id_vars=new_cols[:7], var_name='month_year_depth', value_name='MAX_area'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m75IUvGC5QkS"
   },
   "outputs": [],
   "source": [
    "# Extract depth category (e.g., '0', '015', '050', '150', '150p') and month-year (e.g., 'Jul-16')\n",
    "Gemeinde_2020_100_MAX_exposure_long[['depth_cat', 'month_year']] = (\n",
    "    Gemeinde_2020_100_MAX_exposure_long['month_year_depth']\n",
    "    .str.extract(r'SA_maxRisk_([^_]+)_([A-Za-z]{3}-\\d{2})')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MfcHsSwq5_1Z",
    "outputId": "3dfd63ae-6eaf-416f-ea0b-5a05625693af"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_MAX_exposure_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdN86eZL5rL4"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_MAX_exposure_jan25 = Gemeinde_2020_100_MAX_exposure_long[Gemeinde_2020_100_MAX_exposure_long['month_year'] == 'Jan-25'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y6ayWMnO59R7",
    "outputId": "79f20dbc-ba84-4fb7-c478-dfd696a02ee7"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_MAX_exposure_jan25.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMYCpLPa5v-V"
   },
   "outputs": [],
   "source": [
    "GEM_ID_settle_df = Gemeinde_2020_100_MAX_exposure_jan25.groupby(['GEM_ID', 'month_year'], as_index=False)['MAX_area'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5JTVyfn6tRU"
   },
   "outputs": [],
   "source": [
    "GEM_ID_settle_df.columns = ['GEM_ID', 'month_year', 'settle_area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJ50ccNYna3w"
   },
   "outputs": [],
   "source": [
    "# Select columns that start with 'CU' or are exactly 'Land'\n",
    "sa_columns = new_cols[:25]\n",
    "\n",
    "Gemeinde_2020_100_SA = Gemeinde_2020_100_raw_exposure[sa_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WnZu6_bGKw6t",
    "outputId": "ba5e87e4-8e4a-43df-9911-f6788fb9261b"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_SA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAjdFFHsnrwg"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_SA_jan25 = Gemeinde_2020_100_SA[[\"GEM_ID\", \"GEM_NAME\",\t\"KRS_ID\",\t\"KRS_NAME\",\t\"LAN_ID\",\t\"LAN_NAME\", \"SA_Jan-25\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A1vB2b5XDIUD",
    "outputId": "5c6f4db3-d69f-4aab-90b9-ff7476fe42da"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_SA_jan25.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PdGSRQ4Q7DAz"
   },
   "outputs": [],
   "source": [
    "GEM_ID_settle_df = GEM_ID_settle_df.merge(Gemeinde_2020_100_SA_jan25, on='GEM_ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AMNOgk2y_Umt",
    "outputId": "803b956d-5b1f-4e9b-f263-b6b7325251d2"
   },
   "outputs": [],
   "source": [
    "# Filter out rows where Gemeinde == 'Germany'\n",
    "filtered = GEM_ID_settle_df[GEM_ID_settle_df.GEM_NAME != 'Germany']\n",
    "filtered[\"settle_area\"] = filtered[\"settle_area\"].round(6)\n",
    "\n",
    "# Group and count\n",
    "grouped = filtered.groupby(['GEM_NAME', 'KRS_NAME', 'LAN_NAME', 'settle_area']).size().reset_index(name='count')\n",
    "\n",
    "# Check for duplicates (count > 1)\n",
    "duplicates = grouped[grouped['count'] > 1]\n",
    "\n",
    "print(f\"Number of duplicate combinations: {len(duplicates)}\")\n",
    "print(duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EOa2C73KD2sD",
    "outputId": "d8814de1-66ab-448c-b17f-6247aa42cb22"
   },
   "outputs": [],
   "source": [
    "GEM_ID_settle_df[\n",
    "    (GEM_ID_settle_df['GEM_NAME'] == 'Helgoland') &\n",
    "    (GEM_ID_settle_df['KRS_NAME'] == 'Pinneberg') &\n",
    "    (GEM_ID_settle_df['LAN_NAME'] == 'Schleswig-Holstein')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ewd7iWt7X-H"
   },
   "outputs": [],
   "source": [
    "# Add suffix based on id\n",
    "GEM_ID_settle_df.loc[\n",
    "    GEM_ID_settle_df['GEM_ID'] == 'DEBKGVG2000000C3', 'GEM_NAME'\n",
    "] = GEM_ID_settle_df.loc[\n",
    "    GEM_ID_settle_df['GEM_ID'] == 'DEBKGVG2000000C3', 'GEM_NAME'\n",
    "] + '-1'\n",
    "\n",
    "GEM_ID_settle_df.loc[\n",
    "    GEM_ID_settle_df['GEM_ID'] == 'DEBKGVG2000008HV', 'GEM_NAME'\n",
    "] = GEM_ID_settle_df.loc[\n",
    "    GEM_ID_settle_df['GEM_ID'] == 'DEBKGVG2000008HV', 'GEM_NAME'\n",
    "] + '-2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FWkNPEEH_Mz6",
    "outputId": "63a80e83-d0a7-45a6-c1e3-0bec344a4d18"
   },
   "outputs": [],
   "source": [
    "GEM_ID_settle_df[\n",
    "    (GEM_ID_settle_df['GEM_NAME'] == 'Helgoland') &\n",
    "    (GEM_ID_settle_df['KRS_NAME'] == 'Pinneberg') &\n",
    "    (GEM_ID_settle_df['LAN_NAME'] == 'Schleswig-Holstein')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u1T1b6wO3Zin",
    "outputId": "6550a5cb-e9f9-4f9f-de41-13aca6ca142e"
   },
   "outputs": [],
   "source": [
    "GEM_ID_settle_df.GEM_ID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_FrBfpJXLHC2",
    "outputId": "aded80ed-ee72-4e6c-8034-4c34f61407c9"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_26_MAX_Jan25.id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B6u-lO-bMmnX",
    "outputId": "38cd9c21-9ed6-42dd-b0a6-e9bdc122c469"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_26_MAX_Jan25.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ao_1CVoLGSVr"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_26_MAX_Jan25[\"settle_area_round\"] = Gemeinde_2020_100_26_MAX_Jan25.settle_area.round(6)\n",
    "GEM_ID_settle_df[\"settle_area_round\"] = GEM_ID_settle_df.settle_area.round(6)\n",
    "merged_df = pd.merge(\n",
    "    Gemeinde_2020_100_26_MAX_Jan25,\n",
    "    GEM_ID_settle_df,\n",
    "    left_on=['Gemeinde', 'Kreis', 'Land', 'settle_area_round'],\n",
    "    right_on=['GEM_NAME', 'KRS_NAME', 'LAN_NAME', 'settle_area_round'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p4YMya-NHFw2",
    "outputId": "a2417e43-1211-4764-8e4a-316cb51ada6b"
   },
   "outputs": [],
   "source": [
    "merged_df.GEM_ID.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyDSIs3cGNqh"
   },
   "source": [
    "### Continue climate scenario analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jIWo1OCsy_zy"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_26_MAX_Jan25 = merged_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5BnjR1Bh_LM"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_26_MAX_Jan25']\n",
    "    base_df = Gemeinde_2020_100_26_MAX_Jan25\n",
    "    base_df[f'diff_{year}'] = current_df.MAX_area - base_df.MAX_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IQazAsV1_7pa"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_26_MAX_Jan25']\n",
    "    base_df = Gemeinde_2020_100_26_MAX_Jan25\n",
    "    base_df[f'pct_diff_{year}'] = (current_df.MAX_area - base_df.MAX_area)/(base_df.MAX_area + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o78IBkPah_LM"
   },
   "outputs": [],
   "source": [
    "summary = Gemeinde_2020_100_26_MAX_Jan25[['id', 'GEM_ID', 'Gemeinde', 'Kreis', 'Land']].drop_duplicates().copy()\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    diff_col = f'diff_{year}'\n",
    "\n",
    "    pivot = (\n",
    "        Gemeinde_2020_100_26_MAX_Jan25\n",
    "        .groupby(['id', 'depth_cat'])[diff_col]\n",
    "        .sum()\n",
    "        .unstack(fill_value=0)\n",
    "        .add_prefix(f'sum_diff_{year}_')  # e.g., sum_diff_2030_lt0.5\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    summary = summary.merge(pivot, on='id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJdIPsg4h_LM"
   },
   "outputs": [],
   "source": [
    "# Convert all newly added columns to float\n",
    "new_cols = [col for col in summary.columns if col.startswith('sum_diff_')]\n",
    "summary[new_cols] = summary[new_cols].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-E58AT8Ch_LM"
   },
   "outputs": [],
   "source": [
    "summary_26 = summary.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HnEgo9MZENvV"
   },
   "outputs": [],
   "source": [
    "# Step 1: Calculate total MAX_area per id and depth_cat\n",
    "area_pivot = (\n",
    "    Gemeinde_2020_100_26_MAX_Jan25\n",
    "    .groupby(['id', 'depth_cat'])['MAX_area']\n",
    "    .sum()\n",
    "    .unstack(fill_value=0)\n",
    "    .add_prefix('area_')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Step 2: Merge into summary_26\n",
    "summary_26 = summary_26.merge(area_pivot, on='id', how='left')\n",
    "\n",
    "# Step 3: Calculate percentage risk levels using area for each category\n",
    "years = [2030, 2050, 2080]\n",
    "for year in years:\n",
    "    # Absolute difference sums (already calculated earlier)\n",
    "    summary_26[f'high_risk_{year}'] = (\n",
    "        summary_26.get(f'sum_diff_{year}_lt0.5', 0) +\n",
    "        summary_26.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_26.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_26[f'very_high_risk_{year}'] = (\n",
    "        summary_26.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_26.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_26[f'extreme_risk_{year}'] = summary_26.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "\n",
    "    # Denominators from MAX_area sums\n",
    "    total_area = (\n",
    "        summary_26.get('area_lt0.5', 0) +\n",
    "        summary_26.get('area_lt1.5', 0) +\n",
    "        summary_26.get('area_gt1.5', 0)\n",
    "    ).replace(0, 1e-6)\n",
    "\n",
    "    very_high_area = (\n",
    "        summary_26.get('area_lt1.5', 0) +\n",
    "        summary_26.get('area_gt1.5', 0)\n",
    "    ).replace(0, 1e-6)\n",
    "\n",
    "    extreme_area = summary_26.get('area_gt1.5', 0).replace(0, 1e-6)\n",
    "\n",
    "    # Percentage risk levels\n",
    "    summary_26[f'pct_high_risk_{year}'] = (\n",
    "        summary_26[f'high_risk_{year}'] / total_area * 100\n",
    "    ).round(2)\n",
    "\n",
    "    summary_26[f'pct_very_high_risk_{year}'] = (\n",
    "        summary_26[f'very_high_risk_{year}'] / very_high_area * 100\n",
    "    ).round(2)\n",
    "\n",
    "    summary_26[f'pct_extreme_risk_{year}'] = (\n",
    "        summary_26[f'extreme_risk_{year}'] / extreme_area * 100\n",
    "    ).round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BMBJubBOh_LM",
    "outputId": "710c0937-b576-4e11-c651-1f885a46d086"
   },
   "outputs": [],
   "source": [
    "summary_26.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SsXHqriGFgx2"
   },
   "outputs": [],
   "source": [
    "value_vars = [\n",
    "    'high_risk_2030', 'very_high_risk_2030', 'extreme_risk_2030',\n",
    "    'high_risk_2050', 'very_high_risk_2050', 'extreme_risk_2050',\n",
    "    'high_risk_2080', 'very_high_risk_2080', 'extreme_risk_2080'\n",
    "]\n",
    "\n",
    "# 1. Melt the absolute values\n",
    "summary_diff_long = summary_26.melt(\n",
    "    id_vars=['id', 'GEM_ID', 'Gemeinde', 'Kreis', 'Land'],\n",
    "    value_vars=value_vars,\n",
    "    var_name='risk_year',\n",
    "    value_name='max_diff_2020'\n",
    ")\n",
    "\n",
    "# 2. Melt the percent difference columns\n",
    "summary_pct_long = summary_26.melt(\n",
    "    id_vars=['id', 'GEM_ID', 'Gemeinde', 'Kreis', 'Land'],\n",
    "    value_vars=[f'pct_{v}' for v in value_vars],\n",
    "    var_name='risk_year',\n",
    "    value_name='max_pct_diff_2020'\n",
    ")\n",
    "\n",
    "# 3. Clean `risk_year` for both (they must match for merge)\n",
    "summary_pct_long['risk_year'] = summary_pct_long['risk_year'].str.replace('pct_', '')\n",
    "\n",
    "# 4. Merge the two melted DataFrames\n",
    "summary_26_long = pd.merge(\n",
    "    summary_diff_long,\n",
    "    summary_pct_long,\n",
    "    on=['id', 'GEM_ID', 'Gemeinde', 'Kreis', 'Land', 'risk_year']\n",
    ")\n",
    "\n",
    "# 5. Extract risk level and year\n",
    "summary_26_long[['risk_level', 'year']] = summary_26_long['risk_year'].str.extract(r'(.*)_(\\d{4})')\n",
    "summary_26_long['year'] = summary_26_long['year'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXIRYIyThIRh"
   },
   "source": [
    "### Please store the pre-processed files as pickle files, since you’ll need to load them directly later—especially if the session needs to be restarted due to running out of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5EN5K2W5hE14"
   },
   "outputs": [],
   "source": [
    "summary_26_long.to_pickle(\"max_summary_26_long.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8RTEBJA37Lp0"
   },
   "outputs": [],
   "source": [
    "with open(\"max_summary_26_long.pkl\", \"rb\") as f:\n",
    "    summary_26_long = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTdHi3_UzmP4"
   },
   "source": [
    "### Flooding exposure difference map (future years vs. 2020) - prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TYQgKpHfY2Tz",
    "outputId": "3f8cfb22-ee9b-4c0e-c07f-cd1d2bca1e9a"
   },
   "outputs": [],
   "source": [
    "summary_26_long.risk_level.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAodrISbzpv7"
   },
   "outputs": [],
   "source": [
    "for year in [2030, 2050, 2080]:\n",
    "    globals()[f\"summary_26_above_015_{year}\"] = summary_26_long[\n",
    "        (summary_26_long['risk_level'] == 'high_risk') &\n",
    "        (summary_26_long['Gemeinde'] != 'Germany') &\n",
    "        (summary_26_long['year'] == year)\n",
    "    ].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZOLruQx2_nJ",
    "outputId": "4a1a44eb-8a07-40c5-b353-72bd247060b8"
   },
   "outputs": [],
   "source": [
    "summary_26_above_015_2030.GEM_ID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "orlY58ILjTrw",
    "outputId": "1fdbe20e-762f-44eb-e090-e78a6f64b2de"
   },
   "outputs": [],
   "source": [
    "len(summary_26_above_015_2030)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-JCHoj3Y1hRK"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    summary_26_above_015_df = globals()[f'summary_26_above_015_{y}']\n",
    "\n",
    "    merged_gdf = admin3_boundary.merge(summary_26_above_015_df, left_on='OBJID', right_on='GEM_ID', how='right')\n",
    "\n",
    "    globals()[f'summary_26_above_015_{y}_gdf'] = merged_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJPeDntA_4r-",
    "outputId": "263207a7-d1ea-42f6-dd8b-9b30dd936e10"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    summary_26_above_015_gdf = globals()[f'summary_26_above_015_{y}_gdf']\n",
    "    print(f\"Number of null geometries in summary_26_above_015_{y}_gdf: {len(summary_26_above_015_gdf[summary_26_above_015_gdf.geometry.isna()])}\")\n",
    "    print(f\"Number of unique geometries in summary_26_above_015_{y}_gdf: {len(summary_26_above_015_gdf.geometry.unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9a3kEEgIA2Xu",
    "outputId": "76290d3a-0806-4441-eb4f-6e6a3e037e1d"
   },
   "outputs": [],
   "source": [
    "summary_26_above_015_2030_gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KNQApovaoe88",
    "outputId": "79e48332-b749-4619-ac16-88869fe39e51"
   },
   "outputs": [],
   "source": [
    "# Summary statistics for table in appendix\n",
    "years = [2030, 2050, 2080]\n",
    "results = {}\n",
    "\n",
    "for year in years:\n",
    "    df_name = f\"summary_26_above_015_{year}_gdf\"\n",
    "    df = globals()[df_name]   # fetch DataFrame by name\n",
    "\n",
    "    positive_sum = df.loc[df.max_diff_2020 > 0].max_diff_2020.sum().round(2)\n",
    "    negative_sum = df.loc[df.max_diff_2020 < 0].max_diff_2020.sum().round(2)\n",
    "\n",
    "    results[year] = {\"positive_sum\": positive_sum, \"negative_sum\": negative_sum}\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ti4KlZuDKPEz"
   },
   "source": [
    "### Percentage difference map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sDMARC5khqrJ",
    "outputId": "7e0d6dae-1371-439e-c19b-dda714ff5583"
   },
   "outputs": [],
   "source": [
    "# Years to plot\n",
    "years = [2030, 2050, 2080]\n",
    "save_path = \"/content/drive/MyDrive/Germany_Flood_Study/Gemeinde_climate_scenario_maps/final_adjustments\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "for y in years:\n",
    "    gdf = globals()[f'summary_26_above_015_{y}_gdf'].copy()\n",
    "    gdf = gdf.to_crs(epsg=3857)\n",
    "\n",
    "    # Mask 0 values for transparency\n",
    "    gdf['masked_pct_diff'] = gdf['max_pct_diff_2020'].replace(0, np.nan)\n",
    "\n",
    "    # Colors & breaks\n",
    "    custom_colors = [\n",
    "        '#40e0d0',  # < -10%\n",
    "        '#00ffff',  # -10% ~ -1%\n",
    "        '#808080',  # -1% ~ 1%\n",
    "        '#fee5d9',  # 1% ~ 10%\n",
    "        '#fb6a4a',  # 10% ~ 50%\n",
    "        '#de2d26',  # 50% ~ 100%\n",
    "        '#a50f15'   # > 100%\n",
    "    ]\n",
    "    custom_breaks = [-np.inf, -10, -1, 1, 10, 50, 100, np.inf]\n",
    "\n",
    "    cmap = ListedColormap(custom_colors)\n",
    "    norm = BoundaryNorm(custom_breaks, ncolors=len(custom_colors), clip=False)\n",
    "\n",
    "    # Create figure and axis INSIDE loop\n",
    "    fig, ax = plt.subplots(figsize=(14, 12))\n",
    "\n",
    "    # Plot\n",
    "    gdf.plot(\n",
    "        ax=ax,\n",
    "        column='masked_pct_diff',\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        edgecolor=None,\n",
    "        linewidth=0,\n",
    "        legend=False\n",
    "    )\n",
    "\n",
    "    # Overlay geometry boundaries\n",
    "    gdf.boundary.plot(ax=ax, color='grey', linewidth=0.05)\n",
    "\n",
    "    # Basemap\n",
    "    ctx.add_basemap(ax, source=ctx.providers.CartoDB.PositronNoLabels, crs=gdf.crs)\n",
    "\n",
    "    # Cleanup\n",
    "    ax.axis('off')\n",
    "\n",
    "    # North arrow & scale bar\n",
    "    add_north_arrow(ax, scale=0.75, xlim_pos=0.9025, ylim_pos=0.9,\n",
    "                    color='black', text_scaler=4, text_yT=-1.25)\n",
    "    scale1 = ScaleBar(dx=1, location=\"lower right\", scale_loc=\"bottom\")\n",
    "    ax.add_artist(scale1)\n",
    "\n",
    "    # Custom legend\n",
    "    legend_labels = [\n",
    "        \"<-10%\", \"-10% ~ -1%\",\n",
    "        \"-1% ~ 1%\",\n",
    "        \"1% ~ 10%\", \"10% ~ 50%\", \"50% ~ 100%\", \">100%\"\n",
    "    ]\n",
    "    circles = [\n",
    "        Line2D([0], [0], marker='o', color='None',\n",
    "              markerfacecolor=custom_colors[i],\n",
    "              markeredgecolor='None',\n",
    "              markersize=10,\n",
    "              label=legend_labels[i])\n",
    "        for i in range(len(legend_labels))\n",
    "    ]\n",
    "    ax.legend(\n",
    "        handles=circles,\n",
    "        title=\"Exposure change\",\n",
    "        loc='upper left',\n",
    "        fontsize=12,\n",
    "        title_fontsize=14\n",
    "    )\n",
    "\n",
    "    # Save\n",
    "    file_name = f\"gemeinde_exposure_pct_change_high_risk_26_{y}yr_separate_dry_wet_cyansv2.png\"\n",
    "    full_path = os.path.join(save_path, file_name)\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(full_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    # plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "DvuhWmH_kLWd",
    "outputId": "637816b4-70da-496a-c2eb-fcc40641caff"
   },
   "outputs": [],
   "source": [
    "#scripts for the summary table in the appendix\n",
    "years = [2030, 2050, 2080]\n",
    "summary_list = []\n",
    "\n",
    "for y in years:\n",
    "    gdf = globals()[f'summary_26_above_015_{y}_gdf']\n",
    "\n",
    "    # Count how many Gemeinden are below and above zero\n",
    "    below_zero = (gdf['max_diff_2020'] < 0).sum()\n",
    "    above_zero = (gdf['max_diff_2020'] > 0).sum()\n",
    "\n",
    "    # Distribution stats for each group\n",
    "    below_zero_desc = gdf.loc[gdf['max_diff_2020'] < 0, 'max_diff_2020'].describe()\n",
    "    above_zero_desc = gdf.loc[gdf['max_diff_2020'] > 0, 'max_diff_2020'].describe()\n",
    "\n",
    "    summary_list.append({\n",
    "        'Year': y,\n",
    "        'Below Zero Count': below_zero,\n",
    "        'Above Zero Count': above_zero,\n",
    "        'Below Zero Min': below_zero_desc['min'],\n",
    "        'Below Zero Median': below_zero_desc['50%'],\n",
    "        'Below Zero Max': below_zero_desc['max'],\n",
    "        'Above Zero Min': above_zero_desc['min'],\n",
    "        'Above Zero Median': above_zero_desc['50%'],\n",
    "        'Above Zero Max': above_zero_desc['max']\n",
    "    })\n",
    "\n",
    "# Create a DataFrame with all years\n",
    "summary_df = pd.DataFrame(summary_list)\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mE0oFwEa5wB"
   },
   "source": [
    "## Coastal flooding undefended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBkuzrNfa5wC"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXRPGA2da5wC"
   },
   "outputs": [],
   "source": [
    "# List of years to process\n",
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "# Columns you want to select\n",
    "cu_columns = ['id', 'Gemeinde', 'Kreis', 'Land'] + [col for col in all_columns if col.startswith('CU')]\n",
    "\n",
    "# Loop through each year and modify the corresponding DataFrame\n",
    "for year in years:\n",
    "    if year == 2020:\n",
    "        df_name = f'Gemeinde_{year}_100_raw'  # 2020 uses '_100_raw'\n",
    "    else:\n",
    "        df_name = f'Gemeinde_{year}_100_26_raw'  # 2030, 2050, and 2080 use '_100_85_raw'\n",
    "\n",
    "    # Check if the DataFrame exists before proceeding\n",
    "    if df_name in globals():\n",
    "        # Get the raw DataFrame and select the relevant columns for max flooding data\n",
    "        df_raw = globals()[df_name]\n",
    "        globals()[f'Gemeinde_{year}_100_26_CU'] = df_raw[cu_columns]\n",
    "    else:\n",
    "        print(f'{df_name} does not exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6hARwJvja5wD",
    "outputId": "e0ba4bc7-162d-495d-83b4-ac494b9b2210"
   },
   "outputs": [],
   "source": [
    "len(Gemeinde_2020_100_26_CU) == len(Gemeinde_2030_100_26_CU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8Sphjx-a5wD"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    globals()[f'Gemeinde_{y}_100_26_CU_long'] = globals()[f'Gemeinde_{y}_100_26_CU'].melt(\n",
    "        id_vars=['id', 'Gemeinde', 'Kreis', 'Land'], var_name='month_year_depth', value_name='CU_area'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "j_CwlqVka5wD",
    "outputId": "c536c8b3-81e9-476f-b061-2d0ea7647062"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_26_CU_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0p5ojZga5wD"
   },
   "outputs": [],
   "source": [
    "for df in [Gemeinde_2020_100_26_CU_long, Gemeinde_2030_100_26_CU_long, Gemeinde_2050_100_26_CU_long, Gemeinde_2080_100_26_CU_long]:\n",
    "\n",
    "    # Replace 'CU' with 'MAX' if needed (optional step if not already 'MAX')\n",
    "    # df['month_year_depth'] = df['month_year_depth'].str.replace('CU', 'MAX')\n",
    "\n",
    "    # Split the 'month_year_depth' column into parts\n",
    "    df[['prefix', 'month_year', 'depth_cat']] = df['month_year_depth'].str.extract(r'^(CU)-([A-Za-z]{3}-\\d{2})-(.+)$')\n",
    "\n",
    "    # Drop 'prefix' if not needed\n",
    "    df.drop(columns='prefix', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "7hz83HOga5wD",
    "outputId": "5e04440e-3095-4cb9-c382-0a8e1d41d53f"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2080_100_26_CU_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dSzST-yRa5wE"
   },
   "outputs": [],
   "source": [
    "# Define your DataFrames with their corresponding years\n",
    "year_df_map = {\n",
    "    2020: Gemeinde_2020_100_26_CU_long,\n",
    "    2030: Gemeinde_2030_100_26_CU_long,\n",
    "    2050: Gemeinde_2050_100_26_CU_long,\n",
    "    2080: Gemeinde_2080_100_26_CU_long\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame\n",
    "for year, df in year_df_map.items():\n",
    "\n",
    "    # Filter to only Jan-25\n",
    "    jan25_df = df[df['month_year'] == 'Jan-25'].reset_index(drop=True)\n",
    "\n",
    "    # Set the new global variable\n",
    "    globals()[f'Gemeinde_{year}_100_26_CU_Jan25'] = jan25_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9dthEVka5wE"
   },
   "outputs": [],
   "source": [
    "# Second part: group and create *_settle DataFrames\n",
    "for year in [2020, 2030, 2050, 2080]:\n",
    "    jan25_var_name = f'Gemeinde_{year}_100_26_CU_Jan25'\n",
    "\n",
    "    if jan25_var_name in globals():\n",
    "        jan25_df = globals()[jan25_var_name]\n",
    "\n",
    "        # Group and sum by 'id' and 'month_year'\n",
    "        settle_df = jan25_df.groupby(['id', 'month_year'], as_index=False)['CU_area'].sum()\n",
    "        settle_df.columns = ['id', 'month_year', 'settle_area']\n",
    "        settle_df['settle_area'] = settle_df['settle_area'].astype(float)\n",
    "\n",
    "        # Save to a new global variable\n",
    "        globals()[f'{jan25_var_name}_settle'] = settle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTbNMj8_a5wE"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    long_df = globals()[f'Gemeinde_{y}_100_26_CU_Jan25']\n",
    "    settle_df = globals()[f'Gemeinde_{y}_100_26_CU_Jan25_settle']\n",
    "\n",
    "    merged_df = pd.merge(long_df, settle_df, on=['id', 'month_year'], how='left')\n",
    "\n",
    "    globals()[f'Gemeinde_{y}_100_26_CU_Jan25'] = merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf04bgnja5wF"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    df = globals()[f'Gemeinde_{y}_100_26_CU_Jan25']\n",
    "    globals()[f'Gemeinde_{y}_100_26_CU_Jan25'] = df\n",
    "    df['settle_area_non_zero'] = df.settle_area.replace(0, 1e-6)\n",
    "    globals()[f'Gemeinde_{y}_100_26_CU_Jan25']['settle_area_non_zero'] = df.settle_area_non_zero\n",
    "    globals()[f'Gemeinde_{y}_100_26_CU_Jan25']['depth_cat_area_pct'] = (df.CU_area / df.settle_area_non_zero) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PxLbvFBXa5wG",
    "outputId": "6a7238ba-201f-4780-8d8c-60fd3ac81407"
   },
   "outputs": [],
   "source": [
    "cols = ['id', 'Gemeinde', 'Kreis', 'Land']\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "base_df = Gemeinde_2020_100_26_CU_Jan25\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_26_CU_Jan25']\n",
    "\n",
    "    # Direct comparison of the column values (row by row, in order)\n",
    "    is_same = base_df[cols].equals(current_df[cols])\n",
    "\n",
    "    if is_same:\n",
    "        print(f\"✅ Columns match exactly for year {year}\")\n",
    "    else:\n",
    "        print(f\"❌ Columns differ for year {year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DVV13cAIa5wG"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_26_CU_Jan25']\n",
    "    base_df = Gemeinde_2020_100_26_CU_Jan25\n",
    "    base_df[f'diff_{year}'] = current_df.CU_area - base_df.CU_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ZuEIEFga5wG"
   },
   "outputs": [],
   "source": [
    "summary = Gemeinde_2020_100_26_CU_Jan25[['id', 'Gemeinde', 'Kreis', 'Land']].drop_duplicates().copy()\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    diff_col = f'diff_{year}'\n",
    "\n",
    "    pivot = (\n",
    "        Gemeinde_2020_100_26_CU_Jan25\n",
    "        .groupby(['id', 'depth_cat'])[diff_col]\n",
    "        .sum()\n",
    "        .unstack(fill_value=0)\n",
    "        .add_prefix(f'sum_diff_{year}_')  # e.g., sum_diff_2030_lt0.5\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    summary = summary.merge(pivot, on='id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gSJBlgGza5wG"
   },
   "outputs": [],
   "source": [
    "# Convert all newly added columns to float\n",
    "new_cols = [col for col in summary.columns if col.startswith('sum_diff_')]\n",
    "summary[new_cols] = summary[new_cols].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "e0xcJU3wa5wG",
    "outputId": "f5941843-3eaf-43eb-cce5-21d9fc00e1e0"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_26_CU_Jan25[Gemeinde_2020_100_26_CU_Jan25.Gemeinde == 'Allmendingen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LaoodmY7a5wH"
   },
   "outputs": [],
   "source": [
    "summary_26 = summary.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JTkqitdEa5wH"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    summary_26[f'high_risk_{year}'] = (\n",
    "        summary_26.get(f'sum_diff_{year}_lt0.5', 0) +\n",
    "        summary_26.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_26.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_26[f'very_high_risk_{year}'] = (\n",
    "        summary_26.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_26.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_26[f'extreme_risk_{year}'] = summary_26.get(f'sum_diff_{year}_gt1.5', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g5VLupvja5wH",
    "outputId": "543122eb-368a-4be1-fc7c-8471b8593fc1"
   },
   "outputs": [],
   "source": [
    "summary_26.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMcjWzfua5wH"
   },
   "outputs": [],
   "source": [
    "value_vars = [\n",
    "    'high_risk_2030', 'very_high_risk_2030', 'extreme_risk_2030',\n",
    "    'high_risk_2050', 'very_high_risk_2050', 'extreme_risk_2050',\n",
    "    'high_risk_2080', 'very_high_risk_2080', 'extreme_risk_2080'\n",
    "]\n",
    "\n",
    "cu_summary_26_long = summary_26.melt(\n",
    "    id_vars=['id', 'Gemeinde', 'Kreis', 'Land'],  # keep these\n",
    "    value_vars=value_vars,\n",
    "    var_name='risk_year',\n",
    "    value_name='cu_diff_2020'\n",
    ")\n",
    "\n",
    "# Split the combined column into risk_level and year\n",
    "cu_summary_26_long[['risk_level', 'year']] = cu_summary_26_long['risk_year'].str.extract(r'(.*)_(\\d{4})')\n",
    "cu_summary_26_long['year'] = cu_summary_26_long['year'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "tujFVsvJa5wH",
    "outputId": "2e75475a-c52e-4f89-a384-bfa8919f64ab"
   },
   "outputs": [],
   "source": [
    "cu_summary_26_long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDkskPana5wH"
   },
   "source": [
    "### Please store the pre-processed files as pickle files, since you’ll need to load them directly later—especially if the session needs to be restarted due to running out of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQZgqeK2fhtT"
   },
   "outputs": [],
   "source": [
    "cu_summary_26_long.to_pickle(\"cu_summary_26_long.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLIqQTk14Yfa"
   },
   "outputs": [],
   "source": [
    "with open(\"cu_summary_26_long.pkl\", \"rb\") as f:\n",
    "    cu_summary_26_long = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLM8BY3l4ZCr"
   },
   "source": [
    "## Fluvial flooding undefended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzpXLsfd4ZCr"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EdJn6HPS4ZCr"
   },
   "outputs": [],
   "source": [
    "# List of years to process\n",
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "# Columns you want to select\n",
    "fu_columns = ['id', 'Gemeinde', 'Kreis', 'Land'] + [col for col in all_columns if col.startswith('FU')]\n",
    "\n",
    "# Loop through each year and modify the corresponding DataFrame\n",
    "for year in years:\n",
    "    if year == 2020:\n",
    "        df_name = f'Gemeinde_{year}_100_raw'  # 2020 uses '_100_raw'\n",
    "    else:\n",
    "        df_name = f'Gemeinde_{year}_100_26_raw'  # 2030, 2050, and 2080 use '_100_85_raw'\n",
    "\n",
    "    # Check if the DataFrame exists before proceeding\n",
    "    if df_name in globals():\n",
    "        # Get the raw DataFrame and select the relevant columns for max flooding data\n",
    "        df_raw = globals()[df_name]\n",
    "        globals()[f'Gemeinde_{year}_100_26_FU'] = df_raw[fu_columns]\n",
    "    else:\n",
    "        print(f'{df_name} does not exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_TggYnl4ZCr"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    globals()[f'Gemeinde_{y}_100_26_FU_long'] = globals()[f'Gemeinde_{y}_100_26_FU'].melt(\n",
    "        id_vars=['id', 'Gemeinde', 'Kreis', 'Land'], var_name='month_year_depth', value_name='FU_area'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q9Cx6uhO4ZCr",
    "outputId": "a64208a5-fef3-49da-efe4-a8fceb7c18f4"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_26_FU_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXAov8M84ZCs"
   },
   "outputs": [],
   "source": [
    "for df in [Gemeinde_2020_100_26_FU_long, Gemeinde_2030_100_26_FU_long, Gemeinde_2050_100_26_FU_long, Gemeinde_2080_100_26_FU_long]:\n",
    "\n",
    "    # Replace 'CU' with 'MAX' if needed (optional step if not already 'MAX')\n",
    "    # df['month_year_depth'] = df['month_year_depth'].str.replace('CU', 'MAX')\n",
    "\n",
    "    # Split the 'month_year_depth' column into parts\n",
    "    df[['prefix', 'month_year', 'depth_cat']] = df['month_year_depth'].str.extract(r'^(FU)-([A-Za-z]{3}-\\d{2})-(.+)$')\n",
    "\n",
    "    # Drop 'prefix' if not needed\n",
    "    df.drop(columns='prefix', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6mNx44uI4ZCs",
    "outputId": "d12e31a9-a025-41b9-aa63-d025292f142a"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2080_100_26_FU_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wx7DZQ9Z4ZCs"
   },
   "outputs": [],
   "source": [
    "# Define your DataFrames with their corresponding years\n",
    "year_df_map = {\n",
    "    2020: Gemeinde_2020_100_26_FU_long,\n",
    "    2030: Gemeinde_2030_100_26_FU_long,\n",
    "    2050: Gemeinde_2050_100_26_FU_long,\n",
    "    2080: Gemeinde_2080_100_26_FU_long\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame\n",
    "for year, df in year_df_map.items():\n",
    "\n",
    "    # Filter to only Jan-25\n",
    "    jan25_df = df[df['month_year'] == 'Jan-25'].reset_index(drop=True)\n",
    "\n",
    "    # Set the new global variable\n",
    "    globals()[f'Gemeinde_{year}_100_26_FU_Jan25'] = jan25_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUjTOxOV4ZCs"
   },
   "outputs": [],
   "source": [
    "# Second part: group and create *_settle DataFrames\n",
    "for year in [2020, 2030, 2050, 2080]:\n",
    "    jan25_var_name = f'Gemeinde_{year}_100_26_FU_Jan25'\n",
    "\n",
    "    if jan25_var_name in globals():\n",
    "        jan25_df = globals()[jan25_var_name]\n",
    "\n",
    "        # Group and sum by 'id' and 'month_year'\n",
    "        settle_df = jan25_df.groupby(['id', 'month_year'], as_index=False)['FU_area'].sum()\n",
    "        settle_df.columns = ['id', 'month_year', 'settle_area']\n",
    "        settle_df['settle_area'] = settle_df['settle_area'].astype(float)\n",
    "\n",
    "        # Save to a new global variable\n",
    "        globals()[f'{jan25_var_name}_settle'] = settle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGfz2KVG4ZCs"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    long_df = globals()[f'Gemeinde_{y}_100_26_FU_Jan25']\n",
    "    settle_df = globals()[f'Gemeinde_{y}_100_26_FU_Jan25_settle']\n",
    "\n",
    "    merged_df = pd.merge(long_df, settle_df, on=['id', 'month_year'], how='left')\n",
    "\n",
    "    globals()[f'Gemeinde_{y}_100_26_FU_Jan25'] = merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUZNGWlB4ZCs"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    df = globals()[f'Gemeinde_{y}_100_26_FU_Jan25']\n",
    "    globals()[f'Gemeinde_{y}_100_26_FU_Jan25'] = df\n",
    "    df['settle_area_non_zero'] = df.settle_area.replace(0, 1e-6)\n",
    "    globals()[f'Gemeinde_{y}_100_26_FU_Jan25']['settle_area_non_zero'] = df.settle_area_non_zero\n",
    "    globals()[f'Gemeinde_{y}_100_26_FU_Jan25']['depth_cat_area_pct'] = (df.FU_area / df.settle_area_non_zero) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fTW90yHb4ZCs",
    "outputId": "1e7b0af8-972f-44ee-9b37-13f1e3e82f32"
   },
   "outputs": [],
   "source": [
    "cols = ['id', 'Gemeinde', 'Kreis', 'Land']\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "base_df = Gemeinde_2020_100_26_FU_Jan25\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_26_FU_Jan25']\n",
    "\n",
    "    # Direct comparison of the column values (row by row, in order)\n",
    "    is_same = base_df[cols].equals(current_df[cols])\n",
    "\n",
    "    if is_same:\n",
    "        print(f\"✅ Columns match exactly for year {year}\")\n",
    "    else:\n",
    "        print(f\"❌ Columns differ for year {year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4hUXky94ZCs"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_26_FU_Jan25']\n",
    "    base_df = Gemeinde_2020_100_26_FU_Jan25\n",
    "    base_df[f'diff_{year}'] = current_df.FU_area - base_df.FU_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GtUvvbzx4ZCs"
   },
   "outputs": [],
   "source": [
    "summary = Gemeinde_2020_100_26_FU_Jan25[['id', 'Gemeinde', 'Kreis', 'Land']].drop_duplicates().copy()\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    diff_col = f'diff_{year}'\n",
    "\n",
    "    pivot = (\n",
    "        Gemeinde_2020_100_26_FU_Jan25\n",
    "        .groupby(['id', 'depth_cat'])[diff_col]\n",
    "        .sum()\n",
    "        .unstack(fill_value=0)\n",
    "        .add_prefix(f'sum_diff_{year}_')  # e.g., sum_diff_2030_lt0.5\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    summary = summary.merge(pivot, on='id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JgYkVRLP4ZCs"
   },
   "outputs": [],
   "source": [
    "# Convert all newly added columns to float\n",
    "new_cols = [col for col in summary.columns if col.startswith('sum_diff_')]\n",
    "summary[new_cols] = summary[new_cols].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Efj4kCP84ZCs"
   },
   "outputs": [],
   "source": [
    "summary_26 = summary.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_ZrMFuH4ZCs"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    summary_26[f'high_risk_{year}'] = (\n",
    "        summary_26.get(f'sum_diff_{year}_lt0.5', 0) +\n",
    "        summary_26.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_26.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_26[f'very_high_risk_{year}'] = (\n",
    "        summary_26.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_26.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_26[f'extreme_risk_{year}'] = summary_26.get(f'sum_diff_{year}_gt1.5', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3n9frhDC4ZCt",
    "outputId": "823fc98a-c7f7-4dca-d6ee-79bdaf9ddfcf"
   },
   "outputs": [],
   "source": [
    "summary_26.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sv94Kx104ZCt"
   },
   "outputs": [],
   "source": [
    "value_vars = [\n",
    "    'high_risk_2030', 'very_high_risk_2030', 'extreme_risk_2030',\n",
    "    'high_risk_2050', 'very_high_risk_2050', 'extreme_risk_2050',\n",
    "    'high_risk_2080', 'very_high_risk_2080', 'extreme_risk_2080'\n",
    "]\n",
    "\n",
    "fu_summary_26_long = summary_26.melt(\n",
    "    id_vars=['id', 'Gemeinde', 'Kreis', 'Land'],  # keep these\n",
    "    value_vars=value_vars,\n",
    "    var_name='risk_year',\n",
    "    value_name='fu_diff_2020'\n",
    ")\n",
    "\n",
    "# Split the combined column into risk_level and year\n",
    "fu_summary_26_long[['risk_level', 'year']] = fu_summary_26_long['risk_year'].str.extract(r'(.*)_(\\d{4})')\n",
    "fu_summary_26_long['year'] = fu_summary_26_long['year'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SewS_JSV4ZCt",
    "outputId": "b0580781-fabb-4a38-f495-dcb9f9441d10"
   },
   "outputs": [],
   "source": [
    "fu_summary_26_long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKZOyBui4ZCt"
   },
   "source": [
    "### Please store the pre-processed files as pickle files, since you’ll need to load them directly later—especially if the session needs to be restarted due to running out of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hk5PAPss4ZCt"
   },
   "outputs": [],
   "source": [
    "fu_summary_26_long.to_pickle(\"fu_summary_26_long.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kIlBW7dA7YJ2"
   },
   "outputs": [],
   "source": [
    "with open(\"fu_summary_26_long.pkl\", \"rb\") as f:\n",
    "    fu_summary_26_long = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQJCg7gAJYhJ"
   },
   "source": [
    "## Pluvial flooding defended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ves2q3poJYhK"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqoV8OxyJYhK"
   },
   "outputs": [],
   "source": [
    "# List of years to process\n",
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "# Columns you want to select\n",
    "pd_columns = ['id', 'Gemeinde', 'Kreis', 'Land'] + [col for col in all_columns if col.startswith('PD')]\n",
    "\n",
    "# Loop through each year and modify the corresponding DataFrame\n",
    "for year in years:\n",
    "    if year == 2020:\n",
    "        df_name = f'Gemeinde_{year}_100_raw'  # 2020 uses '_100_raw'\n",
    "    else:\n",
    "        df_name = f'Gemeinde_{year}_100_26_raw'  # 2030, 2050, and 2080 use '_100_85_raw'\n",
    "\n",
    "    # Check if the DataFrame exists before proceeding\n",
    "    if df_name in globals():\n",
    "        # Get the raw DataFrame and select the relevant columns for max flooding data\n",
    "        df_raw = globals()[df_name]\n",
    "        globals()[f'Gemeinde_{year}_100_26_PD'] = df_raw[pd_columns]\n",
    "    else:\n",
    "        print(f'{df_name} does not exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ieYor3oCJYhK"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    globals()[f'Gemeinde_{y}_100_26_PD_long'] = globals()[f'Gemeinde_{y}_100_26_PD'].melt(\n",
    "        id_vars=['id', 'Gemeinde', 'Kreis', 'Land'], var_name='month_year_depth', value_name='PD_area'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vNuimSisJYhL",
    "outputId": "54209c64-9840-401c-a275-d1d1cbd878bd"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_26_PD_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zaxic1E8JYhL"
   },
   "outputs": [],
   "source": [
    "for df in [Gemeinde_2020_100_26_PD_long, Gemeinde_2030_100_26_PD_long, Gemeinde_2050_100_26_PD_long, Gemeinde_2080_100_26_PD_long]:\n",
    "\n",
    "    # Replace 'CU' with 'MAX' if needed (optional step if not already 'MAX')\n",
    "    # df['month_year_depth'] = df['month_year_depth'].str.replace('CU', 'MAX')\n",
    "\n",
    "    # Split the 'month_year_depth' column into parts\n",
    "    df[['prefix', 'month_year', 'depth_cat']] = df['month_year_depth'].str.extract(r'^(PD)-([A-Za-z]{3}-\\d{2})-(.+)$')\n",
    "\n",
    "    # Drop 'prefix' if not needed\n",
    "    df.drop(columns='prefix', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bTo5SnmNJYhL",
    "outputId": "9beb89d6-ad88-4f26-cf70-ddf7acf7d8bf"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2080_100_26_PD_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6sIzORe-JYhL"
   },
   "outputs": [],
   "source": [
    "# Define your DataFrames with their corresponding years\n",
    "year_df_map = {\n",
    "    2020: Gemeinde_2020_100_26_PD_long,\n",
    "    2030: Gemeinde_2030_100_26_PD_long,\n",
    "    2050: Gemeinde_2050_100_26_PD_long,\n",
    "    2080: Gemeinde_2080_100_26_PD_long\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame\n",
    "for year, df in year_df_map.items():\n",
    "\n",
    "    # Filter to only Jan-25\n",
    "    jan25_df = df[df['month_year'] == 'Jan-25'].reset_index(drop=True)\n",
    "\n",
    "    # Set the new global variable\n",
    "    globals()[f'Gemeinde_{year}_100_26_PD_Jan25'] = jan25_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5Uqo1I1JYhL"
   },
   "outputs": [],
   "source": [
    "# Second part: group and create *_settle DataFrames\n",
    "for year in [2020, 2030, 2050, 2080]:\n",
    "    jan25_var_name = f'Gemeinde_{year}_100_26_PD_Jan25'\n",
    "\n",
    "    if jan25_var_name in globals():\n",
    "        jan25_df = globals()[jan25_var_name]\n",
    "\n",
    "        # Group and sum by 'id' and 'month_year'\n",
    "        settle_df = jan25_df.groupby(['id', 'month_year'], as_index=False)['PD_area'].sum()\n",
    "        settle_df.columns = ['id', 'month_year', 'settle_area']\n",
    "        settle_df['settle_area'] = settle_df['settle_area'].astype(float)\n",
    "\n",
    "        # Save to a new global variable\n",
    "        globals()[f'{jan25_var_name}_settle'] = settle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1lGGK-xKJYhL"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    long_df = globals()[f'Gemeinde_{y}_100_26_PD_Jan25']\n",
    "    settle_df = globals()[f'Gemeinde_{y}_100_26_PD_Jan25_settle']\n",
    "\n",
    "    merged_df = pd.merge(long_df, settle_df, on=['id', 'month_year'], how='left')\n",
    "\n",
    "    globals()[f'Gemeinde_{y}_100_26_PD_Jan25'] = merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85Hcnqj6JYhL"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    df = globals()[f'Gemeinde_{y}_100_26_PD_Jan25']\n",
    "    globals()[f'Gemeinde_{y}_100_26_PD_Jan25'] = df\n",
    "    df['settle_area_non_zero'] = df.settle_area.replace(0, 1e-6)\n",
    "    globals()[f'Gemeinde_{y}_100_26_PD_Jan25']['settle_area_non_zero'] = df.settle_area_non_zero\n",
    "    globals()[f'Gemeinde_{y}_100_26_PD_Jan25']['depth_cat_area_pct'] = (df.PD_area / df.settle_area_non_zero) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D71LnJtKJYhL",
    "outputId": "a877013d-ae6e-401c-d190-7dd365534c11"
   },
   "outputs": [],
   "source": [
    "cols = ['id', 'Gemeinde', 'Kreis', 'Land']\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "base_df = Gemeinde_2020_100_26_PD_Jan25\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_26_PD_Jan25']\n",
    "\n",
    "    # Direct comparison of the column values (row by row, in order)\n",
    "    is_same = base_df[cols].equals(current_df[cols])\n",
    "\n",
    "    if is_same:\n",
    "        print(f\"✅ Columns match exactly for year {year}\")\n",
    "    else:\n",
    "        print(f\"❌ Columns differ for year {year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXr7RbGuJYhL"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_26_PD_Jan25']\n",
    "    base_df = Gemeinde_2020_100_26_PD_Jan25\n",
    "    base_df[f'diff_{year}'] = current_df.PD_area - base_df.PD_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "urYmuZ1zJYhM"
   },
   "outputs": [],
   "source": [
    "summary = Gemeinde_2020_100_26_PD_Jan25[['id', 'Gemeinde', 'Kreis', 'Land']].drop_duplicates().copy()\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    diff_col = f'diff_{year}'\n",
    "\n",
    "    pivot = (\n",
    "        Gemeinde_2020_100_26_PD_Jan25\n",
    "        .groupby(['id', 'depth_cat'])[diff_col]\n",
    "        .sum()\n",
    "        .unstack(fill_value=0)\n",
    "        .add_prefix(f'sum_diff_{year}_')  # e.g., sum_diff_2030_lt0.5\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    summary = summary.merge(pivot, on='id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ny7Nc-zQJYhM"
   },
   "outputs": [],
   "source": [
    "# Convert all newly added columns to float\n",
    "new_cols = [col for col in summary.columns if col.startswith('sum_diff_')]\n",
    "summary[new_cols] = summary[new_cols].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-kUTNAeoJYhM"
   },
   "outputs": [],
   "source": [
    "summary_26 = summary.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5R6mq9T4JYhM"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    summary_26[f'high_risk_{year}'] = (\n",
    "        summary_26.get(f'sum_diff_{year}_lt0.5', 0) +\n",
    "        summary_26.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_26.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_26[f'very_high_risk_{year}'] = (\n",
    "        summary_26.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_26.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_26[f'extreme_risk_{year}'] = summary_26.get(f'sum_diff_{year}_gt1.5', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S3znQ8oaJYhM",
    "outputId": "14951e83-a4fd-4ac3-d8b2-03e78a6b2d1a"
   },
   "outputs": [],
   "source": [
    "summary_26.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyvUpEfBJYhM"
   },
   "outputs": [],
   "source": [
    "value_vars = [\n",
    "    'high_risk_2030', 'very_high_risk_2030', 'extreme_risk_2030',\n",
    "    'high_risk_2050', 'very_high_risk_2050', 'extreme_risk_2050',\n",
    "    'high_risk_2080', 'very_high_risk_2080', 'extreme_risk_2080'\n",
    "]\n",
    "\n",
    "pd_summary_26_long = summary_26.melt(\n",
    "    id_vars=['id', 'Gemeinde', 'Kreis', 'Land'],  # keep these\n",
    "    value_vars=value_vars,\n",
    "    var_name='risk_year',\n",
    "    value_name='pd_diff_2020'\n",
    ")\n",
    "\n",
    "# Split the combined column into risk_level and year\n",
    "pd_summary_26_long[['risk_level', 'year']] = pd_summary_26_long['risk_year'].str.extract(r'(.*)_(\\d{4})')\n",
    "pd_summary_26_long['year'] = pd_summary_26_long['year'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8DTqbzP2JYhM",
    "outputId": "c994ba5e-6fb5-4350-e63b-b8b24abeab95"
   },
   "outputs": [],
   "source": [
    "pd_summary_26_long.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPCghViuJYhM"
   },
   "source": [
    "### Please store the pre-processed files as pickle files, since you’ll need to load them directly later—especially if the session needs to be restarted due to running out of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5ryrXITJYhM"
   },
   "outputs": [],
   "source": [
    "pd_summary_26_long.to_pickle(\"pd_summary_26_long.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KyyCMzfuJYhM"
   },
   "outputs": [],
   "source": [
    "with open(\"pd_summary_26_long.pkl\", \"rb\") as f:\n",
    "    pd_summary_26_long = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAYoxlKGvTiv"
   },
   "source": [
    "# SSP2-RCP4.5 (Moderate Optimistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFp-9TuhaFFN"
   },
   "source": [
    "### Read files, pre-processing and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_2HA-0oDvTiw",
    "outputId": "cbb519b3-2fb8-412b-fb6f-7fe02ad64a45"
   },
   "outputs": [],
   "source": [
    "!unzip \"/content/moderate_optimistic.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJ_to_bpvTiw"
   },
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    # '/content/Germany_Gemeinde_2020_100.xlsx',\n",
    "    '/content/moderate_optimistic/Germany_Gemeinde_2030_100_45.xlsx',\n",
    "    '/content/moderate_optimistic/Germany_Gemeinde_2050_100_45.xlsx',\n",
    "    '/content/moderate_optimistic/Germany_Gemeinde_2080_100_45.xlsx',\n",
    "]\n",
    "\n",
    "dataframes = []\n",
    "for path in file_paths:\n",
    "    df = pd.read_excel(path)\n",
    "    df = df.round(6)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Optionally unpack into variables\n",
    "(Gemeinde_2030_100_45_raw, Gemeinde_2050_100_45_raw, Gemeinde_2080_100_45_raw) = dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgiuLsQsvTiw"
   },
   "outputs": [],
   "source": [
    "# Date range: every 6 months\n",
    "dates = pd.date_range(start='2016-07-01', end='2025-01-01', freq='6MS')\n",
    "\n",
    "# Prefixes and suffixes\n",
    "prefixes = ['CU', 'FU', 'PD', 'MAX']\n",
    "suffixes = ['P0', 'lt0.15', 'lt0.5', 'lt1.5', 'gt1.5']\n",
    "\n",
    "# Correct order: by prefix, then suffix, then date\n",
    "all_columns = [\n",
    "    f\"{prefix}-{date.strftime('%b-%y')}-{suffix}\"\n",
    "    for prefix in prefixes\n",
    "    for suffix in suffixes\n",
    "    for date in dates\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PsjDMUXIvTix"
   },
   "outputs": [],
   "source": [
    "# Check if number of replacement columns matches the shape\n",
    "for df in [Gemeinde_2030_100_45_raw, Gemeinde_2050_100_45_raw, Gemeinde_2080_100_45_raw]:\n",
    "    if len(all_columns) == df.shape[1] - 3:\n",
    "      df.columns = list(df.columns[:3]) + all_columns\n",
    "      df.reset_index(drop=True, inplace=True)\n",
    "      df['id'] = df.index\n",
    "    else:\n",
    "      raise ValueError(\"Length of generated column names does not match number of columns (excluding the first one).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "id": "oieum8H3xV1w",
    "outputId": "01a8e0ac-6f60-42e8-97b7-b73be9567fea"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2080_100_45_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roUFis3DvTix"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2030_100_45_raw = Gemeinde_2030_100_45_raw[1:]\n",
    "Gemeinde_2050_100_45_raw = Gemeinde_2050_100_45_raw[1:]\n",
    "Gemeinde_2080_100_45_raw = Gemeinde_2080_100_45_raw[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ln84DYN5vTix"
   },
   "outputs": [],
   "source": [
    "save_path = \"/content/\"\n",
    "\n",
    "# Make sure the directory exists\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "gemeinde_dfs = [Gemeinde_2020_100_raw, Gemeinde_2030_100_45_raw, Gemeinde_2050_100_45_raw, Gemeinde_2080_100_45_raw]\n",
    "\n",
    "filenames = [\n",
    "    \"Gemeinde_2020_100_raw.pkl\", \"Gemeinde_2030_100_45_raw.pkl\", \"Gemeinde_2050_100_45_raw.pkl\", \"Gemeinde_2080_100_45_raw.pkl\"\n",
    "]\n",
    "\n",
    "for df, filename in zip(gemeinde_dfs, filenames):\n",
    "    df.to_pickle(os.path.join(save_path, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPqU6G9tZ8Fb"
   },
   "source": [
    "### **PLEASE re-run if the session restarts before you finish processing this climate scenario:** Read and load files from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJmufjDTvTix"
   },
   "outputs": [],
   "source": [
    "# Define the path where the files are stored\n",
    "save_path = \"/content/\"\n",
    "\n",
    "# List of filenames\n",
    "filenames = [\n",
    "    \"Gemeinde_2020_100_raw.pkl\", \"Gemeinde_2030_100_45_raw.pkl\", \"Gemeinde_2050_100_45_raw.pkl\", \"Gemeinde_2080_100_45_raw.pkl\"\n",
    "]\n",
    "\n",
    "# Load all DataFrames into a list\n",
    "gemeinde_dfs = [pd.read_pickle(os.path.join(save_path, filename)) for filename in filenames]\n",
    "\n",
    "cols_to_convert = [col for col in gemeinde_dfs[0].columns if col not in ['Gemeinde', 'Kreis', 'Land', 'id']]\n",
    "\n",
    "def convert_and_round(df):\n",
    "    df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, errors='coerce').round(6)\n",
    "    return df\n",
    "\n",
    "gemeinde_dfs = [convert_and_round(df) for df in gemeinde_dfs]\n",
    "\n",
    "# Optionally assign them back to individual variable names (if needed)\n",
    "(\n",
    "  Gemeinde_2020_100_raw,\n",
    "  Gemeinde_2030_100_45_raw,\n",
    "  Gemeinde_2050_100_45_raw,\n",
    "  Gemeinde_2080_100_45_raw\n",
    ") = gemeinde_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKL_wwYes0Zh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCBTBdNCvTix"
   },
   "source": [
    "## Combinded flooding (max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbAGzYuIvTix"
   },
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tBiJqLAvTix"
   },
   "outputs": [],
   "source": [
    "# List of years to process\n",
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "# Columns you want to select (CU columns and Land)\n",
    "max_columns = ['id', 'Gemeinde', 'Kreis', 'Land'] + [col for col in all_columns if col.startswith('MAX') or col == 'Land']\n",
    "\n",
    "# Loop through each year and modify the corresponding DataFrame\n",
    "for year in years:\n",
    "    if year == 2020:\n",
    "        df_name = f'Gemeinde_{year}_100_raw'  # 2020 uses '_100_raw'\n",
    "    else:\n",
    "        df_name = f'Gemeinde_{year}_100_45_raw'  # 2030, 2050, and 2080 use '_100_85_raw'\n",
    "\n",
    "    # Check if the DataFrame exists before proceeding\n",
    "    if df_name in globals():\n",
    "        # Get the raw DataFrame and select the relevant columns for max flooding data\n",
    "        df_raw = globals()[df_name]\n",
    "        globals()[f'Gemeinde_{year}_100_45_MAX'] = df_raw[max_columns]\n",
    "    else:\n",
    "        print(f'{df_name} does not exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zmvCjvvkvTix",
    "outputId": "fe88c8af-c7a8-4611-efa8-57a9d7538da1"
   },
   "outputs": [],
   "source": [
    "len(Gemeinde_2020_100_45_MAX) == len(Gemeinde_2030_100_45_MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTOmI5YhvTix"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    globals()[f'Gemeinde_{y}_100_45_MAX_long'] = globals()[f'Gemeinde_{y}_100_45_MAX'].melt(\n",
    "        id_vars=['id', 'Gemeinde', 'Kreis', 'Land'], var_name='month_year_depth', value_name='MAX_area'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "L6aseDkGvTix",
    "outputId": "7e270e76-cf92-42a6-8b8e-5cfa6f3a33c2"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_45_MAX_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpyQ_ijovTix"
   },
   "outputs": [],
   "source": [
    "for df in [Gemeinde_2020_100_45_MAX_long, Gemeinde_2030_100_45_MAX_long, Gemeinde_2050_100_45_MAX_long, Gemeinde_2080_100_45_MAX_long]:\n",
    "\n",
    "    # Replace 'CU' with 'MAX' if needed (optional step if not already 'MAX')\n",
    "    # df['month_year_depth'] = df['month_year_depth'].str.replace('CU', 'MAX')\n",
    "\n",
    "    # Split the 'month_year_depth' column into parts\n",
    "    df[['prefix', 'month_year', 'depth_cat']] = df['month_year_depth'].str.extract(r'^(MAX)-([A-Za-z]{3}-\\d{2})-(.+)$')\n",
    "\n",
    "    # Drop 'prefix' if not needed\n",
    "    df.drop(columns='prefix', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "aba1A_uMvTiy",
    "outputId": "96cc6432-5a13-4f9b-9ae6-de60e4fd765f"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2080_100_45_MAX_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QWhhQQXuvTiy"
   },
   "outputs": [],
   "source": [
    "# Define your DataFrames with their corresponding years\n",
    "year_df_map = {\n",
    "    2020: Gemeinde_2020_100_45_MAX_long,\n",
    "    2030: Gemeinde_2030_100_45_MAX_long,\n",
    "    2050: Gemeinde_2050_100_45_MAX_long,\n",
    "    2080: Gemeinde_2080_100_45_MAX_long\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame\n",
    "for year, df in year_df_map.items():\n",
    "\n",
    "    # Filter to only Jan-25\n",
    "    jan25_df = df[df['month_year'] == 'Jan-25'].reset_index(drop=True)\n",
    "\n",
    "    # Set the new global variable\n",
    "    globals()[f'Gemeinde_{year}_100_45_MAX_Jan25'] = jan25_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtBNHSEZvTiy"
   },
   "outputs": [],
   "source": [
    "# Second part: group and create *_settle DataFrames\n",
    "for year in [2020, 2030, 2050, 2080]:\n",
    "    jan25_var_name = f'Gemeinde_{year}_100_45_MAX_Jan25'\n",
    "\n",
    "    if jan25_var_name in globals():\n",
    "        jan25_df = globals()[jan25_var_name]\n",
    "\n",
    "        # Group and sum by 'id' and 'month_year'\n",
    "        settle_df = jan25_df.groupby(['id', 'month_year'], as_index=False)['MAX_area'].sum()\n",
    "        settle_df.columns = ['id', 'month_year', 'settle_area']\n",
    "        settle_df['settle_area'] = settle_df['settle_area'].astype(float)\n",
    "\n",
    "        # Save to a new global variable\n",
    "        globals()[f'{jan25_var_name}_settle'] = settle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-N5iJ0cBvTiy"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    long_df = globals()[f'Gemeinde_{y}_100_45_MAX_Jan25']\n",
    "    settle_df = globals()[f'Gemeinde_{y}_100_45_MAX_Jan25_settle']\n",
    "\n",
    "    merged_df = pd.merge(long_df, settle_df, on=['id', 'month_year'], how='left')\n",
    "\n",
    "    globals()[f'Gemeinde_{y}_100_45_MAX_Jan25'] = merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZqOJmimmvTiy"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    df = globals()[f'Gemeinde_{y}_100_45_MAX_Jan25']\n",
    "    globals()[f'Gemeinde_{y}_100_45_MAX_Jan25'] = df\n",
    "    df['settle_area_non_zero'] = df.settle_area.replace(0, 1e-6)\n",
    "    globals()[f'Gemeinde_{y}_100_45_MAX_Jan25']['settle_area_non_zero'] = df.settle_area_non_zero\n",
    "    globals()[f'Gemeinde_{y}_100_45_MAX_Jan25']['depth_cat_area_pct'] = (df.MAX_area / df.settle_area_non_zero) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6gFv7wXOvTiy"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_45_MAX_Jan25']\n",
    "    base_df = Gemeinde_2020_100_45_MAX_Jan25\n",
    "    base_df[f'diff_{year}'] = current_df.MAX_area - base_df.MAX_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AHv9BrorvTiy"
   },
   "outputs": [],
   "source": [
    "summary = Gemeinde_2020_100_45_MAX_Jan25[['id', 'Gemeinde', 'Kreis', 'Land']].drop_duplicates().copy()\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    diff_col = f'diff_{year}'\n",
    "\n",
    "    pivot = (\n",
    "        Gemeinde_2020_100_45_MAX_Jan25\n",
    "        .groupby(['id', 'depth_cat'])[diff_col]\n",
    "        .sum()\n",
    "        .unstack(fill_value=0)\n",
    "        .add_prefix(f'sum_diff_{year}_')  # e.g., sum_diff_2030_lt0.5\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    summary = summary.merge(pivot, on='id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r72lxDOKvTiy"
   },
   "outputs": [],
   "source": [
    "# Convert all newly added columns to float\n",
    "new_cols = [col for col in summary.columns if col.startswith('sum_diff_')]\n",
    "summary[new_cols] = summary[new_cols].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "88q9oL1IvTiy",
    "outputId": "16f34349-3f6f-4793-d29b-71d6dae4c38e"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_45_MAX_Jan25[Gemeinde_2020_100_45_MAX_Jan25.Gemeinde == 'Allmendingen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYhadJkfvTiy"
   },
   "outputs": [],
   "source": [
    "summary_45 = summary.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ksGNEhKIvTiy"
   },
   "outputs": [],
   "source": [
    "# Step 1: Calculate total MAX_area per id and depth_cat\n",
    "area_pivot = (\n",
    "    Gemeinde_2020_100_45_MAX_Jan25\n",
    "    .groupby(['id', 'depth_cat'])['MAX_area']\n",
    "    .sum()\n",
    "    .unstack(fill_value=0)\n",
    "    .add_prefix('area_')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Step 2: Merge into summary_26\n",
    "summary_45 = summary_45.merge(area_pivot, on='id', how='left')\n",
    "\n",
    "# Step 3: Calculate percentage risk levels using area for each category\n",
    "years = [2030, 2050, 2080]\n",
    "for year in years:\n",
    "    # Absolute difference sums (already calculated earlier)\n",
    "    summary_45[f'high_risk_{year}'] = (\n",
    "        summary_45.get(f'sum_diff_{year}_lt0.5', 0) +\n",
    "        summary_45.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_45.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_45[f'very_high_risk_{year}'] = (\n",
    "        summary_45.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_45.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_45[f'extreme_risk_{year}'] = summary_45.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "\n",
    "    # Denominators from MAX_area sums\n",
    "    total_area = (\n",
    "        summary_45.get('area_lt0.5', 0) +\n",
    "        summary_45.get('area_lt1.5', 0) +\n",
    "        summary_45.get('area_gt1.5', 0)\n",
    "    ).replace(0, 1e-6)\n",
    "\n",
    "    very_high_area = (\n",
    "        summary_45.get('area_lt1.5', 0) +\n",
    "        summary_45.get('area_gt1.5', 0)\n",
    "    ).replace(0, 1e-6)\n",
    "\n",
    "    extreme_area = summary_45.get('area_gt1.5', 0).replace(0, 1e-6)\n",
    "\n",
    "    # Percentage risk levels\n",
    "    summary_45[f'pct_high_risk_{year}'] = (\n",
    "        summary_45[f'high_risk_{year}'] / total_area * 100\n",
    "    ).round(2)\n",
    "\n",
    "    summary_45[f'pct_very_high_risk_{year}'] = (\n",
    "        summary_45[f'very_high_risk_{year}'] / very_high_area * 100\n",
    "    ).round(2)\n",
    "\n",
    "    summary_45[f'pct_extreme_risk_{year}'] = (\n",
    "        summary_45[f'extreme_risk_{year}'] / extreme_area * 100\n",
    "    ).round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iqwQN5q2vTiy",
    "outputId": "c5cc364e-ab94-4cac-fd6f-c30857f1db8b"
   },
   "outputs": [],
   "source": [
    "summary_45.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvwfrDNvJD2U"
   },
   "outputs": [],
   "source": [
    "value_vars = [\n",
    "    'high_risk_2030', 'very_high_risk_2030', 'extreme_risk_2030',\n",
    "    'high_risk_2050', 'very_high_risk_2050', 'extreme_risk_2050',\n",
    "    'high_risk_2080', 'very_high_risk_2080', 'extreme_risk_2080'\n",
    "]\n",
    "\n",
    "# 1. Melt the absolute values\n",
    "summary_diff_long = summary_45.melt(\n",
    "    id_vars=['id', 'Gemeinde', 'Kreis', 'Land'],\n",
    "    value_vars=value_vars,\n",
    "    var_name='risk_year',\n",
    "    value_name='max_diff_2020'\n",
    ")\n",
    "\n",
    "# 2. Melt the percent difference columns\n",
    "summary_pct_long = summary_45.melt(\n",
    "    id_vars=['id', 'Gemeinde', 'Kreis', 'Land'],\n",
    "    value_vars=[f'pct_{v}' for v in value_vars],\n",
    "    var_name='risk_year',\n",
    "    value_name='max_pct_diff_2020'\n",
    ")\n",
    "\n",
    "# 3. Clean `risk_year` for both (they must match for merge)\n",
    "summary_pct_long['risk_year'] = summary_pct_long['risk_year'].str.replace('pct_', '')\n",
    "\n",
    "# 4. Merge the two melted DataFrames\n",
    "summary_45_long = pd.merge(\n",
    "    summary_diff_long,\n",
    "    summary_pct_long,\n",
    "    on=['id', 'Gemeinde', 'Kreis', 'Land', 'risk_year']\n",
    ")\n",
    "\n",
    "# 5. Extract risk level and year\n",
    "summary_45_long[['risk_level', 'year']] = summary_45_long['risk_year'].str.extract(r'(.*)_(\\d{4})')\n",
    "summary_45_long['year'] = summary_45_long['year'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MwBYKUAYnV3"
   },
   "source": [
    "### Include GEM_ID to match with the gemeinden geometries for mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NbVjxGAOhZop"
   },
   "outputs": [],
   "source": [
    "with open(\"max_summary_26_long.pkl\", \"rb\") as f:\n",
    "    summary_26_long = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "ep0AqWzSmhHC",
    "outputId": "77309429-fde1-41cc-fc41-d3cfd3ab213e"
   },
   "outputs": [],
   "source": [
    "# clean comparison of df1 and df2 by 'id'\n",
    "\n",
    "# 1. select and sort consistently\n",
    "df1_clean = summary_45_long[['id', 'Gemeinde', 'Kreis', 'Land']].drop_duplicates()\n",
    "df2_clean = summary_26_long[['id', 'Gemeinde', 'Kreis', 'Land']].drop_duplicates()\n",
    "\n",
    "# 2. drop non-Gemeinde entries (e.g., rows where Gemeinde or Land is missing)\n",
    "df1_clean = df1_clean.dropna(subset=['Gemeinde', 'Land'])\n",
    "df2_clean = df2_clean.dropna(subset=['Gemeinde', 'Land'])\n",
    "\n",
    "# 3. set 'id' as index to ensure alignment by identifier, not row order\n",
    "df1_clean = df1_clean.set_index('id').sort_index()\n",
    "df2_clean = df2_clean.set_index('id').sort_index()\n",
    "\n",
    "# 4. align and find differences\n",
    "# keep only ids present in both datasets\n",
    "common_ids = df1_clean.index.intersection(df2_clean.index)\n",
    "\n",
    "# find true mismatches by comparing all columns for each id\n",
    "diff_mask = ~(df1_clean.loc[common_ids] == df2_clean.loc[common_ids]).all(axis=1)\n",
    "mismatched_rows = pd.concat(\n",
    "    [df1_clean.loc[diff_mask], df2_clean.loc[diff_mask]],\n",
    "    keys=['df1', 'df2']\n",
    ")\n",
    "\n",
    "# 5. print results\n",
    "print(f\"number of mismatched ids: {diff_mask.sum()}\")\n",
    "display(mismatched_rows)\n",
    "\n",
    "# please note that these results are expected and not true mismatches:\n",
    "# the two \"Helgoland\" entries in Pinneberg, Schleswig-holstein were renamed with suffixes \"-1\" and \"-2\"\n",
    "# to enable joining with the geometries.\n",
    "# since there are no settlement areas in these two gemeinden, the results and maps are not affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ts9QqGtDvTiz"
   },
   "outputs": [],
   "source": [
    "summary_45_long = summary_45_long.merge(summary_26_long[['id', 'GEM_ID']].drop_duplicates().copy(), on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iuu8czzZn9Md",
    "outputId": "b1602bbc-8c78-45e6-9be3-c4ec0e370ef3"
   },
   "outputs": [],
   "source": [
    "summary_45_long.GEM_ID.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3zYWzOVRzGm"
   },
   "source": [
    "### Please store the pre-processed files as pickle files, since you’ll need to load them directly later—especially if the session needs to be restarted due to running out of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJomlQR_oDul"
   },
   "outputs": [],
   "source": [
    "summary_45_long.to_pickle(\"max_summary_45_long.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ca5zcvg97aUU"
   },
   "outputs": [],
   "source": [
    "with open(\"max_summary_45_long.pkl\", \"rb\") as f:\n",
    "    summary_45_long = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WaewbysWgZC"
   },
   "source": [
    "### Flooding exposure difference map (future years vs. 2020) - prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lr5TP7ppWgZC"
   },
   "outputs": [],
   "source": [
    "for year in [2030, 2050, 2080]:\n",
    "    globals()[f\"summary_45_above_015_{year}\"] = summary_45_long[\n",
    "        (summary_45_long['risk_level'] == 'high_risk') &\n",
    "        (summary_45_long['Gemeinde'] != 'Germany') &\n",
    "        (summary_45_long['year'] == year)\n",
    "    ].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hDgA2l4b59y"
   },
   "outputs": [],
   "source": [
    "admin3_boundary = gpd.read_file('/content/admin-GIS/vg250_01-01.gk3.shape.ebenen/vg250_ebenen_0101/VG250_GEM.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7-jUtx3WgZD"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    summary_45_above_015_df = globals()[f'summary_45_above_015_{y}']\n",
    "\n",
    "    merged_gdf = admin3_boundary.merge(summary_45_above_015_df, left_on='OBJID', right_on='GEM_ID', how='right')\n",
    "\n",
    "    globals()[f'summary_45_above_015_{y}_gdf'] = merged_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oprvb7ieWgZD",
    "outputId": "97f33760-37fe-4369-d69e-799f158c40ff"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    summary_45_above_015_gdf = globals()[f'summary_45_above_015_{y}_gdf']\n",
    "    print(f\"Number of null geometries in summary_45_above_015_{y}_gdf: {len(summary_45_above_015_gdf[summary_45_above_015_gdf.geometry.isna()])}\")\n",
    "    print(f\"Number of unique geometries in summary_45_above_015_{y}_gdf: {len(summary_45_above_015_gdf.geometry.unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IKqdQ3apJ-dC",
    "outputId": "223015b1-0401-46b9-e186-84f3c7bec252"
   },
   "outputs": [],
   "source": [
    "# Summary statistics for table in appendix\n",
    "years = [2030, 2050, 2080]\n",
    "results = {}\n",
    "\n",
    "for year in years:\n",
    "    df_name = f\"summary_45_above_015_{year}_gdf\"\n",
    "    df = globals()[df_name]   # fetch DataFrame by name\n",
    "\n",
    "    positive_sum = df.loc[df.max_diff_2020 > 0].max_diff_2020.sum().round(2)\n",
    "    negative_sum = df.loc[df.max_diff_2020 < 0].max_diff_2020.sum().round(2)\n",
    "\n",
    "    results[year] = {\"positive_sum\": positive_sum, \"negative_sum\": negative_sum}\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDsMwRfxK7CW"
   },
   "source": [
    "### Percentage difference map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1ueo5hXi5veF",
    "outputId": "34c47956-d14f-42ae-f678-ebf28a9a62d1"
   },
   "outputs": [],
   "source": [
    "# Years to plot\n",
    "years = [2030, 2050, 2080]\n",
    "save_path = \"/content/drive/MyDrive/Germany_Flood_Study/Gemeinde_climate_scenario_maps/final_adjustments\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "for y in years:\n",
    "    gdf = globals()[f'summary_45_above_015_{y}_gdf'].copy()\n",
    "    gdf = gdf.to_crs(epsg=3857)\n",
    "\n",
    "    # Mask 0 values for transparency\n",
    "    gdf['masked_pct_diff'] = gdf['max_pct_diff_2020'].replace(0, np.nan)\n",
    "\n",
    "    # Colors & breaks\n",
    "    custom_colors = [\n",
    "        '#40e0d0',  # < -10%\n",
    "        '#00ffff',  # -10% ~ -1%\n",
    "        '#808080',  # -1% ~ 1%\n",
    "        '#fee5d9',  # 1% ~ 10%\n",
    "        '#fb6a4a',  # 10% ~ 50%\n",
    "        '#de2d26',  # 50% ~ 100%\n",
    "        '#a50f15'   # > 100%\n",
    "    ]\n",
    "    custom_breaks = [-np.inf, -10, -1, 1, 10, 50, 100, np.inf]\n",
    "\n",
    "    cmap = ListedColormap(custom_colors)\n",
    "    norm = BoundaryNorm(custom_breaks, ncolors=len(custom_colors), clip=False)\n",
    "\n",
    "    # Create figure and axis INSIDE loop\n",
    "    fig, ax = plt.subplots(figsize=(14, 12))\n",
    "\n",
    "    # Plot\n",
    "    gdf.plot(\n",
    "        ax=ax,\n",
    "        column='masked_pct_diff',\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        edgecolor=None,\n",
    "        linewidth=0,\n",
    "        legend=False\n",
    "    )\n",
    "\n",
    "    # Overlay geometry boundaries\n",
    "    gdf.boundary.plot(ax=ax, color='grey', linewidth=0.05)\n",
    "\n",
    "    # Basemap\n",
    "    ctx.add_basemap(ax, source=ctx.providers.CartoDB.PositronNoLabels, crs=gdf.crs)\n",
    "\n",
    "    # Cleanup\n",
    "    ax.axis('off')\n",
    "\n",
    "    # North arrow & scale bar\n",
    "    add_north_arrow(ax, scale=0.75, xlim_pos=0.9025, ylim_pos=0.9,\n",
    "                    color='black', text_scaler=4, text_yT=-1.25)\n",
    "    scale1 = ScaleBar(dx=1, location=\"lower right\", scale_loc=\"bottom\")\n",
    "    ax.add_artist(scale1)\n",
    "\n",
    "    # Custom legend\n",
    "    legend_labels = [\n",
    "        \"<-10%\", \"-10% ~ -1%\",\n",
    "        \"-1% ~ 1%\",\n",
    "        \"1% ~ 10%\", \"10% ~ 50%\", \"50% ~ 100%\", \">100%\"\n",
    "    ]\n",
    "    circles = [\n",
    "        Line2D([0], [0], marker='o', color='None',\n",
    "              markerfacecolor=custom_colors[i],\n",
    "              markeredgecolor='None',\n",
    "              markersize=10,\n",
    "              label=legend_labels[i])\n",
    "        for i in range(len(legend_labels))\n",
    "    ]\n",
    "    ax.legend(\n",
    "        handles=circles,\n",
    "        title=\"Exposure change\",\n",
    "        loc='upper left',\n",
    "        fontsize=12,\n",
    "        title_fontsize=14\n",
    "    )\n",
    "\n",
    "    # Save\n",
    "    file_name = f\"gemeinde_exposure_pct_change_high_risk_45_{y}yr_separate_dry_wet_cyansv2.png\"\n",
    "    full_path = os.path.join(save_path, file_name)\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(full_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    # plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSvjRjh1f-f4"
   },
   "source": [
    "## Coastal flooding undefended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuUsHaYpf-f4"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LZKnNKff-f5"
   },
   "outputs": [],
   "source": [
    "# List of years to process\n",
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "# Columns you want to select\n",
    "cu_columns = ['id', 'Gemeinde', 'Kreis', 'Land'] + [col for col in all_columns if col.startswith('CU')]\n",
    "\n",
    "# Loop through each year and modify the corresponding DataFrame\n",
    "for year in years:\n",
    "    if year == 2020:\n",
    "        df_name = f'Gemeinde_{year}_100_raw'  # 2020 uses '_100_raw'\n",
    "    else:\n",
    "        df_name = f'Gemeinde_{year}_100_45_raw'  # 2030, 2050, and 2080 use '_100_85_raw'\n",
    "\n",
    "    # Check if the DataFrame exists before proceeding\n",
    "    if df_name in globals():\n",
    "        # Get the raw DataFrame and select the relevant columns for max flooding data\n",
    "        df_raw = globals()[df_name]\n",
    "        globals()[f'Gemeinde_{year}_100_45_CU'] = df_raw[cu_columns]\n",
    "    else:\n",
    "        print(f'{df_name} does not exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ms-028QIf-f5"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    globals()[f'Gemeinde_{y}_100_45_CU_long'] = globals()[f'Gemeinde_{y}_100_45_CU'].melt(\n",
    "        id_vars=['id', 'Gemeinde', 'Kreis', 'Land'], var_name='month_year_depth', value_name='CU_area'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mvwy03mTf-f5",
    "outputId": "2f7d5c02-9bab-4588-9c2d-cc775cad7a49"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_45_CU_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liLlZ3fOf-f5"
   },
   "outputs": [],
   "source": [
    "for df in [Gemeinde_2020_100_45_CU_long, Gemeinde_2030_100_45_CU_long, Gemeinde_2050_100_45_CU_long, Gemeinde_2080_100_45_CU_long]:\n",
    "\n",
    "    # Split the 'month_year_depth' column into parts\n",
    "    df[['prefix', 'month_year', 'depth_cat']] = df['month_year_depth'].str.extract(r'^(CU)-([A-Za-z]{3}-\\d{2})-(.+)$')\n",
    "\n",
    "    # Drop 'prefix' if not needed\n",
    "    df.drop(columns='prefix', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kb-x64wXf-f5",
    "outputId": "ec51f4a4-111f-46e1-a594-f240db1506a9"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2080_100_45_CU_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oIXbQ-rKf-f6"
   },
   "outputs": [],
   "source": [
    "# Define your DataFrames with their corresponding years\n",
    "year_df_map = {\n",
    "    2020: Gemeinde_2020_100_45_CU_long,\n",
    "    2030: Gemeinde_2030_100_45_CU_long,\n",
    "    2050: Gemeinde_2050_100_45_CU_long,\n",
    "    2080: Gemeinde_2080_100_45_CU_long\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame\n",
    "for year, df in year_df_map.items():\n",
    "\n",
    "    # Filter to only Jan-25\n",
    "    jan25_df = df[df['month_year'] == 'Jan-25'].reset_index(drop=True)\n",
    "\n",
    "    # Set the new global variable\n",
    "    globals()[f'Gemeinde_{year}_100_45_CU_Jan25'] = jan25_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZC0HbaRff-f6"
   },
   "outputs": [],
   "source": [
    "# Second part: group and create *_settle DataFrames\n",
    "for year in [2020, 2030, 2050, 2080]:\n",
    "    jan25_var_name = f'Gemeinde_{year}_100_45_CU_Jan25'\n",
    "\n",
    "    if jan25_var_name in globals():\n",
    "        jan25_df = globals()[jan25_var_name]\n",
    "\n",
    "        # Group and sum by 'id' and 'month_year'\n",
    "        settle_df = jan25_df.groupby(['id', 'month_year'], as_index=False)['CU_area'].sum()\n",
    "        settle_df.columns = ['id', 'month_year', 'settle_area']\n",
    "        settle_df['settle_area'] = settle_df['settle_area'].astype(float)\n",
    "\n",
    "        # Save to a new global variable\n",
    "        globals()[f'{jan25_var_name}_settle'] = settle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPFwrG2Of-f6"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    long_df = globals()[f'Gemeinde_{y}_100_45_CU_Jan25']\n",
    "    settle_df = globals()[f'Gemeinde_{y}_100_45_CU_Jan25_settle']\n",
    "\n",
    "    merged_df = pd.merge(long_df, settle_df, on=['id', 'month_year'], how='left')\n",
    "\n",
    "    globals()[f'Gemeinde_{y}_100_45_CU_Jan25'] = merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebI3G6y9f-f6"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    df = globals()[f'Gemeinde_{y}_100_45_CU_Jan25']\n",
    "    globals()[f'Gemeinde_{y}_100_45_CU_Jan25'] = df\n",
    "    df['settle_area_non_zero'] = df.settle_area.replace(0, 1e-6)\n",
    "    globals()[f'Gemeinde_{y}_100_45_CU_Jan25']['settle_area_non_zero'] = df.settle_area_non_zero\n",
    "    globals()[f'Gemeinde_{y}_100_45_CU_Jan25']['depth_cat_area_pct'] = (df.CU_area / df.settle_area_non_zero) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m7EpZHHaf-f6",
    "outputId": "24558260-eee1-4f57-dd90-4a11f31aa6a2"
   },
   "outputs": [],
   "source": [
    "cols = ['id', 'Gemeinde', 'Kreis', 'Land']\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "base_df = Gemeinde_2020_100_45_CU_Jan25\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_45_CU_Jan25']\n",
    "\n",
    "    # Direct comparison of the column values (row by row, in order)\n",
    "    is_same = base_df[cols].equals(current_df[cols])\n",
    "\n",
    "    if is_same:\n",
    "        print(f\"✅ Columns match exactly for year {year}\")\n",
    "    else:\n",
    "        print(f\"❌ Columns differ for year {year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SESahEYvf-f6"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_45_CU_Jan25']\n",
    "    base_df = Gemeinde_2020_100_45_CU_Jan25\n",
    "    base_df[f'diff_{year}'] = current_df.CU_area - base_df.CU_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1Emeo2Nf-f6"
   },
   "outputs": [],
   "source": [
    "summary = Gemeinde_2020_100_45_CU_Jan25[['id', 'Gemeinde', 'Kreis', 'Land']].drop_duplicates().copy()\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    diff_col = f'diff_{year}'\n",
    "\n",
    "    pivot = (\n",
    "        Gemeinde_2020_100_45_CU_Jan25\n",
    "        .groupby(['id', 'depth_cat'])[diff_col]\n",
    "        .sum()\n",
    "        .unstack(fill_value=0)\n",
    "        .add_prefix(f'sum_diff_{year}_')  # e.g., sum_diff_2030_lt0.5\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    summary = summary.merge(pivot, on='id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fR_4Gfd0f-f6"
   },
   "outputs": [],
   "source": [
    "# Convert all newly added columns to float\n",
    "new_cols = [col for col in summary.columns if col.startswith('sum_diff_')]\n",
    "summary[new_cols] = summary[new_cols].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3rkD4LIQf-f7",
    "outputId": "47c5f4ca-c663-429f-9503-d3d8147d5b37"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_45_CU_Jan25[Gemeinde_2020_100_45_CU_Jan25.Gemeinde == 'Allmendingen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HyKOO5uZf-f7"
   },
   "outputs": [],
   "source": [
    "summary_45 = summary.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "myAsFfsjf-f7"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    summary_45[f'high_risk_{year}'] = (\n",
    "        summary_45.get(f'sum_diff_{year}_lt0.5', 0) +\n",
    "        summary_45.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_45.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_45[f'very_high_risk_{year}'] = (\n",
    "        summary_45.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_45.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_45[f'extreme_risk_{year}'] = summary_45.get(f'sum_diff_{year}_gt1.5', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mqlzLu2Of-f7",
    "outputId": "70ee2286-8496-4ced-cab7-d9bdeb5479bc"
   },
   "outputs": [],
   "source": [
    "summary_45.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8a3sd7Df-f7"
   },
   "outputs": [],
   "source": [
    "value_vars = [\n",
    "    'high_risk_2030', 'very_high_risk_2030', 'extreme_risk_2030',\n",
    "    'high_risk_2050', 'very_high_risk_2050', 'extreme_risk_2050',\n",
    "    'high_risk_2080', 'very_high_risk_2080', 'extreme_risk_2080'\n",
    "]\n",
    "\n",
    "cu_summary_45_long = summary_45.melt(\n",
    "    id_vars=['id', 'Gemeinde', 'Kreis', 'Land'],  # keep these\n",
    "    value_vars=value_vars,\n",
    "    var_name='risk_year',\n",
    "    value_name='cu_diff_2020'\n",
    ")\n",
    "\n",
    "# Split the combined column into risk_level and year\n",
    "cu_summary_45_long[['risk_level', 'year']] = cu_summary_45_long['risk_year'].str.extract(r'(.*)_(\\d{4})')\n",
    "cu_summary_45_long['year'] = cu_summary_45_long['year'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BU535nyof-f7"
   },
   "source": [
    "### Please store the pre-processed files as pickle files, since you’ll need to load them directly later—especially if the session needs to be restarted due to running out of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9-7NNftf-f7"
   },
   "outputs": [],
   "source": [
    "cu_summary_45_long.to_pickle(\"cu_summary_45_long.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uyoB-x1z6nmi"
   },
   "outputs": [],
   "source": [
    "with open('cu_summary_45_long.pkl', 'rb') as f:\n",
    "    cu_summary_45_long = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCmhoHmv6oQR"
   },
   "source": [
    "## Fluvial flooding undefended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzasnAYf6oQS"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_4XMX3n6oQS"
   },
   "outputs": [],
   "source": [
    "# List of years to process\n",
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "# Columns you want to select (CU columns and Land)\n",
    "fu_columns = ['id', 'Gemeinde', 'Kreis', 'Land'] + [col for col in all_columns]\n",
    "\n",
    "# Loop through each year and modify the corresponding DataFrame\n",
    "for year in years:\n",
    "    if year == 2020:\n",
    "        df_name = f'Gemeinde_{year}_100_raw'  # 2020 uses '_100_raw'\n",
    "    else:\n",
    "        df_name = f'Gemeinde_{year}_100_45_raw'  # 2030, 2050, and 2080 use '_100_85_raw'\n",
    "\n",
    "    # Check if the DataFrame exists before proceeding\n",
    "    if df_name in globals():\n",
    "        # Get the raw DataFrame and select the relevant columns for max flooding data\n",
    "        df_raw = globals()[df_name]\n",
    "        globals()[f'Gemeinde_{year}_100_45_FU'] = df_raw[fu_columns]\n",
    "    else:\n",
    "        print(f'{df_name} does not exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYsuZSqU6oQS"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    globals()[f'Gemeinde_{y}_100_45_FU_long'] = globals()[f'Gemeinde_{y}_100_45_FU'].melt(\n",
    "        id_vars=['id', 'Gemeinde', 'Kreis', 'Land'], var_name='month_year_depth', value_name='FU_area'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "IXGwLecm6oQS",
    "outputId": "174d6677-37ed-44f1-a89c-fb0973a21a21"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_45_FU_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvbX2Ptd6oQT"
   },
   "outputs": [],
   "source": [
    "for df in [Gemeinde_2020_100_45_FU_long, Gemeinde_2030_100_45_FU_long, Gemeinde_2050_100_45_FU_long, Gemeinde_2080_100_45_FU_long]:\n",
    "\n",
    "    # Split the 'month_year_depth' column into parts\n",
    "    df[['prefix', 'month_year', 'depth_cat']] = df['month_year_depth'].str.extract(r'^(FU)-([A-Za-z]{3}-\\d{2})-(.+)$')\n",
    "\n",
    "    # Drop 'prefix' if not needed\n",
    "    df.drop(columns='prefix', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "pL1d2jAI6oQT",
    "outputId": "a7f4a401-4f88-4519-fb84-c9f7cf30831e"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2080_100_45_FU_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWuWNZ876oQT"
   },
   "outputs": [],
   "source": [
    "# Define your DataFrames with their corresponding years\n",
    "year_df_map = {\n",
    "    2020: Gemeinde_2020_100_45_FU_long,\n",
    "    2030: Gemeinde_2030_100_45_FU_long,\n",
    "    2050: Gemeinde_2050_100_45_FU_long,\n",
    "    2080: Gemeinde_2080_100_45_FU_long\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame\n",
    "for year, df in year_df_map.items():\n",
    "\n",
    "    # Filter to only Jan-25\n",
    "    jan25_df = df[df['month_year'] == 'Jan-25'].reset_index(drop=True)\n",
    "\n",
    "    # Set the new global variable\n",
    "    globals()[f'Gemeinde_{year}_100_45_FU_Jan25'] = jan25_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKLlmnw26oQT"
   },
   "outputs": [],
   "source": [
    "# Second part: group and create *_settle DataFrames\n",
    "for year in [2020, 2030, 2050, 2080]:\n",
    "    jan25_var_name = f'Gemeinde_{year}_100_45_FU_Jan25'\n",
    "\n",
    "    if jan25_var_name in globals():\n",
    "        jan25_df = globals()[jan25_var_name]\n",
    "\n",
    "        # Group and sum by 'id' and 'month_year'\n",
    "        settle_df = jan25_df.groupby(['id', 'month_year'], as_index=False)['FU_area'].sum()\n",
    "        settle_df.columns = ['id', 'month_year', 'settle_area']\n",
    "        settle_df['settle_area'] = settle_df['settle_area'].astype(float)\n",
    "\n",
    "        # Save to a new global variable\n",
    "        globals()[f'{jan25_var_name}_settle'] = settle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FfDljLw6oQU"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    long_df = globals()[f'Gemeinde_{y}_100_45_FU_Jan25']\n",
    "    settle_df = globals()[f'Gemeinde_{y}_100_45_FU_Jan25_settle']\n",
    "\n",
    "    merged_df = pd.merge(long_df, settle_df, on=['id', 'month_year'], how='left')\n",
    "\n",
    "    globals()[f'Gemeinde_{y}_100_45_FU_Jan25'] = merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rC8QzHPY6oQU"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    df = globals()[f'Gemeinde_{y}_100_45_FU_Jan25']\n",
    "    globals()[f'Gemeinde_{y}_100_45_FU_Jan25'] = df\n",
    "    df['settle_area_non_zero'] = df.settle_area.replace(0, 1e-6)\n",
    "    globals()[f'Gemeinde_{y}_100_45_FU_Jan25']['settle_area_non_zero'] = df.settle_area_non_zero\n",
    "    globals()[f'Gemeinde_{y}_100_45_FU_Jan25']['depth_cat_area_pct'] = (df.FU_area / df.settle_area_non_zero) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UipWPmZr6oQU",
    "outputId": "151cc5af-8e68-49ab-8089-9e8ce277936d"
   },
   "outputs": [],
   "source": [
    "cols = ['id', 'Gemeinde', 'Kreis', 'Land']\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "base_df = Gemeinde_2020_100_45_FU_Jan25\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_45_FU_Jan25']\n",
    "\n",
    "    # Direct comparison of the column values (row by row, in order)\n",
    "    is_same = base_df[cols].equals(current_df[cols])\n",
    "\n",
    "    if is_same:\n",
    "        print(f\"✅ Columns match exactly for year {year}\")\n",
    "    else:\n",
    "        print(f\"❌ Columns differ for year {year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bIFn292G6oQV"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_45_FU_Jan25']\n",
    "    base_df = Gemeinde_2020_100_45_FU_Jan25\n",
    "    base_df[f'diff_{year}'] = current_df.FU_area - base_df.FU_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5Tfgm9O6oQV"
   },
   "outputs": [],
   "source": [
    "summary = Gemeinde_2020_100_45_FU_Jan25[['id', 'Gemeinde', 'Kreis', 'Land']].drop_duplicates().copy()\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    diff_col = f'diff_{year}'\n",
    "\n",
    "    pivot = (\n",
    "        Gemeinde_2020_100_45_FU_Jan25\n",
    "        .groupby(['id', 'depth_cat'])[diff_col]\n",
    "        .sum()\n",
    "        .unstack(fill_value=0)\n",
    "        .add_prefix(f'sum_diff_{year}_')  # e.g., sum_diff_2030_lt0.5\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    summary = summary.merge(pivot, on='id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7EvLX3Ue6oQV"
   },
   "outputs": [],
   "source": [
    "# Convert all newly added columns to float\n",
    "new_cols = [col for col in summary.columns if col.startswith('sum_diff_')]\n",
    "summary[new_cols] = summary[new_cols].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpnYF_om6oQW"
   },
   "outputs": [],
   "source": [
    "summary_45 = summary.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6agR33is6oQW"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    summary_45[f'high_risk_{year}'] = (\n",
    "        summary_45.get(f'sum_diff_{year}_lt0.5', 0) +\n",
    "        summary_45.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_45.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_45[f'very_high_risk_{year}'] = (\n",
    "        summary_45.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_45.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_45[f'extreme_risk_{year}'] = summary_45.get(f'sum_diff_{year}_gt1.5', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JhrzF8Ce6oQW",
    "outputId": "df7f440f-a45a-4887-9dbe-6f4119471529"
   },
   "outputs": [],
   "source": [
    "summary_45.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9573TNW6oQW"
   },
   "outputs": [],
   "source": [
    "value_vars = [\n",
    "    'high_risk_2030', 'very_high_risk_2030', 'extreme_risk_2030',\n",
    "    'high_risk_2050', 'very_high_risk_2050', 'extreme_risk_2050',\n",
    "    'high_risk_2080', 'very_high_risk_2080', 'extreme_risk_2080'\n",
    "]\n",
    "\n",
    "fu_summary_45_long = summary_45.melt(\n",
    "    id_vars=['id', 'Gemeinde', 'Kreis', 'Land'],  # keep these\n",
    "    value_vars=value_vars,\n",
    "    var_name='risk_year',\n",
    "    value_name='fu_diff_2020'\n",
    ")\n",
    "\n",
    "# Split the combined column into risk_level and year\n",
    "fu_summary_45_long[['risk_level', 'year']] = fu_summary_45_long['risk_year'].str.extract(r'(.*)_(\\d{4})')\n",
    "fu_summary_45_long['year'] = fu_summary_45_long['year'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0oJZa3f6oQX"
   },
   "source": [
    "### Please store the pre-processed files as pickle files, since you’ll need to load them directly later—especially if the session needs to be restarted due to running out of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jW9-flWv6oQX"
   },
   "outputs": [],
   "source": [
    "fu_summary_45_long.to_pickle(\"fu_summary_45_long.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xswG1p5kuUjD"
   },
   "outputs": [],
   "source": [
    "with open('fu_summary_45_long.pkl', 'rb') as f:\n",
    "    fu_summary_45_long = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulho6HQGMaoY"
   },
   "source": [
    "## Pluvial flooding defended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6APOSD0MaoY"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOoBytPfMaoZ"
   },
   "outputs": [],
   "source": [
    "# List of years to process\n",
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "# Columns you want to select (CU columns and Land)\n",
    "pd_columns = ['id', 'Gemeinde', 'Kreis', 'Land'] + [col for col in all_columns if col.startswith('PD')]\n",
    "\n",
    "# Loop through each year and modify the corresponding DataFrame\n",
    "for year in years:\n",
    "    if year == 2020:\n",
    "        df_name = f'Gemeinde_{year}_100_raw'  # 2020 uses '_100_raw'\n",
    "    else:\n",
    "        df_name = f'Gemeinde_{year}_100_45_raw'  # 2030, 2050, and 2080 use '_100_85_raw'\n",
    "\n",
    "    # Check if the DataFrame exists before proceeding\n",
    "    if df_name in globals():\n",
    "        # Get the raw DataFrame and select the relevant columns for max flooding data\n",
    "        df_raw = globals()[df_name]\n",
    "        globals()[f'Gemeinde_{year}_100_45_PD'] = df_raw[pd_columns]\n",
    "    else:\n",
    "        print(f'{df_name} does not exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnzP-IRYMaoZ"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    globals()[f'Gemeinde_{y}_100_45_PD_long'] = globals()[f'Gemeinde_{y}_100_45_PD'].melt(\n",
    "        id_vars=['id', 'Gemeinde', 'Kreis', 'Land'], var_name='month_year_depth', value_name='PD_area'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "YtSZVWV8MaoZ",
    "outputId": "6e493872-2588-44e0-cfbb-abc8da232f60"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_45_PD_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRxeHGhOMaoZ"
   },
   "outputs": [],
   "source": [
    "for df in [Gemeinde_2020_100_45_PD_long, Gemeinde_2030_100_45_PD_long, Gemeinde_2050_100_45_PD_long, Gemeinde_2080_100_45_PD_long]:\n",
    "\n",
    "    # Split the 'month_year_depth' column into parts\n",
    "    df[['prefix', 'month_year', 'depth_cat']] = df['month_year_depth'].str.extract(r'^(PD)-([A-Za-z]{3}-\\d{2})-(.+)$')\n",
    "\n",
    "    # Drop 'prefix' if not needed\n",
    "    df.drop(columns='prefix', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "sv3v6NnmMaoZ",
    "outputId": "9a8da107-71d7-4e94-a82b-3cdde7c921ae"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2080_100_45_PD_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5D3ryq7RMaoa"
   },
   "outputs": [],
   "source": [
    "# Define your DataFrames with their corresponding years\n",
    "year_df_map = {\n",
    "    2020: Gemeinde_2020_100_45_PD_long,\n",
    "    2030: Gemeinde_2030_100_45_PD_long,\n",
    "    2050: Gemeinde_2050_100_45_PD_long,\n",
    "    2080: Gemeinde_2080_100_45_PD_long\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame\n",
    "for year, df in year_df_map.items():\n",
    "\n",
    "    # Filter to only Jan-25\n",
    "    jan25_df = df[df['month_year'] == 'Jan-25'].reset_index(drop=True)\n",
    "\n",
    "    # Set the new global variable\n",
    "    globals()[f'Gemeinde_{year}_100_45_PD_Jan25'] = jan25_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SF8DKh4YMaoa"
   },
   "outputs": [],
   "source": [
    "# Second part: group and create *_settle DataFrames\n",
    "for year in [2020, 2030, 2050, 2080]:\n",
    "    jan25_var_name = f'Gemeinde_{year}_100_45_PD_Jan25'\n",
    "\n",
    "    if jan25_var_name in globals():\n",
    "        jan25_df = globals()[jan25_var_name]\n",
    "\n",
    "        # Group and sum by 'id' and 'month_year'\n",
    "        settle_df = jan25_df.groupby(['id', 'month_year'], as_index=False)['PD_area'].sum()\n",
    "        settle_df.columns = ['id', 'month_year', 'settle_area']\n",
    "        settle_df['settle_area'] = settle_df['settle_area'].astype(float)\n",
    "\n",
    "        # Save to a new global variable\n",
    "        globals()[f'{jan25_var_name}_settle'] = settle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIaFqieZMaoa"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    long_df = globals()[f'Gemeinde_{y}_100_45_PD_Jan25']\n",
    "    settle_df = globals()[f'Gemeinde_{y}_100_45_PD_Jan25_settle']\n",
    "\n",
    "    merged_df = pd.merge(long_df, settle_df, on=['id', 'month_year'], how='left')\n",
    "\n",
    "    globals()[f'Gemeinde_{y}_100_45_PD_Jan25'] = merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whek5DzEMaoa"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    df = globals()[f'Gemeinde_{y}_100_45_PD_Jan25']\n",
    "    globals()[f'Gemeinde_{y}_100_45_PD_Jan25'] = df\n",
    "    df['settle_area_non_zero'] = df.settle_area.replace(0, 1e-6)\n",
    "    globals()[f'Gemeinde_{y}_100_45_PD_Jan25']['settle_area_non_zero'] = df.settle_area_non_zero\n",
    "    globals()[f'Gemeinde_{y}_100_45_PD_Jan25']['depth_cat_area_pct'] = (df.PD_area / df.settle_area_non_zero) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HvTq5pH-Maoa",
    "outputId": "f3b91fc0-b1df-4310-d912-2db6dd1497ae"
   },
   "outputs": [],
   "source": [
    "cols = ['id', 'Gemeinde', 'Kreis', 'Land']\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "base_df = Gemeinde_2020_100_45_PD_Jan25\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_45_PD_Jan25']\n",
    "\n",
    "    # Direct comparison of the column values (row by row, in order)\n",
    "    is_same = base_df[cols].equals(current_df[cols])\n",
    "\n",
    "    if is_same:\n",
    "        print(f\"✅ Columns match exactly for year {year}\")\n",
    "    else:\n",
    "        print(f\"❌ Columns differ for year {year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUpjyZZeMaoa"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_45_PD_Jan25']\n",
    "    base_df = Gemeinde_2020_100_45_PD_Jan25\n",
    "    base_df[f'diff_{year}'] = current_df.PD_area - base_df.PD_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGM5R5vgMaoa"
   },
   "outputs": [],
   "source": [
    "summary = Gemeinde_2020_100_45_PD_Jan25[['id', 'Gemeinde', 'Kreis', 'Land']].drop_duplicates().copy()\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    diff_col = f'diff_{year}'\n",
    "\n",
    "    pivot = (\n",
    "        Gemeinde_2020_100_45_PD_Jan25\n",
    "        .groupby(['id', 'depth_cat'])[diff_col]\n",
    "        .sum()\n",
    "        .unstack(fill_value=0)\n",
    "        .add_prefix(f'sum_diff_{year}_')  # e.g., sum_diff_2030_lt0.5\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    summary = summary.merge(pivot, on='id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87hgWWWxMaob"
   },
   "outputs": [],
   "source": [
    "# Convert all newly added columns to float\n",
    "new_cols = [col for col in summary.columns if col.startswith('sum_diff_')]\n",
    "summary[new_cols] = summary[new_cols].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40ddFfobMaob"
   },
   "outputs": [],
   "source": [
    "summary_45 = summary.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uz-UxDyQMaob"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    summary_45[f'high_risk_{year}'] = (\n",
    "        summary_45.get(f'sum_diff_{year}_lt0.5', 0) +\n",
    "        summary_45.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_45.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_45[f'very_high_risk_{year}'] = (\n",
    "        summary_45.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_45.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_45[f'extreme_risk_{year}'] = summary_45.get(f'sum_diff_{year}_gt1.5', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x9nqedsmMaob",
    "outputId": "eb7a6221-03e0-42ed-e0c1-e4edb6e63e02"
   },
   "outputs": [],
   "source": [
    "summary_45.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pf8GCSlCMaob"
   },
   "outputs": [],
   "source": [
    "value_vars = [\n",
    "    'high_risk_2030', 'very_high_risk_2030', 'extreme_risk_2030',\n",
    "    'high_risk_2050', 'very_high_risk_2050', 'extreme_risk_2050',\n",
    "    'high_risk_2080', 'very_high_risk_2080', 'extreme_risk_2080'\n",
    "]\n",
    "\n",
    "pd_summary_45_long = summary_45.melt(\n",
    "    id_vars=['id', 'Gemeinde', 'Kreis', 'Land'],  # keep these\n",
    "    value_vars=value_vars,\n",
    "    var_name='risk_year',\n",
    "    value_name='pd_diff_2020'\n",
    ")\n",
    "\n",
    "# Split the combined column into risk_level and year\n",
    "pd_summary_45_long[['risk_level', 'year']] = pd_summary_45_long['risk_year'].str.extract(r'(.*)_(\\d{4})')\n",
    "pd_summary_45_long['year'] = pd_summary_45_long['year'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nxo1lBaVMaob"
   },
   "source": [
    "### Please store the pre-processed files as pickle files, since you’ll need to load them directly later—especially if the session needs to be restarted due to running out of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zj8UsfoJMaob"
   },
   "outputs": [],
   "source": [
    "pd_summary_45_long.to_pickle(\"pd_summary_45_long.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRTpu5bhMaob"
   },
   "outputs": [],
   "source": [
    "with open(\"pd_summary_45_long.pkl\", \"rb\") as f:\n",
    "    pd_summary_45_long = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6EEfQ1y4qoK"
   },
   "source": [
    "# SSP3-RCP7.0 (Moderate Pessimistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6HfgN4baJxt"
   },
   "source": [
    "### Read files, pre-processing and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bLQUuvCf4qoK",
    "outputId": "0b37c8ff-ceee-4c76-f4db-8a407c15bdfb"
   },
   "outputs": [],
   "source": [
    "!unzip \"/content/moderate_pessimistic.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YPY0xTKc4qoL"
   },
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    # '/content/Germany_Gemeinde_2020_100.xlsx',\n",
    "    '/content/moderate_pessimistic/Germany_Gemeinde_2030_100_70.xlsx',\n",
    "    '/content/moderate_pessimistic/Germany_Gemeinde_2050_100_70.xlsx',\n",
    "    '/content/moderate_pessimistic/Germany_Gemeinde_2080_100_70.xlsx',\n",
    "]\n",
    "\n",
    "dataframes = []\n",
    "for path in file_paths:\n",
    "    df = pd.read_excel(path)\n",
    "    df = df.round(6)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Optionally unpack into variables\n",
    "(Gemeinde_2030_100_70_raw, Gemeinde_2050_100_70_raw, Gemeinde_2080_100_70_raw) = dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-gvfKox4qoL"
   },
   "outputs": [],
   "source": [
    "# Date range: every 6 months\n",
    "dates = pd.date_range(start='2016-07-01', end='2025-01-01', freq='6MS')\n",
    "\n",
    "# Prefixes and suffixes\n",
    "prefixes = ['CU', 'FU', 'PD', 'MAX']\n",
    "suffixes = ['P0', 'lt0.15', 'lt0.5', 'lt1.5', 'gt1.5']\n",
    "\n",
    "# Correct order: by prefix, then suffix, then date\n",
    "all_columns = [\n",
    "    f\"{prefix}-{date.strftime('%b-%y')}-{suffix}\"\n",
    "    for prefix in prefixes\n",
    "    for suffix in suffixes\n",
    "    for date in dates\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "209zFZZU4qoL"
   },
   "outputs": [],
   "source": [
    "# Check if number of replacement columns matches the shape\n",
    "for df in [Gemeinde_2030_100_70_raw, Gemeinde_2050_100_70_raw, Gemeinde_2080_100_70_raw]:\n",
    "    if len(all_columns) == df.shape[1] - 3:\n",
    "      df.columns = list(df.columns[:3]) + all_columns\n",
    "      df.reset_index(drop=True, inplace=True)\n",
    "      df['id'] = df.index\n",
    "    else:\n",
    "      raise ValueError(\"Length of generated column names does not match number of columns (excluding the first one).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "id": "BXcokPKu4qoL",
    "outputId": "b333a4bd-ed22-4e4a-d5cf-bb4b82f04182"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2030_100_70_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4zyKHIf4qoL"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2030_100_70_raw = Gemeinde_2030_100_70_raw[1:]\n",
    "Gemeinde_2050_100_70_raw = Gemeinde_2050_100_70_raw[1:]\n",
    "Gemeinde_2080_100_70_raw = Gemeinde_2080_100_70_raw[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etbkh5114qoL"
   },
   "outputs": [],
   "source": [
    "save_path = \"/content/\"\n",
    "\n",
    "# Make sure the directory exists\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "gemeinde_dfs = [Gemeinde_2020_100_raw, Gemeinde_2030_100_70_raw, Gemeinde_2050_100_70_raw, Gemeinde_2080_100_70_raw]\n",
    "\n",
    "filenames = [\n",
    "    \"Gemeinde_2020_100_raw.pkl\", \"Gemeinde_2030_100_70_raw.pkl\", \"Gemeinde_2050_100_70_raw.pkl\", \"Gemeinde_2080_100_70_raw.pkl\"\n",
    "]\n",
    "\n",
    "for df, filename in zip(gemeinde_dfs, filenames):\n",
    "    df.to_pickle(os.path.join(save_path, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvIAoHXCaOJR"
   },
   "source": [
    "### **PLEASE re-run if the session restarts before you finish processing this climate scenario:** Read and load file from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fe2Ka2uw4qoL"
   },
   "outputs": [],
   "source": [
    "# Define the path where the files are stored\n",
    "save_path = \"/content/\"\n",
    "\n",
    "# List of filenames\n",
    "filenames = [\n",
    "    \"Gemeinde_2020_100_raw.pkl\", \"Gemeinde_2030_100_70_raw.pkl\", \"Gemeinde_2050_100_70_raw.pkl\", \"Gemeinde_2080_100_70_raw.pkl\"\n",
    "]\n",
    "\n",
    "# Load all DataFrames into a list\n",
    "gemeinde_dfs = [pd.read_pickle(os.path.join(save_path, filename)) for filename in filenames]\n",
    "\n",
    "cols_to_convert = [col for col in gemeinde_dfs[0].columns if col not in ['Gemeinde', 'Kreis', 'Land', 'id']]\n",
    "\n",
    "def convert_and_round(df):\n",
    "    df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, errors='coerce').round(6)\n",
    "    return df\n",
    "\n",
    "gemeinde_dfs = [convert_and_round(df) for df in gemeinde_dfs]\n",
    "\n",
    "# Optionally assign them back to individual variable names (if needed)\n",
    "(\n",
    "  Gemeinde_2020_100_raw,\n",
    "  Gemeinde_2030_100_70_raw,\n",
    "  Gemeinde_2050_100_70_raw,\n",
    "  Gemeinde_2080_100_70_raw\n",
    ") = gemeinde_dfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQvTeJCk4qoL"
   },
   "source": [
    "## Combined flooding (max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbRgplAJ4qoL"
   },
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwCgL9kZ4qoL"
   },
   "outputs": [],
   "source": [
    "# List of years to process\n",
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "# Columns you want to select (CU columns and Land)\n",
    "max_columns = ['id', 'Gemeinde', 'Kreis', 'Land'] + [col for col in all_columns if col.startswith('MAX') or col == 'Land']\n",
    "\n",
    "# Loop through each year and modify the corresponding DataFrame\n",
    "for year in years:\n",
    "    if year == 2020:\n",
    "        df_name = f'Gemeinde_{year}_100_raw'  # 2020 uses '_100_raw'\n",
    "    else:\n",
    "        df_name = f'Gemeinde_{year}_100_70_raw'  # 2030, 2050, and 2080 use '_100_85_raw'\n",
    "\n",
    "    # Check if the DataFrame exists before proceeding\n",
    "    if df_name in globals():\n",
    "        # Get the raw DataFrame and select the relevant columns for max flooding data\n",
    "        df_raw = globals()[df_name]\n",
    "        globals()[f'Gemeinde_{year}_100_70_MAX'] = df_raw[max_columns]\n",
    "    else:\n",
    "        print(f'{df_name} does not exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gm2UYk64qoM"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    globals()[f'Gemeinde_{y}_100_70_MAX_long'] = globals()[f'Gemeinde_{y}_100_70_MAX'].melt(\n",
    "        id_vars=['id', 'Gemeinde', 'Kreis', 'Land'], var_name='month_year_depth', value_name='MAX_area'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KFXCUpMH4qoM",
    "outputId": "eaf5abbb-d85d-4d7f-bd75-50cc98c24f4e"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_70_MAX_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QbMZQSC44qoM"
   },
   "outputs": [],
   "source": [
    "for df in [Gemeinde_2020_100_70_MAX_long, Gemeinde_2030_100_70_MAX_long, Gemeinde_2050_100_70_MAX_long, Gemeinde_2080_100_70_MAX_long]:\n",
    "\n",
    "    # Replace 'CU' with 'MAX' if needed (optional step if not already 'MAX')\n",
    "    # df['month_year_depth'] = df['month_year_depth'].str.replace('CU', 'MAX')\n",
    "\n",
    "    # Split the 'month_year_depth' column into parts\n",
    "    df[['prefix', 'month_year', 'depth_cat']] = df['month_year_depth'].str.extract(r'^(MAX)-([A-Za-z]{3}-\\d{2})-(.+)$')\n",
    "\n",
    "    # Drop 'prefix' if not needed\n",
    "    df.drop(columns='prefix', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-GTpFPSk4qoM",
    "outputId": "e6181339-e8da-42e3-c161-e01d7787b4c7"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2080_100_70_MAX_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wlr1U7mn4qoM"
   },
   "outputs": [],
   "source": [
    "# Define your DataFrames with their corresponding years\n",
    "year_df_map = {\n",
    "    2020: Gemeinde_2020_100_70_MAX_long,\n",
    "    2030: Gemeinde_2030_100_70_MAX_long,\n",
    "    2050: Gemeinde_2050_100_70_MAX_long,\n",
    "    2080: Gemeinde_2080_100_70_MAX_long\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame\n",
    "for year, df in year_df_map.items():\n",
    "\n",
    "    # Filter to only Jan-25\n",
    "    jan25_df = df[df['month_year'] == 'Jan-25'].reset_index(drop=True)\n",
    "\n",
    "    # Set the new global variable\n",
    "    globals()[f'Gemeinde_{year}_100_70_MAX_Jan25'] = jan25_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBt9hAaP4qoM"
   },
   "outputs": [],
   "source": [
    "# Second part: group and create *_settle DataFrames\n",
    "for year in [2020, 2030, 2050, 2080]:\n",
    "    jan25_var_name = f'Gemeinde_{year}_100_70_MAX_Jan25'\n",
    "\n",
    "    if jan25_var_name in globals():\n",
    "        jan25_df = globals()[jan25_var_name]\n",
    "\n",
    "        # Group and sum by 'id' and 'month_year'\n",
    "        settle_df = jan25_df.groupby(['id', 'month_year'], as_index=False)['MAX_area'].sum()\n",
    "        settle_df.columns = ['id', 'month_year', 'settle_area']\n",
    "        settle_df['settle_area'] = settle_df['settle_area'].astype(float)\n",
    "\n",
    "        # Save to a new global variable\n",
    "        globals()[f'{jan25_var_name}_settle'] = settle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AHyRoTz-4qoM"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    long_df = globals()[f'Gemeinde_{y}_100_70_MAX_Jan25']\n",
    "    settle_df = globals()[f'Gemeinde_{y}_100_70_MAX_Jan25_settle']\n",
    "\n",
    "    merged_df = pd.merge(long_df, settle_df, on=['id', 'month_year'], how='left')\n",
    "\n",
    "    globals()[f'Gemeinde_{y}_100_70_MAX_Jan25'] = merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6bBWV2K14qoM"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    df = globals()[f'Gemeinde_{y}_100_70_MAX_Jan25']\n",
    "    globals()[f'Gemeinde_{y}_100_70_MAX_Jan25'] = df\n",
    "    df['settle_area_non_zero'] = df.settle_area.replace(0, 1e-6)\n",
    "    globals()[f'Gemeinde_{y}_100_70_MAX_Jan25']['settle_area_non_zero'] = df.settle_area_non_zero\n",
    "    globals()[f'Gemeinde_{y}_100_70_MAX_Jan25']['depth_cat_area_pct'] = (df.MAX_area / df.settle_area_non_zero) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5Iu0y8w4qoM"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_70_MAX_Jan25']\n",
    "    base_df = Gemeinde_2020_100_70_MAX_Jan25\n",
    "    base_df[f'diff_{year}'] = current_df.MAX_area - base_df.MAX_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eu7me70C4qoM"
   },
   "outputs": [],
   "source": [
    "summary = Gemeinde_2020_100_70_MAX_Jan25[['id', 'Gemeinde', 'Kreis', 'Land']].drop_duplicates().copy()\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    diff_col = f'diff_{year}'\n",
    "\n",
    "    pivot = (\n",
    "        Gemeinde_2020_100_70_MAX_Jan25\n",
    "        .groupby(['id', 'depth_cat'])[diff_col]\n",
    "        .sum()\n",
    "        .unstack(fill_value=0)\n",
    "        .add_prefix(f'sum_diff_{year}_')  # e.g., sum_diff_2030_lt0.5\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    summary = summary.merge(pivot, on='id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6o9pAdz94qoM"
   },
   "outputs": [],
   "source": [
    "# Convert all newly added columns to float\n",
    "new_cols = [col for col in summary.columns if col.startswith('sum_diff_')]\n",
    "summary[new_cols] = summary[new_cols].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GsX_AATg4qoM",
    "outputId": "7c38c455-8e35-4d60-8099-b3ef4e8172b7"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_70_MAX_Jan25[Gemeinde_2020_100_70_MAX_Jan25.Gemeinde == 'Allmendingen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBlQ2S4J4qoN"
   },
   "outputs": [],
   "source": [
    "summary_70 = summary.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcfeOSXBKJqF"
   },
   "outputs": [],
   "source": [
    "# Step 1: Calculate total MAX_area per id and depth_cat\n",
    "area_pivot = (\n",
    "    Gemeinde_2020_100_70_MAX_Jan25\n",
    "    .groupby(['id', 'depth_cat'])['MAX_area']\n",
    "    .sum()\n",
    "    .unstack(fill_value=0)\n",
    "    .add_prefix('area_')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Step 2: Merge into summary_26\n",
    "summary_70 = summary_70.merge(area_pivot, on='id', how='left')\n",
    "\n",
    "# Step 3: Calculate percentage risk levels using area for each category\n",
    "years = [2030, 2050, 2080]\n",
    "for year in years:\n",
    "    # Absolute difference sums (already calculated earlier)\n",
    "    summary_70[f'high_risk_{year}'] = (\n",
    "        summary_70.get(f'sum_diff_{year}_lt0.5', 0) +\n",
    "        summary_70.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_70.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_70[f'very_high_risk_{year}'] = (\n",
    "        summary_70.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_70.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_70[f'extreme_risk_{year}'] = summary_70.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "\n",
    "    # Denominators from MAX_area sums\n",
    "    total_area = (\n",
    "        summary_70.get('area_lt0.5', 0) +\n",
    "        summary_70.get('area_lt1.5', 0) +\n",
    "        summary_70.get('area_gt1.5', 0)\n",
    "    ).replace(0, 1e-6)\n",
    "\n",
    "    very_high_area = (\n",
    "        summary_70.get('area_lt1.5', 0) +\n",
    "        summary_70.get('area_gt1.5', 0)\n",
    "    ).replace(0, 1e-6)\n",
    "\n",
    "    extreme_area = summary_70.get('area_gt1.5', 0).replace(0, 1e-6)\n",
    "\n",
    "    # Percentage risk levels\n",
    "    summary_70[f'pct_high_risk_{year}'] = (\n",
    "        summary_70[f'high_risk_{year}'] / total_area * 100\n",
    "    ).round(2)\n",
    "\n",
    "    summary_70[f'pct_very_high_risk_{year}'] = (\n",
    "        summary_70[f'very_high_risk_{year}'] / very_high_area * 100\n",
    "    ).round(2)\n",
    "\n",
    "    summary_70[f'pct_extreme_risk_{year}'] = (\n",
    "        summary_70[f'extreme_risk_{year}'] / extreme_area * 100\n",
    "    ).round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yfrnUvHb4qoN",
    "outputId": "b397e51f-0b2a-47df-d6a3-561ea32bc3bd"
   },
   "outputs": [],
   "source": [
    "summary_70.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJbAu0ZZKsut"
   },
   "outputs": [],
   "source": [
    "value_vars = [\n",
    "    'high_risk_2030', 'very_high_risk_2030', 'extreme_risk_2030',\n",
    "    'high_risk_2050', 'very_high_risk_2050', 'extreme_risk_2050',\n",
    "    'high_risk_2080', 'very_high_risk_2080', 'extreme_risk_2080'\n",
    "]\n",
    "\n",
    "# 1. Melt the absolute values\n",
    "summary_diff_long = summary_70.melt(\n",
    "    id_vars=['id', 'Gemeinde', 'Kreis', 'Land'],\n",
    "    value_vars=value_vars,\n",
    "    var_name='risk_year',\n",
    "    value_name='max_diff_2020'\n",
    ")\n",
    "\n",
    "# 2. Melt the percent difference columns\n",
    "summary_pct_long = summary_70.melt(\n",
    "    id_vars=['id', 'Gemeinde', 'Kreis', 'Land'],\n",
    "    value_vars=[f'pct_{v}' for v in value_vars],\n",
    "    var_name='risk_year',\n",
    "    value_name='max_pct_diff_2020'\n",
    ")\n",
    "\n",
    "# 3. Clean `risk_year` for both (they must match for merge)\n",
    "summary_pct_long['risk_year'] = summary_pct_long['risk_year'].str.replace('pct_', '')\n",
    "\n",
    "# 4. Merge the two melted DataFrames\n",
    "summary_70_long = pd.merge(\n",
    "    summary_diff_long,\n",
    "    summary_pct_long,\n",
    "    on=['id', 'Gemeinde', 'Kreis', 'Land', 'risk_year']\n",
    ")\n",
    "\n",
    "# 5. Extract risk level and year\n",
    "summary_70_long[['risk_level', 'year']] = summary_70_long['risk_year'].str.extract(r'(.*)_(\\d{4})')\n",
    "summary_70_long['year'] = summary_70_long['year'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rhsv646by_f_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hu0rK9fPYwvC"
   },
   "source": [
    "### Include GEM_ID to match with the gemeinden geometries for mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6DUN_7Niydjn"
   },
   "outputs": [],
   "source": [
    "with open(\"max_summary_26_long.pkl\", \"rb\") as f:\n",
    "    summary_26_long = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "_eJKm7qLygVo",
    "outputId": "9a09e590-6788-4697-9534-7fa17d0a46f6"
   },
   "outputs": [],
   "source": [
    "# clean comparison of df1 and df2 by 'id'\n",
    "\n",
    "# 1. select and sort consistently\n",
    "df1_clean = summary_70_long[['id', 'Gemeinde', 'Kreis', 'Land']].drop_duplicates()\n",
    "df2_clean = summary_26_long[['id', 'Gemeinde', 'Kreis', 'Land']].drop_duplicates()\n",
    "\n",
    "# 2. drop non-Gemeinde entries (e.g., rows where Gemeinde or Land is missing)\n",
    "df1_clean = df1_clean.dropna(subset=['Gemeinde', 'Land'])\n",
    "df2_clean = df2_clean.dropna(subset=['Gemeinde', 'Land'])\n",
    "\n",
    "# 3. set 'id' as index to ensure alignment by identifier, not row order\n",
    "df1_clean = df1_clean.set_index('id').sort_index()\n",
    "df2_clean = df2_clean.set_index('id').sort_index()\n",
    "\n",
    "# 4. align and find differences\n",
    "# keep only ids present in both datasets\n",
    "common_ids = df1_clean.index.intersection(df2_clean.index)\n",
    "\n",
    "# find true mismatches by comparing all columns for each id\n",
    "diff_mask = ~(df1_clean.loc[common_ids] == df2_clean.loc[common_ids]).all(axis=1)\n",
    "mismatched_rows = pd.concat(\n",
    "    [df1_clean.loc[diff_mask], df2_clean.loc[diff_mask]],\n",
    "    keys=['df1', 'df2']\n",
    ")\n",
    "\n",
    "# 5. print results\n",
    "print(f\"number of mismatched ids: {diff_mask.sum()}\")\n",
    "display(mismatched_rows)\n",
    "\n",
    "# please note that these results are expected and not true mismatches:\n",
    "# the two \"Helgoland\" entries in Pinneberg, Schleswig-holstein were renamed with suffixes \"-1\" and \"-2\"\n",
    "# to enable joining with the geometries.\n",
    "# since there are no settlement areas in these two gemeinden, the results and maps are not affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8iJSF_UlYwvD"
   },
   "outputs": [],
   "source": [
    "summary_70_long = summary_70_long.merge(summary_26_long[['id', 'GEM_ID']].drop_duplicates().copy(), on='id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ul-_7YPeR9Yw"
   },
   "source": [
    "### Please store the pre-processed files as pickle files, since you’ll need to load them directly later—especially if the session needs to be restarted due to running out of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DJ8ZQfkzXbB"
   },
   "outputs": [],
   "source": [
    "summary_70_long.to_pickle(\"max_summary_70_long.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEtmBn7I7rO8"
   },
   "outputs": [],
   "source": [
    "with open(\"max_summary_70_long.pkl\", \"rb\") as f:\n",
    "    summary_70_long = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VerCJXrHZawU"
   },
   "source": [
    "### Flooding exposure difference map (future years vs. 2020) - prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BvyKYM1TZawV",
    "outputId": "ba483780-54e6-49ba-a113-0ecda5d52394"
   },
   "outputs": [],
   "source": [
    "len(summary_70_long)/11112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_YiYl5QDZawV"
   },
   "outputs": [],
   "source": [
    "for year in [2030, 2050, 2080]:\n",
    "    globals()[f\"summary_70_above_015_{year}\"] = summary_70_long[\n",
    "        (summary_70_long['risk_level'] == 'high_risk') &\n",
    "        (summary_70_long['Gemeinde'] != 'Germany') &\n",
    "        (summary_70_long['year'] == year)\n",
    "    ].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TdmxJaFqZawV",
    "outputId": "d272a21b-148f-4d9e-b0bd-e33291c0cdd3"
   },
   "outputs": [],
   "source": [
    "summary_70_above_015_2030.GEM_ID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CkQaqhCcANL"
   },
   "outputs": [],
   "source": [
    "admin3_boundary = gpd.read_file('/content/admin-GIS/vg250_01-01.gk3.shape.ebenen/vg250_ebenen_0101/VG250_GEM.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tmb-ELFHZawV"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    summary_70_above_015_df = globals()[f'summary_70_above_015_{y}']\n",
    "\n",
    "    merged_gdf = admin3_boundary.merge(summary_70_above_015_df, left_on='OBJID', right_on='GEM_ID', how='right')\n",
    "\n",
    "    globals()[f'summary_70_above_015_{y}_gdf'] = merged_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wcc8Ojl4ZawV",
    "outputId": "139724d9-ae59-4d8f-9a36-a0d65a0a9eb2"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    summary_70_above_015_gdf = globals()[f'summary_70_above_015_{y}_gdf']\n",
    "    print(f\"Number of null geometries in summary_70_above_015_{y}_gdf: {len(summary_70_above_015_gdf[summary_70_above_015_gdf.geometry.isna()])}\")\n",
    "    print(f\"Number of unique geometries in summary_70_above_015_{y}_gdf: {len(summary_70_above_015_gdf.geometry.unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJeCefrgKYXd",
    "outputId": "cabc7f69-4cf9-4f4c-afc4-d003cd4a9a47"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "results = {}\n",
    "\n",
    "for year in years:\n",
    "    df_name = f\"summary_70_above_015_{year}_gdf\"\n",
    "    df = globals()[df_name]   # fetch DataFrame by name\n",
    "\n",
    "    positive_sum = df.loc[df.max_diff_2020 > 0].max_diff_2020.sum().round(2)\n",
    "    negative_sum = df.loc[df.max_diff_2020 < 0].max_diff_2020.sum().round(2)\n",
    "\n",
    "    results[year] = {\"positive_sum\": positive_sum, \"negative_sum\": negative_sum}\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrhXMbQtPOx8"
   },
   "source": [
    "### Percentage difference map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rYdCVrb59MBj",
    "outputId": "47d75398-b4ae-4e0c-d0a6-f3d5d49fa2f2"
   },
   "outputs": [],
   "source": [
    "# Years to plot\n",
    "years = [2030, 2050, 2080]\n",
    "save_path = \"/content/drive/MyDrive/Germany_Flood_Study/Gemeinde_climate_scenario_maps/final_adjustments\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "for y in years:\n",
    "    gdf = globals()[f'summary_70_above_015_{y}_gdf'].copy()\n",
    "    gdf = gdf.to_crs(epsg=3857)\n",
    "\n",
    "    # Mask 0 values for transparency\n",
    "    gdf['masked_pct_diff'] = gdf['max_pct_diff_2020'].replace(0, np.nan)\n",
    "\n",
    "    # Colors & breaks\n",
    "    custom_colors = [\n",
    "        '#40e0d0',  # < -10%\n",
    "        '#00ffff',  # -10% ~ -1%\n",
    "        '#808080',  # -1% ~ 1%\n",
    "        '#fee5d9',  # 1% ~ 10%\n",
    "        '#fb6a4a',  # 10% ~ 50%\n",
    "        '#de2d26',  # 50% ~ 100%\n",
    "        '#a50f15'   # > 100%\n",
    "    ]\n",
    "    custom_breaks = [-np.inf, -10, -1, 1, 10, 50, 100, np.inf]\n",
    "\n",
    "    cmap = ListedColormap(custom_colors)\n",
    "    norm = BoundaryNorm(custom_breaks, ncolors=len(custom_colors), clip=False)\n",
    "\n",
    "    # Create figure and axis INSIDE loop\n",
    "    fig, ax = plt.subplots(figsize=(14, 12))\n",
    "\n",
    "    # Plot\n",
    "    gdf.plot(\n",
    "        ax=ax,\n",
    "        column='masked_pct_diff',\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        edgecolor=None,\n",
    "        linewidth=0,\n",
    "        legend=False\n",
    "    )\n",
    "\n",
    "    # Overlay geometry boundaries\n",
    "    gdf.boundary.plot(ax=ax, color='grey', linewidth=0.05)\n",
    "\n",
    "    # Basemap\n",
    "    ctx.add_basemap(ax, source=ctx.providers.CartoDB.PositronNoLabels, crs=gdf.crs)\n",
    "\n",
    "    # Cleanup\n",
    "    ax.axis('off')\n",
    "\n",
    "    # North arrow & scale bar\n",
    "    add_north_arrow(ax, scale=0.75, xlim_pos=0.9025, ylim_pos=0.9,\n",
    "                    color='black', text_scaler=4, text_yT=-1.25)\n",
    "    scale1 = ScaleBar(dx=1, location=\"lower right\", scale_loc=\"bottom\")\n",
    "    ax.add_artist(scale1)\n",
    "\n",
    "    # Custom legend\n",
    "    legend_labels = [\n",
    "        \"<-10%\", \"-10% ~ -1%\",\n",
    "        \"-1% ~ 1%\",\n",
    "        \"1% ~ 10%\", \"10% ~ 50%\", \"50% ~ 100%\", \">100%\"\n",
    "    ]\n",
    "    circles = [\n",
    "        Line2D([0], [0], marker='o', color='None',\n",
    "              markerfacecolor=custom_colors[i],\n",
    "              markeredgecolor='None',\n",
    "              markersize=10,\n",
    "              label=legend_labels[i])\n",
    "        for i in range(len(legend_labels))\n",
    "    ]\n",
    "    ax.legend(\n",
    "        handles=circles,\n",
    "        title=\"Exposure change\",\n",
    "        loc='upper left',\n",
    "        fontsize=12,\n",
    "        title_fontsize=14\n",
    "    )\n",
    "\n",
    "    # Save\n",
    "    file_name = f\"gemeinde_exposure_pct_change_high_risk_70_{y}yr_separate_dry_wet_cyansv2.png\"\n",
    "    full_path = os.path.join(save_path, file_name)\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(full_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    # plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Lk-wiQEuWXG"
   },
   "source": [
    "## Coastal flooding undefended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "607Cn7NSuWXH"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5xbJeNFuWXH"
   },
   "outputs": [],
   "source": [
    "# List of years to process\n",
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "# Columns you want to select (CU columns and Land)\n",
    "cu_columns = ['id', 'Gemeinde', 'Kreis', 'Land'] + [col for col in all_columns if col.startswith('CU')]\n",
    "\n",
    "# Loop through each year and modify the corresponding DataFrame\n",
    "for year in years:\n",
    "    if year == 2020:\n",
    "        df_name = f'Gemeinde_{year}_100_raw'  # 2020 uses '_100_raw'\n",
    "    else:\n",
    "        df_name = f'Gemeinde_{year}_100_70_raw'  # 2030, 2050, and 2080 use '_100_85_raw'\n",
    "\n",
    "    # Check if the DataFrame exists before proceeding\n",
    "    if df_name in globals():\n",
    "        # Get the raw DataFrame and select the relevant columns for max flooding data\n",
    "        df_raw = globals()[df_name]\n",
    "        globals()[f'Gemeinde_{year}_100_70_CU'] = df_raw[cu_columns]\n",
    "    else:\n",
    "        print(f'{df_name} does not exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ny-yNNhGuWXH"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    globals()[f'Gemeinde_{y}_100_70_CU_long'] = globals()[f'Gemeinde_{y}_100_70_CU'].melt(\n",
    "        id_vars=['id', 'Gemeinde', 'Kreis', 'Land'], var_name='month_year_depth', value_name='CU_area'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "_HR2cCvGuWXI",
    "outputId": "7bee57dc-c2ef-49e6-d637-651f42b583af"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_70_CU_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0J6ZFDIuWXI"
   },
   "outputs": [],
   "source": [
    "for df in [Gemeinde_2020_100_70_CU_long, Gemeinde_2030_100_70_CU_long, Gemeinde_2050_100_70_CU_long, Gemeinde_2080_100_70_CU_long]:\n",
    "\n",
    "    # Replace 'CU' with 'MAX' if needed (optional step if not already 'MAX')\n",
    "    # df['month_year_depth'] = df['month_year_depth'].str.replace('CU', 'MAX')\n",
    "\n",
    "    # Split the 'month_year_depth' column into parts\n",
    "    df[['prefix', 'month_year', 'depth_cat']] = df['month_year_depth'].str.extract(r'^(CU)-([A-Za-z]{3}-\\d{2})-(.+)$')\n",
    "\n",
    "    # Drop 'prefix' if not needed\n",
    "    df.drop(columns='prefix', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "06O99LfYuWXI",
    "outputId": "491683bf-261c-4f76-e36e-fcb23cccb9e4"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2080_100_70_CU_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFOEKbpyuWXI"
   },
   "outputs": [],
   "source": [
    "# Define your DataFrames with their corresponding years\n",
    "year_df_map = {\n",
    "    2020: Gemeinde_2020_100_70_CU_long,\n",
    "    2030: Gemeinde_2030_100_70_CU_long,\n",
    "    2050: Gemeinde_2050_100_70_CU_long,\n",
    "    2080: Gemeinde_2080_100_70_CU_long\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame\n",
    "for year, df in year_df_map.items():\n",
    "\n",
    "    # Filter to only Jan-25\n",
    "    jan25_df = df[df['month_year'] == 'Jan-25'].reset_index(drop=True)\n",
    "\n",
    "    # Set the new global variable\n",
    "    globals()[f'Gemeinde_{year}_100_70_CU_Jan25'] = jan25_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MG-LhXOmuWXI"
   },
   "outputs": [],
   "source": [
    "# Second part: group and create *_settle DataFrames\n",
    "for year in [2020, 2030, 2050, 2080]:\n",
    "    jan25_var_name = f'Gemeinde_{year}_100_70_CU_Jan25'\n",
    "\n",
    "    if jan25_var_name in globals():\n",
    "        jan25_df = globals()[jan25_var_name]\n",
    "\n",
    "        # Group and sum by 'id' and 'month_year'\n",
    "        settle_df = jan25_df.groupby(['id', 'month_year'], as_index=False)['CU_area'].sum()\n",
    "        settle_df.columns = ['id', 'month_year', 'settle_area']\n",
    "        settle_df['settle_area'] = settle_df['settle_area'].astype(float)\n",
    "\n",
    "        # Save to a new global variable\n",
    "        globals()[f'{jan25_var_name}_settle'] = settle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ulxh5fMkuWXI"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    long_df = globals()[f'Gemeinde_{y}_100_70_CU_Jan25']\n",
    "    settle_df = globals()[f'Gemeinde_{y}_100_70_CU_Jan25_settle']\n",
    "\n",
    "    merged_df = pd.merge(long_df, settle_df, on=['id', 'month_year'], how='left')\n",
    "\n",
    "    globals()[f'Gemeinde_{y}_100_70_CU_Jan25'] = merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3QwoyLouWXI"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    df = globals()[f'Gemeinde_{y}_100_70_CU_Jan25']\n",
    "    globals()[f'Gemeinde_{y}_100_70_CU_Jan25'] = df\n",
    "    df['settle_area_non_zero'] = df.settle_area.replace(0, 1e-6)\n",
    "    globals()[f'Gemeinde_{y}_100_70_CU_Jan25']['settle_area_non_zero'] = df.settle_area_non_zero\n",
    "    globals()[f'Gemeinde_{y}_100_70_CU_Jan25']['depth_cat_area_pct'] = (df.CU_area / df.settle_area_non_zero) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vjtAfw0vuWXJ",
    "outputId": "168664d6-c882-4672-b41f-46707f84b30b"
   },
   "outputs": [],
   "source": [
    "cols = ['id', 'Gemeinde', 'Kreis', 'Land']\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "base_df = Gemeinde_2020_100_70_CU_Jan25\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_70_CU_Jan25']\n",
    "\n",
    "    # Direct comparison of the column values (row by row, in order)\n",
    "    is_same = base_df[cols].equals(current_df[cols])\n",
    "\n",
    "    if is_same:\n",
    "        print(f\"✅ Columns match exactly for year {year}\")\n",
    "    else:\n",
    "        print(f\"❌ Columns differ for year {year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fwzUQYAuWXJ"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_70_CU_Jan25']\n",
    "    base_df = Gemeinde_2020_100_70_CU_Jan25\n",
    "    base_df[f'diff_{year}'] = current_df.CU_area - base_df.CU_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ftrph-T1uWXJ"
   },
   "outputs": [],
   "source": [
    "summary = Gemeinde_2020_100_70_CU_Jan25[['id', 'Gemeinde', 'Kreis', 'Land']].drop_duplicates().copy()\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    diff_col = f'diff_{year}'\n",
    "\n",
    "    pivot = (\n",
    "        Gemeinde_2020_100_70_CU_Jan25\n",
    "        .groupby(['id', 'depth_cat'])[diff_col]\n",
    "        .sum()\n",
    "        .unstack(fill_value=0)\n",
    "        .add_prefix(f'sum_diff_{year}_')  # e.g., sum_diff_2030_lt0.5\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    summary = summary.merge(pivot, on='id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNco6GJQuWXJ"
   },
   "outputs": [],
   "source": [
    "# Convert all newly added columns to float\n",
    "new_cols = [col for col in summary.columns if col.startswith('sum_diff_')]\n",
    "summary[new_cols] = summary[new_cols].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XNBLugiuWXJ"
   },
   "outputs": [],
   "source": [
    "summary_70 = summary.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GpP_0pNduWXJ"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    summary_70[f'high_risk_{year}'] = (\n",
    "        summary_70.get(f'sum_diff_{year}_lt0.5', 0) +\n",
    "        summary_70.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_70.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_70[f'very_high_risk_{year}'] = (\n",
    "        summary_70.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_70.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_70[f'extreme_risk_{year}'] = summary_70.get(f'sum_diff_{year}_gt1.5', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pXlqR9ttuWXJ",
    "outputId": "77d9c263-e3c5-43f2-eaf2-564656b3e129"
   },
   "outputs": [],
   "source": [
    "summary_70.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPc6QRW_uWXJ"
   },
   "outputs": [],
   "source": [
    "value_vars = [\n",
    "    'high_risk_2030', 'very_high_risk_2030', 'extreme_risk_2030',\n",
    "    'high_risk_2050', 'very_high_risk_2050', 'extreme_risk_2050',\n",
    "    'high_risk_2080', 'very_high_risk_2080', 'extreme_risk_2080'\n",
    "]\n",
    "\n",
    "cu_summary_70_long = summary_70.melt(\n",
    "    id_vars=['id', 'Gemeinde', 'Kreis', 'Land'],  # keep these\n",
    "    value_vars=value_vars,\n",
    "    var_name='risk_year',\n",
    "    value_name='cu_diff_2020'\n",
    ")\n",
    "\n",
    "# Split the combined column into risk_level and year\n",
    "cu_summary_70_long[['risk_level', 'year']] = cu_summary_70_long['risk_year'].str.extract(r'(.*)_(\\d{4})')\n",
    "cu_summary_70_long['year'] = cu_summary_70_long['year'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVPD59ecuWXJ"
   },
   "source": [
    "### Please store the pre-processed files as pickle files, since you’ll need to load them directly later—especially if the session needs to be restarted due to running out of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdHHQsvduWXK"
   },
   "outputs": [],
   "source": [
    "cu_summary_70_long.to_pickle(\"cu_summary_70_long.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dPzsama9HPu"
   },
   "outputs": [],
   "source": [
    "with open('cu_summary_70_long.pkl', 'rb') as f:\n",
    "    cu_summary_70_long = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBdByeP09H3U"
   },
   "source": [
    "## Fluvial flooding undefended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYzX9flw9H3U"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QmkzZbA9H3U"
   },
   "outputs": [],
   "source": [
    "# List of years to process\n",
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "# Columns you want to select (CU columns and Land)\n",
    "fu_columns = ['id', 'Gemeinde', 'Kreis', 'Land'] + [col for col in all_columns if col.startswith('FU')]\n",
    "\n",
    "# Loop through each year and modify the corresponding DataFrame\n",
    "for year in years:\n",
    "    if year == 2020:\n",
    "        df_name = f'Gemeinde_{year}_100_raw'  # 2020 uses '_100_raw'\n",
    "    else:\n",
    "        df_name = f'Gemeinde_{year}_100_70_raw'  # 2030, 2050, and 2080 use '_100_85_raw'\n",
    "\n",
    "    # Check if the DataFrame exists before proceeding\n",
    "    if df_name in globals():\n",
    "        # Get the raw DataFrame and select the relevant columns for max flooding data\n",
    "        df_raw = globals()[df_name]\n",
    "        globals()[f'Gemeinde_{year}_100_70_FU'] = df_raw[fu_columns]\n",
    "    else:\n",
    "        print(f'{df_name} does not exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPrxikOo9H3U"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    globals()[f'Gemeinde_{y}_100_70_FU_long'] = globals()[f'Gemeinde_{y}_100_70_FU'].melt(\n",
    "        id_vars=['id', 'Gemeinde', 'Kreis', 'Land'], var_name='month_year_depth', value_name='FU_area'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "oZDVRiUu9H3V",
    "outputId": "48745362-ee5d-4843-f900-0b9de4fb7184"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_70_FU_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3CPDLJjv9H3V"
   },
   "outputs": [],
   "source": [
    "for df in [Gemeinde_2020_100_70_FU_long, Gemeinde_2030_100_70_FU_long, Gemeinde_2050_100_70_FU_long, Gemeinde_2080_100_70_FU_long]:\n",
    "\n",
    "    # Replace 'CU' with 'MAX' if needed (optional step if not already 'MAX')\n",
    "    # df['month_year_depth'] = df['month_year_depth'].str.replace('CU', 'MAX')\n",
    "\n",
    "    # Split the 'month_year_depth' column into parts\n",
    "    df[['prefix', 'month_year', 'depth_cat']] = df['month_year_depth'].str.extract(r'^(FU)-([A-Za-z]{3}-\\d{2})-(.+)$')\n",
    "\n",
    "    # Drop 'prefix' if not needed\n",
    "    df.drop(columns='prefix', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ZtoLqoHq9H3V",
    "outputId": "03f8c1eb-afd5-48ce-fead-31cb4f13c0fa"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2080_100_70_FU_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSnMUlCr9H3V"
   },
   "outputs": [],
   "source": [
    "# Define your DataFrames with their corresponding years\n",
    "year_df_map = {\n",
    "    2020: Gemeinde_2020_100_70_FU_long,\n",
    "    2030: Gemeinde_2030_100_70_FU_long,\n",
    "    2050: Gemeinde_2050_100_70_FU_long,\n",
    "    2080: Gemeinde_2080_100_70_FU_long\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame\n",
    "for year, df in year_df_map.items():\n",
    "\n",
    "    # Filter to only Jan-25\n",
    "    jan25_df = df[df['month_year'] == 'Jan-25'].reset_index(drop=True)\n",
    "\n",
    "    # Set the new global variable\n",
    "    globals()[f'Gemeinde_{year}_100_70_FU_Jan25'] = jan25_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "971CbwWk9H3W"
   },
   "outputs": [],
   "source": [
    "# Second part: group and create *_settle DataFrames\n",
    "for year in [2020, 2030, 2050, 2080]:\n",
    "    jan25_var_name = f'Gemeinde_{year}_100_70_FU_Jan25'\n",
    "\n",
    "    if jan25_var_name in globals():\n",
    "        jan25_df = globals()[jan25_var_name]\n",
    "\n",
    "        # Group and sum by 'id' and 'month_year'\n",
    "        settle_df = jan25_df.groupby(['id', 'month_year'], as_index=False)['FU_area'].sum()\n",
    "        settle_df.columns = ['id', 'month_year', 'settle_area']\n",
    "        settle_df['settle_area'] = settle_df['settle_area'].astype(float)\n",
    "\n",
    "        # Save to a new global variable\n",
    "        globals()[f'{jan25_var_name}_settle'] = settle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqC6PjT89H3W"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    long_df = globals()[f'Gemeinde_{y}_100_70_FU_Jan25']\n",
    "    settle_df = globals()[f'Gemeinde_{y}_100_70_FU_Jan25_settle']\n",
    "\n",
    "    merged_df = pd.merge(long_df, settle_df, on=['id', 'month_year'], how='left')\n",
    "\n",
    "    globals()[f'Gemeinde_{y}_100_70_FU_Jan25'] = merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_Wlv6ps9H3X"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    df = globals()[f'Gemeinde_{y}_100_70_FU_Jan25']\n",
    "    globals()[f'Gemeinde_{y}_100_70_FU_Jan25'] = df\n",
    "    df['settle_area_non_zero'] = df.settle_area.replace(0, 1e-6)\n",
    "    globals()[f'Gemeinde_{y}_100_70_FU_Jan25']['settle_area_non_zero'] = df.settle_area_non_zero\n",
    "    globals()[f'Gemeinde_{y}_100_70_FU_Jan25']['depth_cat_area_pct'] = (df.FU_area / df.settle_area_non_zero) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oTGVongh9H3X",
    "outputId": "823ccae9-9f2e-4bd5-be26-bd16c37d80e2"
   },
   "outputs": [],
   "source": [
    "cols = ['id', 'Gemeinde', 'Kreis', 'Land']\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "base_df = Gemeinde_2020_100_70_FU_Jan25\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_70_FU_Jan25']\n",
    "\n",
    "    # Direct comparison of the column values (row by row, in order)\n",
    "    is_same = base_df[cols].equals(current_df[cols])\n",
    "\n",
    "    if is_same:\n",
    "        print(f\"✅ Columns match exactly for year {year}\")\n",
    "    else:\n",
    "        print(f\"❌ Columns differ for year {year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pue1H9lv9H3X"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_70_FU_Jan25']\n",
    "    base_df = Gemeinde_2020_100_70_FU_Jan25\n",
    "    base_df[f'diff_{year}'] = current_df.FU_area - base_df.FU_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtrM9Vdb9H3X"
   },
   "outputs": [],
   "source": [
    "summary = Gemeinde_2020_100_70_FU_Jan25[['id', 'Gemeinde', 'Kreis', 'Land']].drop_duplicates().copy()\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    diff_col = f'diff_{year}'\n",
    "\n",
    "    pivot = (\n",
    "        Gemeinde_2020_100_70_FU_Jan25\n",
    "        .groupby(['id', 'depth_cat'])[diff_col]\n",
    "        .sum()\n",
    "        .unstack(fill_value=0)\n",
    "        .add_prefix(f'sum_diff_{year}_')  # e.g., sum_diff_2030_lt0.5\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    summary = summary.merge(pivot, on='id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfRxEBek9H3X"
   },
   "outputs": [],
   "source": [
    "# Convert all newly added columns to float\n",
    "new_cols = [col for col in summary.columns if col.startswith('sum_diff_')]\n",
    "summary[new_cols] = summary[new_cols].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUTgl6vT9H3Y"
   },
   "outputs": [],
   "source": [
    "summary_70 = summary.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSBC_WGR9H3Y"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    summary_70[f'high_risk_{year}'] = (\n",
    "        summary_70.get(f'sum_diff_{year}_lt0.5', 0) +\n",
    "        summary_70.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_70.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_70[f'very_high_risk_{year}'] = (\n",
    "        summary_70.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_70.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_70[f'extreme_risk_{year}'] = summary_70.get(f'sum_diff_{year}_gt1.5', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4EWF_NTd9H3Y",
    "outputId": "cf506c1e-56ed-4585-9e74-727a76b47920"
   },
   "outputs": [],
   "source": [
    "summary_70.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-IsrRGjG9H3Y"
   },
   "outputs": [],
   "source": [
    "value_vars = [\n",
    "    'high_risk_2030', 'very_high_risk_2030', 'extreme_risk_2030',\n",
    "    'high_risk_2050', 'very_high_risk_2050', 'extreme_risk_2050',\n",
    "    'high_risk_2080', 'very_high_risk_2080', 'extreme_risk_2080'\n",
    "]\n",
    "\n",
    "fu_summary_70_long = summary_70.melt(\n",
    "    id_vars=['id', 'Gemeinde', 'Kreis', 'Land'],  # keep these\n",
    "    value_vars=value_vars,\n",
    "    var_name='risk_year',\n",
    "    value_name='fu_diff_2020'\n",
    ")\n",
    "\n",
    "# Split the combined column into risk_level and year\n",
    "fu_summary_70_long[['risk_level', 'year']] = fu_summary_70_long['risk_year'].str.extract(r'(.*)_(\\d{4})')\n",
    "fu_summary_70_long['year'] = fu_summary_70_long['year'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIXFFul19H3Z"
   },
   "source": [
    "### Please store the pre-processed files as pickle files, since you’ll need to load them directly later—especially if the session needs to be restarted due to running out of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-u0FZ2mx9H3Z"
   },
   "outputs": [],
   "source": [
    "fu_summary_70_long.to_pickle(\"fu_summary_70_long.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRh_wmgtuiTv"
   },
   "outputs": [],
   "source": [
    "with open('fu_summary_70_long.pkl', 'rb') as f:\n",
    "    fu_summary_70_long = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E57a_ZchN8VR"
   },
   "source": [
    "## Pluvial flooding defended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugbn-lazN8VR"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSPNni4TN8VS"
   },
   "outputs": [],
   "source": [
    "# List of years to process\n",
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "# Columns you want to select (CU columns and Land)\n",
    "pd_columns = ['id', 'Gemeinde', 'Kreis', 'Land'] + [col for col in all_columns if col.startswith('PD') or col == 'Land']\n",
    "\n",
    "# Loop through each year and modify the corresponding DataFrame\n",
    "for year in years:\n",
    "    if year == 2020:\n",
    "        df_name = f'Gemeinde_{year}_100_raw'  # 2020 uses '_100_raw'\n",
    "    else:\n",
    "        df_name = f'Gemeinde_{year}_100_70_raw'  # 2030, 2050, and 2080 use '_100_85_raw'\n",
    "\n",
    "    # Check if the DataFrame exists before proceeding\n",
    "    if df_name in globals():\n",
    "        # Get the raw DataFrame and select the relevant columns for max flooding data\n",
    "        df_raw = globals()[df_name]\n",
    "        globals()[f'Gemeinde_{year}_100_70_PD'] = df_raw[pd_columns]\n",
    "    else:\n",
    "        print(f'{df_name} does not exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jh206HEaN8VS"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    globals()[f'Gemeinde_{y}_100_70_PD_long'] = globals()[f'Gemeinde_{y}_100_70_PD'].melt(\n",
    "        id_vars=['id', 'Gemeinde', 'Kreis', 'Land'], var_name='month_year_depth', value_name='PD_area'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "LHFmRRTCN8VS",
    "outputId": "7f881c0c-11f6-4061-bb80-8001a609f983"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_70_PD_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgpVDjI6N8VS"
   },
   "outputs": [],
   "source": [
    "for df in [Gemeinde_2020_100_70_PD_long, Gemeinde_2030_100_70_PD_long, Gemeinde_2050_100_70_PD_long, Gemeinde_2080_100_70_PD_long]:\n",
    "\n",
    "    # Replace 'CU' with 'MAX' if needed (optional step if not already 'MAX')\n",
    "    # df['month_year_depth'] = df['month_year_depth'].str.replace('CU', 'MAX')\n",
    "\n",
    "    # Split the 'month_year_depth' column into parts\n",
    "    df[['prefix', 'month_year', 'depth_cat']] = df['month_year_depth'].str.extract(r'^(PD)-([A-Za-z]{3}-\\d{2})-(.+)$')\n",
    "\n",
    "    # Drop 'prefix' if not needed\n",
    "    df.drop(columns='prefix', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "TSaOPE0CN8VS",
    "outputId": "157b44ff-f438-428d-f467-3c5fb0a99c33"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2080_100_70_PD_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dF-KLlE5N8VS"
   },
   "outputs": [],
   "source": [
    "# Define your DataFrames with their corresponding years\n",
    "year_df_map = {\n",
    "    2020: Gemeinde_2020_100_70_PD_long,\n",
    "    2030: Gemeinde_2030_100_70_PD_long,\n",
    "    2050: Gemeinde_2050_100_70_PD_long,\n",
    "    2080: Gemeinde_2080_100_70_PD_long\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame\n",
    "for year, df in year_df_map.items():\n",
    "\n",
    "    # Filter to only Jan-25\n",
    "    jan25_df = df[df['month_year'] == 'Jan-25'].reset_index(drop=True)\n",
    "\n",
    "    # Set the new global variable\n",
    "    globals()[f'Gemeinde_{year}_100_70_PD_Jan25'] = jan25_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fX9291kJN8VS"
   },
   "outputs": [],
   "source": [
    "# Second part: group and create *_settle DataFrames\n",
    "for year in [2020, 2030, 2050, 2080]:\n",
    "    jan25_var_name = f'Gemeinde_{year}_100_70_PD_Jan25'\n",
    "\n",
    "    if jan25_var_name in globals():\n",
    "        jan25_df = globals()[jan25_var_name]\n",
    "\n",
    "        # Group and sum by 'id' and 'month_year'\n",
    "        settle_df = jan25_df.groupby(['id', 'month_year'], as_index=False)['PD_area'].sum()\n",
    "        settle_df.columns = ['id', 'month_year', 'settle_area']\n",
    "        settle_df['settle_area'] = settle_df['settle_area'].astype(float)\n",
    "\n",
    "        # Save to a new global variable\n",
    "        globals()[f'{jan25_var_name}_settle'] = settle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e395ATqpN8VT"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    long_df = globals()[f'Gemeinde_{y}_100_70_PD_Jan25']\n",
    "    settle_df = globals()[f'Gemeinde_{y}_100_70_PD_Jan25_settle']\n",
    "\n",
    "    merged_df = pd.merge(long_df, settle_df, on=['id', 'month_year'], how='left')\n",
    "\n",
    "    globals()[f'Gemeinde_{y}_100_70_PD_Jan25'] = merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SzR3ozN5N8VT"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    df = globals()[f'Gemeinde_{y}_100_70_PD_Jan25']\n",
    "    globals()[f'Gemeinde_{y}_100_70_PD_Jan25'] = df\n",
    "    df['settle_area_non_zero'] = df.settle_area.replace(0, 1e-6)\n",
    "    globals()[f'Gemeinde_{y}_100_70_PD_Jan25']['settle_area_non_zero'] = df.settle_area_non_zero\n",
    "    globals()[f'Gemeinde_{y}_100_70_PD_Jan25']['depth_cat_area_pct'] = (df.PD_area / df.settle_area_non_zero) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mlMfCBp7N8VU",
    "outputId": "e1bddbcd-c26c-46ea-f2f6-e6a7eb4c35f6"
   },
   "outputs": [],
   "source": [
    "cols = ['id', 'Gemeinde', 'Kreis', 'Land']\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "base_df = Gemeinde_2020_100_70_PD_Jan25\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_70_PD_Jan25']\n",
    "\n",
    "    # Direct comparison of the column values (row by row, in order)\n",
    "    is_same = base_df[cols].equals(current_df[cols])\n",
    "\n",
    "    if is_same:\n",
    "        print(f\"✅ Columns match exactly for year {year}\")\n",
    "    else:\n",
    "        print(f\"❌ Columns differ for year {year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3DZwvNnN8VU"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_70_PD_Jan25']\n",
    "    base_df = Gemeinde_2020_100_70_PD_Jan25\n",
    "    base_df[f'diff_{year}'] = current_df.PD_area - base_df.PD_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VrjylLxxN8VU"
   },
   "outputs": [],
   "source": [
    "summary = Gemeinde_2020_100_70_PD_Jan25[['id', 'Gemeinde', 'Kreis', 'Land']].drop_duplicates().copy()\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    diff_col = f'diff_{year}'\n",
    "\n",
    "    pivot = (\n",
    "        Gemeinde_2020_100_70_PD_Jan25\n",
    "        .groupby(['id', 'depth_cat'])[diff_col]\n",
    "        .sum()\n",
    "        .unstack(fill_value=0)\n",
    "        .add_prefix(f'sum_diff_{year}_')  # e.g., sum_diff_2030_lt0.5\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    summary = summary.merge(pivot, on='id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZX0B26OoN8VU"
   },
   "outputs": [],
   "source": [
    "# Convert all newly added columns to float\n",
    "new_cols = [col for col in summary.columns if col.startswith('sum_diff_')]\n",
    "summary[new_cols] = summary[new_cols].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lecs_u6TN8VU"
   },
   "outputs": [],
   "source": [
    "summary_70 = summary.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4X1XMn12N8VU"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    summary_70[f'high_risk_{year}'] = (\n",
    "        summary_70.get(f'sum_diff_{year}_lt0.5', 0) +\n",
    "        summary_70.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_70.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_70[f'very_high_risk_{year}'] = (\n",
    "        summary_70.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_70.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_70[f'extreme_risk_{year}'] = summary_70.get(f'sum_diff_{year}_gt1.5', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UuZKX-KFN8VU",
    "outputId": "5b07c495-9ca3-41dc-bb75-390bae9a8eb3"
   },
   "outputs": [],
   "source": [
    "summary_70.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LrRaGQcN8VU"
   },
   "outputs": [],
   "source": [
    "value_vars = [\n",
    "    'high_risk_2030', 'very_high_risk_2030', 'extreme_risk_2030',\n",
    "    'high_risk_2050', 'very_high_risk_2050', 'extreme_risk_2050',\n",
    "    'high_risk_2080', 'very_high_risk_2080', 'extreme_risk_2080'\n",
    "]\n",
    "\n",
    "pd_summary_70_long = summary_70.melt(\n",
    "    id_vars=['id', 'Gemeinde', 'Kreis', 'Land'],  # keep these\n",
    "    value_vars=value_vars,\n",
    "    var_name='risk_year',\n",
    "    value_name='pd_diff_2020'\n",
    ")\n",
    "\n",
    "# Split the combined column into risk_level and year\n",
    "pd_summary_70_long[['risk_level', 'year']] = pd_summary_70_long['risk_year'].str.extract(r'(.*)_(\\d{4})')\n",
    "pd_summary_70_long['year'] = pd_summary_70_long['year'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBVSEIDRN8VU"
   },
   "source": [
    "### Please store the pre-processed files as pickle files, since you’ll need to load them directly later—especially if the session needs to be restarted due to running out of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XbENQkSiN8VV"
   },
   "outputs": [],
   "source": [
    "pd_summary_70_long.to_pickle(\"pd_summary_70_long.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03lv8Q6LN8VV"
   },
   "outputs": [],
   "source": [
    "with open(\"pd_summary_70_long.pkl\", \"rb\") as f:\n",
    "    pd_summary_70_long = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdtf1ADjhhTA"
   },
   "source": [
    "# SSP5-RCP8.5 (Most Pessimistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GByCfhuqSmz"
   },
   "source": [
    "### Read files, pre-processing and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WuWzCHhMWJvp",
    "outputId": "1197996a-56f6-47f5-8ed5-bb16b2e279ed"
   },
   "outputs": [],
   "source": [
    "!unzip \"/content/most_pessimistic.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0s-BcWXeXLB"
   },
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    '/content/Germany_Gemeinde_2020_100.xlsx',\n",
    "    '/content/most_pessimistic/Germany_Gemeinde_2030_100_85.xlsx',\n",
    "    '/content/most_pessimistic/Germany_Gemeinde_2050_100_85.xlsx',\n",
    "    '/content/most_pessimistic/Germany_Gemeinde_2080_100_85.xlsx',\n",
    "]\n",
    "\n",
    "dataframes = []\n",
    "for path in file_paths:\n",
    "    df = pd.read_excel(path)\n",
    "    df = df.round(6)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Optionally unpack into variables\n",
    "(Gemeinde_2020_100_raw, Gemeinde_2030_100_85_raw, Gemeinde_2050_100_85_raw, Gemeinde_2080_100_85_raw) = dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "id": "YfCB_ZOtnpHK",
    "outputId": "12ba03d9-7065-43b9-aa29-5ae13ca41fd2"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SaOOnXHDancj"
   },
   "outputs": [],
   "source": [
    "# Date range: every 6 months\n",
    "dates = pd.date_range(start='2016-07-01', end='2025-01-01', freq='6MS')\n",
    "\n",
    "# Prefixes and suffixes\n",
    "prefixes = ['CU', 'FU', 'PD', 'MAX']\n",
    "suffixes = ['P0', 'lt0.15', 'lt0.5', 'lt1.5', 'gt1.5']\n",
    "\n",
    "# Correct order: by prefix, then suffix, then date\n",
    "all_columns = [\n",
    "    f\"{prefix}-{date.strftime('%b-%y')}-{suffix}\"\n",
    "    for prefix in prefixes\n",
    "    for suffix in suffixes\n",
    "    for date in dates\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opZz-F_YdFUB"
   },
   "outputs": [],
   "source": [
    "# Check if number of replacement columns matches the shape\n",
    "for df in [Gemeinde_2020_100_raw, Gemeinde_2030_100_85_raw, Gemeinde_2050_100_85_raw, Gemeinde_2080_100_85_raw]:\n",
    "    if len(all_columns) == df.shape[1] - 3:\n",
    "      df.columns = list(df.columns[:3]) + all_columns\n",
    "      df.reset_index(drop=True, inplace=True)\n",
    "      df['id'] = df.index\n",
    "    else:\n",
    "      raise ValueError(\"Length of generated column names does not match number of columns (excluding the first one).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "id": "zzyWTdl8dJEe",
    "outputId": "02483671-77d5-4425-b2c0-471ce0e5ecd1"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2030_100_85_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "id": "RDhap5zX3Lor",
    "outputId": "393ee1b9-1293-42c9-cd06-daa53877f83a"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2030_100_85_raw[1:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-WdAqBE3HfU"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_raw = Gemeinde_2020_100_raw[1:]\n",
    "Gemeinde_2030_100_85_raw = Gemeinde_2030_100_85_raw[1:]\n",
    "Gemeinde_2050_100_85_raw = Gemeinde_2050_100_85_raw[1:]\n",
    "Gemeinde_2080_100_85_raw = Gemeinde_2080_100_85_raw[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6rDpYtP1mo1"
   },
   "outputs": [],
   "source": [
    "save_path = \"/content/\"\n",
    "\n",
    "# Make sure the directory exists\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "gemeinde_dfs = [Gemeinde_2020_100_raw, Gemeinde_2030_100_85_raw, Gemeinde_2050_100_85_raw, Gemeinde_2080_100_85_raw]\n",
    "\n",
    "filenames = [\n",
    "    \"Gemeinde_2020_100_raw.pkl\", \"Gemeinde_2030_100_85_aw.pkl\", \"Gemeinde_2050_100_85_raw.pkl\", \"Gemeinde_2080_100_85_raw.pkl\"\n",
    "]\n",
    "\n",
    "for df, filename in zip(gemeinde_dfs, filenames):\n",
    "    df.to_pickle(os.path.join(save_path, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHzfrN7hqOvu"
   },
   "source": [
    "### **PLEASE re-run if the session restarts before you finish processing this climate scenario:** Read and load file from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jC6hUfipD5Xs"
   },
   "outputs": [],
   "source": [
    "# Define the path where the files are stored\n",
    "save_path = \"/content/\"\n",
    "\n",
    "# List of filenames\n",
    "filenames = [\n",
    "    \"Gemeinde_2020_100_raw.pkl\", \"Gemeinde_2030_100_85_aw.pkl\", \"Gemeinde_2050_100_85_raw.pkl\", \"Gemeinde_2080_100_85_raw.pkl\"\n",
    "]\n",
    "\n",
    "# Load all DataFrames into a list\n",
    "gemeinde_dfs = [pd.read_pickle(os.path.join(save_path, filename)) for filename in filenames]\n",
    "\n",
    "cols_to_convert = [col for col in gemeinde_dfs[0].columns if col not in ['Gemeinde', 'Kreis', 'Land', 'id']]\n",
    "\n",
    "def convert_and_round(df):\n",
    "    df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, errors='coerce').round(6)\n",
    "    return df\n",
    "\n",
    "gemeinde_dfs = [convert_and_round(df) for df in gemeinde_dfs]\n",
    "\n",
    "# Optionally assign them back to individual variable names (if needed)\n",
    "(\n",
    "  Gemeinde_2020_100_raw,\n",
    "  Gemeinde_2030_100_85_raw,\n",
    "  Gemeinde_2050_100_85_raw,\n",
    "  Gemeinde_2080_100_85_raw\n",
    ") = gemeinde_dfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fehn-cmI0vFm"
   },
   "source": [
    "## Combined flooding (max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6nnauaP0vFn"
   },
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-aLNW-q0vFn"
   },
   "outputs": [],
   "source": [
    "# List of years to process\n",
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "# Columns you want to select (CU columns and Land)\n",
    "max_columns = ['id', 'Gemeinde', 'Kreis', 'Land'] + [col for col in all_columns if col.startswith('MAX') or col == 'Land']\n",
    "\n",
    "# Loop through each year and modify the corresponding DataFrame\n",
    "for year in years:\n",
    "    if year == 2020:\n",
    "        df_name = f'Gemeinde_{year}_100_raw'  # 2020 uses '_100_raw'\n",
    "    else:\n",
    "        df_name = f'Gemeinde_{year}_100_85_raw'  # 2030, 2050, and 2080 use '_100_85_raw'\n",
    "\n",
    "    # Check if the DataFrame exists before proceeding\n",
    "    if df_name in globals():\n",
    "        # Get the raw DataFrame and select the relevant columns for max flooding data\n",
    "        df_raw = globals()[df_name]\n",
    "        globals()[f'Gemeinde_{year}_100_85_MAX'] = df_raw[max_columns]\n",
    "    else:\n",
    "        print(f'{df_name} does not exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLdWV-Xa0vFo"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    globals()[f'Gemeinde_{y}_100_85_MAX_long'] = globals()[f'Gemeinde_{y}_100_85_MAX'].melt(\n",
    "        id_vars=['id', 'Gemeinde', 'Kreis', 'Land'], var_name='month_year_depth', value_name='MAX_area'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "N1c-UeYynAnM",
    "outputId": "8a96c695-a61e-400e-a899-217378bd4b7a"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_85_MAX_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpKzSNC10vFo"
   },
   "outputs": [],
   "source": [
    "for df in [Gemeinde_2020_100_85_MAX_long, Gemeinde_2030_100_85_MAX_long, Gemeinde_2050_100_85_MAX_long, Gemeinde_2080_100_85_MAX_long]:\n",
    "\n",
    "    # Replace 'CU' with 'MAX' if needed (optional step if not already 'MAX')\n",
    "    # df['month_year_depth'] = df['month_year_depth'].str.replace('CU', 'MAX')\n",
    "\n",
    "    # Split the 'month_year_depth' column into parts\n",
    "    df[['prefix', 'month_year', 'depth_cat']] = df['month_year_depth'].str.extract(r'^(MAX)-([A-Za-z]{3}-\\d{2})-(.+)$')\n",
    "\n",
    "    # Drop 'prefix' if not needed\n",
    "    df.drop(columns='prefix', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Gt9Qg3rP0vFo",
    "outputId": "5bb7995e-0409-4fa2-f222-faaa23cac412"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2080_100_85_MAX_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_H6qlUzjprgo"
   },
   "outputs": [],
   "source": [
    "# Define your DataFrames with their corresponding years\n",
    "year_df_map = {\n",
    "    2020: Gemeinde_2020_100_85_MAX_long,\n",
    "    2030: Gemeinde_2030_100_85_MAX_long,\n",
    "    2050: Gemeinde_2050_100_85_MAX_long,\n",
    "    2080: Gemeinde_2080_100_85_MAX_long\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame\n",
    "for year, df in year_df_map.items():\n",
    "\n",
    "    # Filter to only Jan-25\n",
    "    jan25_df = df[df['month_year'] == 'Jan-25'].reset_index(drop=True)\n",
    "\n",
    "    # Set the new global variable\n",
    "    globals()[f'Gemeinde_{year}_100_85_MAX_Jan25'] = jan25_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKyQYk_J0vFp"
   },
   "outputs": [],
   "source": [
    "# Second part: group and create *_settle DataFrames\n",
    "for year in [2020, 2030, 2050, 2080]:\n",
    "    jan25_var_name = f'Gemeinde_{year}_100_85_MAX_Jan25'\n",
    "\n",
    "    if jan25_var_name in globals():\n",
    "        jan25_df = globals()[jan25_var_name]\n",
    "\n",
    "        # Group and sum by 'id' and 'month_year'\n",
    "        settle_df = jan25_df.groupby(['id', 'month_year'], as_index=False)['MAX_area'].sum()\n",
    "        settle_df.columns = ['id', 'month_year', 'settle_area']\n",
    "        settle_df['settle_area'] = settle_df['settle_area'].astype(float)\n",
    "\n",
    "        # Save to a new global variable\n",
    "        globals()[f'{jan25_var_name}_settle'] = settle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPeoo1nC0vFp"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    long_df = globals()[f'Gemeinde_{y}_100_85_MAX_Jan25']\n",
    "    settle_df = globals()[f'Gemeinde_{y}_100_85_MAX_Jan25_settle']\n",
    "\n",
    "    merged_df = pd.merge(long_df, settle_df, on=['id', 'month_year'], how='left')\n",
    "\n",
    "    globals()[f'Gemeinde_{y}_100_85_MAX_Jan25'] = merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQNSA3mZ0vFp"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    df = globals()[f'Gemeinde_{y}_100_85_MAX_Jan25']\n",
    "    globals()[f'Gemeinde_{y}_100_85_MAX_Jan25'] = df\n",
    "    df['settle_area_non_zero'] = df.settle_area.replace(0, 1e-6)\n",
    "    globals()[f'Gemeinde_{y}_100_85_MAX_Jan25']['settle_area_non_zero'] = df.settle_area_non_zero\n",
    "    globals()[f'Gemeinde_{y}_100_85_MAX_Jan25']['depth_cat_area_pct'] = (df.MAX_area / df.settle_area_non_zero) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aa5oofk5DW0j"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_85_MAX_Jan25']\n",
    "    base_df = Gemeinde_2020_100_85_MAX_Jan25\n",
    "    base_df[f'diff_{year}'] = current_df.MAX_area - base_df.MAX_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nf2wFJ7rGFKI"
   },
   "outputs": [],
   "source": [
    "summary = Gemeinde_2020_100_85_MAX_Jan25[['id', 'Gemeinde', 'Kreis', 'Land']].drop_duplicates().copy()\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    diff_col = f'diff_{year}'\n",
    "\n",
    "    pivot = (\n",
    "        Gemeinde_2020_100_85_MAX_Jan25\n",
    "        .groupby(['id', 'depth_cat'])[diff_col]\n",
    "        .sum()\n",
    "        .unstack(fill_value=0)\n",
    "        .add_prefix(f'sum_diff_{year}_')  # e.g., sum_diff_2030_lt0.5\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    summary = summary.merge(pivot, on='id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ozEopI1fXN-"
   },
   "outputs": [],
   "source": [
    "# Convert all newly added columns to float\n",
    "new_cols = [col for col in summary.columns if col.startswith('sum_diff_')]\n",
    "summary[new_cols] = summary[new_cols].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBq4KqXXVF42"
   },
   "outputs": [],
   "source": [
    "summary_85 = summary.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0ffqdvVMFYj"
   },
   "outputs": [],
   "source": [
    "# Step 1: Calculate total MAX_area per id and depth_cat\n",
    "area_pivot = (\n",
    "    Gemeinde_2020_100_85_MAX_Jan25\n",
    "    .groupby(['id', 'depth_cat'])['MAX_area']\n",
    "    .sum()\n",
    "    .unstack(fill_value=0)\n",
    "    .add_prefix('area_')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Step 2: Merge into summary_26\n",
    "summary_85 = summary_85.merge(area_pivot, on='id', how='left')\n",
    "\n",
    "# Step 3: Calculate percentage risk levels using area for each category\n",
    "years = [2030, 2050, 2080]\n",
    "for year in years:\n",
    "    # Absolute difference sums (already calculated earlier)\n",
    "    summary_85[f'high_risk_{year}'] = (\n",
    "        summary_85.get(f'sum_diff_{year}_lt0.5', 0) +\n",
    "        summary_85.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_85.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_85[f'very_high_risk_{year}'] = (\n",
    "        summary_85.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_85.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_85[f'extreme_risk_{year}'] = summary_85.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "\n",
    "    # Denominators from MAX_area sums\n",
    "    total_area = (\n",
    "        summary_85.get('area_lt0.5', 0) +\n",
    "        summary_85.get('area_lt1.5', 0) +\n",
    "        summary_85.get('area_gt1.5', 0)\n",
    "    ).replace(0, 1e-6)\n",
    "\n",
    "    very_high_area = (\n",
    "        summary_85.get('area_lt1.5', 0) +\n",
    "        summary_85.get('area_gt1.5', 0)\n",
    "    ).replace(0, 1e-6)\n",
    "\n",
    "    extreme_area = summary_85.get('area_gt1.5', 0).replace(0, 1e-6)\n",
    "\n",
    "    # Percentage risk levels\n",
    "    summary_85[f'pct_high_risk_{year}'] = (\n",
    "        summary_85[f'high_risk_{year}'] / total_area * 100\n",
    "    ).round(2)\n",
    "\n",
    "    summary_85[f'pct_very_high_risk_{year}'] = (\n",
    "        summary_85[f'very_high_risk_{year}'] / very_high_area * 100\n",
    "    ).round(2)\n",
    "\n",
    "    summary_85[f'pct_extreme_risk_{year}'] = (\n",
    "        summary_85[f'extreme_risk_{year}'] / extreme_area * 100\n",
    "    ).round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zCXc1pDkVZ1b",
    "outputId": "a5dbe764-2472-469c-bd24-0475b3ee1104"
   },
   "outputs": [],
   "source": [
    "summary_85.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "okWU67-3MwRs"
   },
   "outputs": [],
   "source": [
    "value_vars = [\n",
    "    'high_risk_2030', 'very_high_risk_2030', 'extreme_risk_2030',\n",
    "    'high_risk_2050', 'very_high_risk_2050', 'extreme_risk_2050',\n",
    "    'high_risk_2080', 'very_high_risk_2080', 'extreme_risk_2080'\n",
    "]\n",
    "\n",
    "# 1. Melt the absolute values\n",
    "summary_diff_long = summary_85.melt(\n",
    "    id_vars=['id', 'Gemeinde', 'Kreis', 'Land'],\n",
    "    value_vars=value_vars,\n",
    "    var_name='risk_year',\n",
    "    value_name='max_diff_2020'\n",
    ")\n",
    "\n",
    "# 2. Melt the percent difference columns\n",
    "summary_pct_long = summary_85.melt(\n",
    "    id_vars=['id', 'Gemeinde', 'Kreis', 'Land'],\n",
    "    value_vars=[f'pct_{v}' for v in value_vars],\n",
    "    var_name='risk_year',\n",
    "    value_name='max_pct_diff_2020'\n",
    ")\n",
    "\n",
    "# 3. Clean `risk_year` for both (they must match for merge)\n",
    "summary_pct_long['risk_year'] = summary_pct_long['risk_year'].str.replace('pct_', '')\n",
    "\n",
    "# 4. Merge the two melted DataFrames\n",
    "summary_85_long = pd.merge(\n",
    "    summary_diff_long,\n",
    "    summary_pct_long,\n",
    "    on=['id', 'Gemeinde', 'Kreis', 'Land', 'risk_year']\n",
    ")\n",
    "\n",
    "# 5. Extract risk level and year\n",
    "summary_85_long[['risk_level', 'year']] = summary_85_long['risk_year'].str.extract(r'(.*)_(\\d{4})')\n",
    "summary_85_long['year'] = summary_85_long['year'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KKl-l4JbRYQ"
   },
   "source": [
    "### Include GEM_ID to match with the gemeinden geometries for mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8kwITmle31fe"
   },
   "outputs": [],
   "source": [
    "with open(\"max_summary_26_long.pkl\", \"rb\") as f:\n",
    "    summary_26_long = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "apaK6mkabRYQ",
    "outputId": "e42afb17-3a46-40f6-d3af-e161791d9e39"
   },
   "outputs": [],
   "source": [
    "# clean comparison of df1 and df2 by 'id'\n",
    "\n",
    "# 1. select and sort consistently\n",
    "df1_clean = summary_85_long[['id', 'Gemeinde', 'Kreis', 'Land']].drop_duplicates()\n",
    "df2_clean = summary_26_long[['id', 'Gemeinde', 'Kreis', 'Land']].drop_duplicates()\n",
    "\n",
    "# 2. drop non-Gemeinde entries (e.g., rows where Gemeinde or Land is missing)\n",
    "df1_clean = df1_clean.dropna(subset=['Gemeinde', 'Land'])\n",
    "df2_clean = df2_clean.dropna(subset=['Gemeinde', 'Land'])\n",
    "\n",
    "# 3. set 'id' as index to ensure alignment by identifier, not row order\n",
    "df1_clean = df1_clean.set_index('id').sort_index()\n",
    "df2_clean = df2_clean.set_index('id').sort_index()\n",
    "\n",
    "# 4. align and find differences\n",
    "# keep only ids present in both datasets\n",
    "common_ids = df1_clean.index.intersection(df2_clean.index)\n",
    "\n",
    "# find true mismatches by comparing all columns for each id\n",
    "diff_mask = ~(df1_clean.loc[common_ids] == df2_clean.loc[common_ids]).all(axis=1)\n",
    "mismatched_rows = pd.concat(\n",
    "    [df1_clean.loc[diff_mask], df2_clean.loc[diff_mask]],\n",
    "    keys=['df1', 'df2']\n",
    ")\n",
    "\n",
    "# 5. print results\n",
    "print(f\"number of mismatched ids: {diff_mask.sum()}\")\n",
    "display(mismatched_rows)\n",
    "\n",
    "# please note that these results are expected and not true mismatches:\n",
    "# the two \"Helgoland\" entries in Pinneberg, Schleswig-holstein were renamed with suffixes \"-1\" and \"-2\"\n",
    "# to enable joining with the geometries.\n",
    "# since there are no settlement areas in these two gemeinden, the results and maps are not affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jt2eD3J9bRYQ"
   },
   "outputs": [],
   "source": [
    "summary_85_long = summary_85_long.merge(summary_26_long[['id', 'GEM_ID']].drop_duplicates().copy(), on='id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKDD3z3s3p6d"
   },
   "source": [
    "### Please store the pre-processed files as pickle files, since you’ll need to load them directly later—especially if the session needs to be restarted due to running out of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1GYju173xGe"
   },
   "outputs": [],
   "source": [
    "summary_85_long.to_pickle(\"max_summary_85_long.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62wAb4ef7yl8"
   },
   "outputs": [],
   "source": [
    "with open(\"max_summary_85_long.pkl\", \"rb\") as f:\n",
    "    summary_85_long = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6LlQwwHbdF9"
   },
   "source": [
    "### Flooding exposure difference map (future years vs. 2020) - prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jOFOG4XbdF-"
   },
   "outputs": [],
   "source": [
    "for year in [2030, 2050, 2080]:\n",
    "    globals()[f\"summary_85_above_015_{year}\"] = summary_85_long[\n",
    "        (summary_85_long['risk_level'] == 'high_risk') &\n",
    "        (summary_85_long['Gemeinde'] != 'Germany') &\n",
    "        (summary_85_long['year'] == year)\n",
    "    ].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T9FVmjbdbdF-",
    "outputId": "5eb2527b-df1d-4138-cf29-00795ee384cd"
   },
   "outputs": [],
   "source": [
    "summary_85_above_015_2030.GEM_ID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bp8CIueXcEyB"
   },
   "outputs": [],
   "source": [
    "admin3_boundary = gpd.read_file('/content/admin-GIS/vg250_01-01.gk3.shape.ebenen/vg250_ebenen_0101/VG250_GEM.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6GDT5CUbdF-"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    summary_85_above_015_df = globals()[f'summary_85_above_015_{y}']\n",
    "\n",
    "    merged_gdf = admin3_boundary.merge(summary_85_above_015_df, left_on='OBJID', right_on='GEM_ID', how='right')\n",
    "\n",
    "    globals()[f'summary_85_above_015_{y}_gdf'] = merged_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "izy9ZF2YbdF_",
    "outputId": "23e9fde0-5530-414c-f022-5cf988552a80"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    summary_85_above_015_gdf = globals()[f'summary_85_above_015_{y}_gdf']\n",
    "    print(f\"Number of null geometries in summary_85_above_015_{y}_gdf: {len(summary_85_above_015_gdf[summary_85_above_015_gdf.geometry.isna()])}\")\n",
    "    print(f\"Number of unique geometries in summary_85_above_015_{y}_gdf: {len(summary_85_above_015_gdf.geometry.unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zRoLn2O6K3Va",
    "outputId": "7e1c7a88-0a88-40f0-84f3-2be0cf063175"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "results = {}\n",
    "\n",
    "for year in years:\n",
    "    df_name = f\"summary_85_above_015_{year}_gdf\"\n",
    "    df = globals()[df_name]   # fetch DataFrame by name\n",
    "\n",
    "    positive_sum = df.loc[df.max_diff_2020 > 0].max_diff_2020.sum().round(2)\n",
    "    negative_sum = df.loc[df.max_diff_2020 < 0].max_diff_2020.sum().round(2)\n",
    "\n",
    "    results[year] = {\"positive_sum\": positive_sum, \"negative_sum\": negative_sum}\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kk-iEdaP_Lo"
   },
   "source": [
    "### Percentage difference map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SKQ42FIX-_tm",
    "outputId": "9b5e76ab-3084-4c64-d385-2c40aa1650d3"
   },
   "outputs": [],
   "source": [
    "# Years to plot\n",
    "years = [2030, 2050, 2080]\n",
    "save_path = \"/content/drive/MyDrive/Germany_Flood_Study/Gemeinde_climate_scenario_maps/final_adjustments\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "for y in years:\n",
    "    gdf = globals()[f'summary_85_above_015_{y}_gdf'].copy()\n",
    "    gdf = gdf.to_crs(epsg=3857)\n",
    "\n",
    "    # Mask 0 values for transparency\n",
    "    gdf['masked_pct_diff'] = gdf['max_pct_diff_2020'].replace(0, np.nan)\n",
    "\n",
    "    # Colors & breaks\n",
    "    custom_colors = [\n",
    "        '#40e0d0',  # < -10%\n",
    "        '#00ffff',  # -10% ~ -1%\n",
    "        '#808080',  # -1% ~ 1%\n",
    "        '#fee5d9',  # 1% ~ 10%\n",
    "        '#fb6a4a',  # 10% ~ 50%\n",
    "        '#de2d26',  # 50% ~ 100%\n",
    "        '#a50f15'   # > 100%\n",
    "    ]\n",
    "    custom_breaks = [-np.inf, -10, -1, 1, 10, 50, 100, np.inf]\n",
    "\n",
    "    cmap = ListedColormap(custom_colors)\n",
    "    norm = BoundaryNorm(custom_breaks, ncolors=len(custom_colors), clip=False)\n",
    "\n",
    "    # Create figure and axis INSIDE loop\n",
    "    fig, ax = plt.subplots(figsize=(14, 12))\n",
    "\n",
    "    # Plot\n",
    "    gdf.plot(\n",
    "        ax=ax,\n",
    "        column='masked_pct_diff',\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        edgecolor=None,\n",
    "        linewidth=0,\n",
    "        legend=False\n",
    "    )\n",
    "\n",
    "    # Overlay geometry boundaries\n",
    "    gdf.boundary.plot(ax=ax, color='grey', linewidth=0.05)\n",
    "\n",
    "    # Basemap\n",
    "    ctx.add_basemap(ax, source=ctx.providers.CartoDB.PositronNoLabels, crs=gdf.crs)\n",
    "\n",
    "    # Cleanup\n",
    "    ax.axis('off')\n",
    "\n",
    "    # North arrow & scale bar\n",
    "    add_north_arrow(ax, scale=0.75, xlim_pos=0.9025, ylim_pos=0.9,\n",
    "                    color='black', text_scaler=4, text_yT=-1.25)\n",
    "    scale1 = ScaleBar(dx=1, location=\"lower right\", scale_loc=\"bottom\")\n",
    "    ax.add_artist(scale1)\n",
    "\n",
    "    # Custom legend\n",
    "    legend_labels = [\n",
    "        \"<-10%\", \"-10% ~ -1%\",\n",
    "        \"-1% ~ 1%\",\n",
    "        \"1% ~ 10%\", \"10% ~ 50%\", \"50% ~ 100%\", \">100%\"\n",
    "    ]\n",
    "    circles = [\n",
    "        Line2D([0], [0], marker='o', color='None',\n",
    "              markerfacecolor=custom_colors[i],\n",
    "              markeredgecolor='None',\n",
    "              markersize=10,\n",
    "              label=legend_labels[i])\n",
    "        for i in range(len(legend_labels))\n",
    "    ]\n",
    "    ax.legend(\n",
    "        handles=circles,\n",
    "        title=\"Exposure change\",\n",
    "        loc='upper left',\n",
    "        fontsize=12,\n",
    "        title_fontsize=14\n",
    "    )\n",
    "\n",
    "    # Save\n",
    "    file_name = f\"gemeinde_exposure_pct_change_high_risk_85_{y}yr_separate_dry_wet_cyansv2.png\"\n",
    "    full_path = os.path.join(save_path, file_name)\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(full_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    # plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pic3dnYp0n3r"
   },
   "source": [
    "## Coastal flooding undefended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KKG4lne0n3r"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4KbO_jGH0n3s"
   },
   "outputs": [],
   "source": [
    "# List of years to process\n",
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "# Columns you want to select (CU columns and Land)\n",
    "cu_columns = ['id', 'Gemeinde', 'Kreis', 'Land'] + [col for col in all_columns if col.startswith('CU')]\n",
    "\n",
    "# Loop through each year and modify the corresponding DataFrame\n",
    "for year in years:\n",
    "    if year == 2020:\n",
    "        df_name = f'Gemeinde_{year}_100_raw'  # 2020 uses '_100_raw'\n",
    "    else:\n",
    "        df_name = f'Gemeinde_{year}_100_85_raw'  # 2030, 2050, and 2080 use '_100_85_raw'\n",
    "\n",
    "    # Check if the DataFrame exists before proceeding\n",
    "    if df_name in globals():\n",
    "        # Get the raw DataFrame and select the relevant columns for max flooding data\n",
    "        df_raw = globals()[df_name]\n",
    "        globals()[f'Gemeinde_{year}_100_85_CU'] = df_raw[cu_columns]\n",
    "    else:\n",
    "        print(f'{df_name} does not exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3OQkcWoj0n3s"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    globals()[f'Gemeinde_{y}_100_85_CU_long'] = globals()[f'Gemeinde_{y}_100_85_CU'].melt(\n",
    "        id_vars=['id', 'Gemeinde', 'Kreis', 'Land'], var_name='month_year_depth', value_name='CU_area'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5xZ0qrpu0n3s",
    "outputId": "608d8886-1fd6-49e4-e15b-73ba769415b7"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_85_CU_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9EnnwOp0n3s"
   },
   "outputs": [],
   "source": [
    "for df in [Gemeinde_2020_100_85_CU_long, Gemeinde_2030_100_85_CU_long, Gemeinde_2050_100_85_CU_long, Gemeinde_2080_100_85_CU_long]:\n",
    "\n",
    "    # Replace 'CU' with 'MAX' if needed (optional step if not already 'MAX')\n",
    "    # df['month_year_depth'] = df['month_year_depth'].str.replace('CU', 'MAX')\n",
    "\n",
    "    # Split the 'month_year_depth' column into parts\n",
    "    df[['prefix', 'month_year', 'depth_cat']] = df['month_year_depth'].str.extract(r'^(CU)-([A-Za-z]{3}-\\d{2})-(.+)$')\n",
    "\n",
    "    # Drop 'prefix' if not needed\n",
    "    df.drop(columns='prefix', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4KKQBKsi0n3s",
    "outputId": "ec80e5a7-83a4-4db1-d368-a6223341d848"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2080_100_85_CU_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbFCtV5_0n3s"
   },
   "outputs": [],
   "source": [
    "# Define your DataFrames with their corresponding years\n",
    "year_df_map = {\n",
    "    2020: Gemeinde_2020_100_85_CU_long,\n",
    "    2030: Gemeinde_2030_100_85_CU_long,\n",
    "    2050: Gemeinde_2050_100_85_CU_long,\n",
    "    2080: Gemeinde_2080_100_85_CU_long\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame\n",
    "for year, df in year_df_map.items():\n",
    "\n",
    "    # Filter to only Jan-25\n",
    "    jan25_df = df[df['month_year'] == 'Jan-25'].reset_index(drop=True)\n",
    "\n",
    "    # Set the new global variable\n",
    "    globals()[f'Gemeinde_{year}_100_85_CU_Jan25'] = jan25_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mEdc6gws0n3t"
   },
   "outputs": [],
   "source": [
    "# Second part: group and create *_settle DataFrames\n",
    "for year in [2020, 2030, 2050, 2080]:\n",
    "    jan25_var_name = f'Gemeinde_{year}_100_85_CU_Jan25'\n",
    "\n",
    "    if jan25_var_name in globals():\n",
    "        jan25_df = globals()[jan25_var_name]\n",
    "\n",
    "        # Group and sum by 'id' and 'month_year'\n",
    "        settle_df = jan25_df.groupby(['id', 'month_year'], as_index=False)['CU_area'].sum()\n",
    "        settle_df.columns = ['id', 'month_year', 'settle_area']\n",
    "        settle_df['settle_area'] = settle_df['settle_area'].astype(float)\n",
    "\n",
    "        # Save to a new global variable\n",
    "        globals()[f'{jan25_var_name}_settle'] = settle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPZZNL_U0n3t"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    long_df = globals()[f'Gemeinde_{y}_100_85_CU_Jan25']\n",
    "    settle_df = globals()[f'Gemeinde_{y}_100_85_CU_Jan25_settle']\n",
    "\n",
    "    merged_df = pd.merge(long_df, settle_df, on=['id', 'month_year'], how='left')\n",
    "\n",
    "    globals()[f'Gemeinde_{y}_100_85_CU_Jan25'] = merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qANkYNCw0n3t"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    df = globals()[f'Gemeinde_{y}_100_85_CU_Jan25']\n",
    "    globals()[f'Gemeinde_{y}_100_85_CU_Jan25'] = df\n",
    "    df['settle_area_non_zero'] = df.settle_area.replace(0, 1e-6)\n",
    "    globals()[f'Gemeinde_{y}_100_85_CU_Jan25']['settle_area_non_zero'] = df.settle_area_non_zero\n",
    "    globals()[f'Gemeinde_{y}_100_85_CU_Jan25']['depth_cat_area_pct'] = (df.CU_area / df.settle_area_non_zero) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ygzg9T0J0n3t",
    "outputId": "72652f88-44de-454d-caf9-b37e932f1432"
   },
   "outputs": [],
   "source": [
    "cols = ['id', 'Gemeinde', 'Kreis', 'Land']\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "base_df = Gemeinde_2020_100_85_CU_Jan25\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_85_CU_Jan25']\n",
    "\n",
    "    # Direct comparison of the column values (row by row, in order)\n",
    "    is_same = base_df[cols].equals(current_df[cols])\n",
    "\n",
    "    if is_same:\n",
    "        print(f\"✅ Columns match exactly for year {year}\")\n",
    "    else:\n",
    "        print(f\"❌ Columns differ for year {year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBRr9bpt0n3t"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_85_CU_Jan25']\n",
    "    base_df = Gemeinde_2020_100_85_CU_Jan25\n",
    "    base_df[f'diff_{year}'] = current_df.CU_area - base_df.CU_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "armV1ePt0n3t"
   },
   "outputs": [],
   "source": [
    "summary = Gemeinde_2020_100_85_CU_Jan25[['id', 'Gemeinde', 'Kreis', 'Land']].drop_duplicates().copy()\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    diff_col = f'diff_{year}'\n",
    "\n",
    "    pivot = (\n",
    "        Gemeinde_2020_100_85_CU_Jan25\n",
    "        .groupby(['id', 'depth_cat'])[diff_col]\n",
    "        .sum()\n",
    "        .unstack(fill_value=0)\n",
    "        .add_prefix(f'sum_diff_{year}_')  # e.g., sum_diff_2030_lt0.5\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    summary = summary.merge(pivot, on='id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k7NWEMD60n3t"
   },
   "outputs": [],
   "source": [
    "# Convert all newly added columns to float\n",
    "new_cols = [col for col in summary.columns if col.startswith('sum_diff_')]\n",
    "summary[new_cols] = summary[new_cols].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ze9tMb-Z0n3u"
   },
   "outputs": [],
   "source": [
    "summary_85 = summary.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9PKc_OR0n3u"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    summary_85[f'high_risk_{year}'] = (\n",
    "        summary_85.get(f'sum_diff_{year}_lt0.5', 0) +\n",
    "        summary_85.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_85.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_85[f'very_high_risk_{year}'] = (\n",
    "        summary_85.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_85.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_85[f'extreme_risk_{year}'] = summary_85.get(f'sum_diff_{year}_gt1.5', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xbyvepfH0n3u",
    "outputId": "72feba59-2114-4e42-e259-4429578c12e6"
   },
   "outputs": [],
   "source": [
    "summary_85.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0EDsOcr0n3u"
   },
   "outputs": [],
   "source": [
    "value_vars = [\n",
    "    'high_risk_2030', 'very_high_risk_2030', 'extreme_risk_2030',\n",
    "    'high_risk_2050', 'very_high_risk_2050', 'extreme_risk_2050',\n",
    "    'high_risk_2080', 'very_high_risk_2080', 'extreme_risk_2080'\n",
    "]\n",
    "\n",
    "cu_summary_85_long = summary_85.melt(\n",
    "    id_vars=['id', 'Gemeinde', 'Kreis', 'Land'],  # keep these\n",
    "    value_vars=value_vars,\n",
    "    var_name='risk_year',\n",
    "    value_name='cu_diff_2020'\n",
    ")\n",
    "\n",
    "# Split the combined column into risk_level and year\n",
    "cu_summary_85_long[['risk_level', 'year']] = cu_summary_85_long['risk_year'].str.extract(r'(.*)_(\\d{4})')\n",
    "cu_summary_85_long['year'] = cu_summary_85_long['year'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vW-ejVj00n3u"
   },
   "source": [
    "### Please store the pre-processed files as pickle files, since you’ll need to load them directly later—especially if the session needs to be restarted due to running out of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BDTlFzQE0n3u"
   },
   "outputs": [],
   "source": [
    "cu_summary_85_long.to_pickle(\"cu_summary_85_long.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ue9-ITiAunj_"
   },
   "outputs": [],
   "source": [
    "with open('cu_summary_85_long.pkl', 'rb') as f:\n",
    "    cu_summary_85_long = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ldiXpqX_UxY"
   },
   "source": [
    "## Fluvial flooding undefended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdzizhHT_UxZ"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BldstaLF_UxZ"
   },
   "outputs": [],
   "source": [
    "# List of years to process\n",
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "# Columns you want to select (CU columns and Land)\n",
    "fu_columns = ['id', 'Gemeinde', 'Kreis', 'Land'] + [col for col in all_columns if col.startswith('FU')]\n",
    "\n",
    "# Loop through each year and modify the corresponding DataFrame\n",
    "for year in years:\n",
    "    if year == 2020:\n",
    "        df_name = f'Gemeinde_{year}_100_raw'  # 2020 uses '_100_raw'\n",
    "    else:\n",
    "        df_name = f'Gemeinde_{year}_100_85_raw'  # 2030, 2050, and 2080 use '_100_85_raw'\n",
    "\n",
    "    # Check if the DataFrame exists before proceeding\n",
    "    if df_name in globals():\n",
    "        # Get the raw DataFrame and select the relevant columns for max flooding data\n",
    "        df_raw = globals()[df_name]\n",
    "        globals()[f'Gemeinde_{year}_100_85_FU'] = df_raw[fu_columns]\n",
    "    else:\n",
    "        print(f'{df_name} does not exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWUj3zeV_Uxb"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    globals()[f'Gemeinde_{y}_100_85_FU_long'] = globals()[f'Gemeinde_{y}_100_85_FU'].melt(\n",
    "        id_vars=['id', 'Gemeinde', 'Kreis', 'Land'], var_name='month_year_depth', value_name='FU_area'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pa_UfdtZ_Uxd",
    "outputId": "86b1e128-10f7-4215-eafe-4c47dc6b6f30"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_85_FU_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UW2TPMPn_Uxh"
   },
   "outputs": [],
   "source": [
    "for df in [Gemeinde_2020_100_85_FU_long, Gemeinde_2030_100_85_FU_long, Gemeinde_2050_100_85_FU_long, Gemeinde_2080_100_85_FU_long]:\n",
    "\n",
    "    # Replace 'CU' with 'MAX' if needed (optional step if not already 'MAX')\n",
    "    # df['month_year_depth'] = df['month_year_depth'].str.replace('CU', 'MAX')\n",
    "\n",
    "    # Split the 'month_year_depth' column into parts\n",
    "    df[['prefix', 'month_year', 'depth_cat']] = df['month_year_depth'].str.extract(r'^(FU)-([A-Za-z]{3}-\\d{2})-(.+)$')\n",
    "\n",
    "    # Drop 'prefix' if not needed\n",
    "    df.drop(columns='prefix', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J6F2Hksg_Uxi",
    "outputId": "17bac7df-192d-411d-ac6b-7fabc305c483"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2080_100_85_FU_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GArge3W7_Uxj"
   },
   "outputs": [],
   "source": [
    "# Define your DataFrames with their corresponding years\n",
    "year_df_map = {\n",
    "    2020: Gemeinde_2020_100_85_FU_long,\n",
    "    2030: Gemeinde_2030_100_85_FU_long,\n",
    "    2050: Gemeinde_2050_100_85_FU_long,\n",
    "    2080: Gemeinde_2080_100_85_FU_long\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame\n",
    "for year, df in year_df_map.items():\n",
    "\n",
    "    # Filter to only Jan-25\n",
    "    jan25_df = df[df['month_year'] == 'Jan-25'].reset_index(drop=True)\n",
    "\n",
    "    # Set the new global variable\n",
    "    globals()[f'Gemeinde_{year}_100_85_FU_Jan25'] = jan25_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tapnIIh9_Uxn"
   },
   "outputs": [],
   "source": [
    "# Second part: group and create *_settle DataFrames\n",
    "for year in [2020, 2030, 2050, 2080]:\n",
    "    jan25_var_name = f'Gemeinde_{year}_100_85_FU_Jan25'\n",
    "\n",
    "    if jan25_var_name in globals():\n",
    "        jan25_df = globals()[jan25_var_name]\n",
    "\n",
    "        # Group and sum by 'id' and 'month_year'\n",
    "        settle_df = jan25_df.groupby(['id', 'month_year'], as_index=False)['FU_area'].sum()\n",
    "        settle_df.columns = ['id', 'month_year', 'settle_area']\n",
    "        settle_df['settle_area'] = settle_df['settle_area'].astype(float)\n",
    "\n",
    "        # Save to a new global variable\n",
    "        globals()[f'{jan25_var_name}_settle'] = settle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oWNTlC4N_Uxn"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    long_df = globals()[f'Gemeinde_{y}_100_85_FU_Jan25']\n",
    "    settle_df = globals()[f'Gemeinde_{y}_100_85_FU_Jan25_settle']\n",
    "\n",
    "    merged_df = pd.merge(long_df, settle_df, on=['id', 'month_year'], how='left')\n",
    "\n",
    "    globals()[f'Gemeinde_{y}_100_85_FU_Jan25'] = merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RHlh5ZHa_Uxo"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    df = globals()[f'Gemeinde_{y}_100_85_FU_Jan25']\n",
    "    globals()[f'Gemeinde_{y}_100_85_FU_Jan25'] = df\n",
    "    df['settle_area_non_zero'] = df.settle_area.replace(0, 1e-6)\n",
    "    globals()[f'Gemeinde_{y}_100_85_FU_Jan25']['settle_area_non_zero'] = df.settle_area_non_zero\n",
    "    globals()[f'Gemeinde_{y}_100_85_FU_Jan25']['depth_cat_area_pct'] = (df.FU_area / df.settle_area_non_zero) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPc1iTr8_Uxo",
    "outputId": "9df249e8-7223-4e34-99f9-46be496969eb"
   },
   "outputs": [],
   "source": [
    "cols = ['id', 'Gemeinde', 'Kreis', 'Land']\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "base_df = Gemeinde_2020_100_85_FU_Jan25\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_85_FU_Jan25']\n",
    "\n",
    "    # Direct comparison of the column values (row by row, in order)\n",
    "    is_same = base_df[cols].equals(current_df[cols])\n",
    "\n",
    "    if is_same:\n",
    "        print(f\"✅ Columns match exactly for year {year}\")\n",
    "    else:\n",
    "        print(f\"❌ Columns differ for year {year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zq-V566Z_Uxo"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_85_FU_Jan25']\n",
    "    base_df = Gemeinde_2020_100_85_FU_Jan25\n",
    "    base_df[f'diff_{year}'] = current_df.FU_area - base_df.FU_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXTTfWsx_Uxo"
   },
   "outputs": [],
   "source": [
    "summary = Gemeinde_2020_100_85_FU_Jan25[['id', 'Gemeinde', 'Kreis', 'Land']].drop_duplicates().copy()\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    diff_col = f'diff_{year}'\n",
    "\n",
    "    pivot = (\n",
    "        Gemeinde_2020_100_85_FU_Jan25\n",
    "        .groupby(['id', 'depth_cat'])[diff_col]\n",
    "        .sum()\n",
    "        .unstack(fill_value=0)\n",
    "        .add_prefix(f'sum_diff_{year}_')  # e.g., sum_diff_2030_lt0.5\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    summary = summary.merge(pivot, on='id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfeFXMJb_Uxp"
   },
   "outputs": [],
   "source": [
    "# Convert all newly added columns to float\n",
    "new_cols = [col for col in summary.columns if col.startswith('sum_diff_')]\n",
    "summary[new_cols] = summary[new_cols].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7tP6niU_Uxp"
   },
   "outputs": [],
   "source": [
    "summary_85 = summary.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBMSLACK_Uxp"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    summary_85[f'high_risk_{year}'] = (\n",
    "        summary_85.get(f'sum_diff_{year}_lt0.5', 0) +\n",
    "        summary_85.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_85.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_85[f'very_high_risk_{year}'] = (\n",
    "        summary_85.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_85.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_85[f'extreme_risk_{year}'] = summary_85.get(f'sum_diff_{year}_gt1.5', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PAH-LeKR_Uxp",
    "outputId": "24a992ee-02e1-458c-d807-64e6d5bb9ed6"
   },
   "outputs": [],
   "source": [
    "summary_85.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJxq3hdv_Uxp"
   },
   "outputs": [],
   "source": [
    "value_vars = [\n",
    "    'high_risk_2030', 'very_high_risk_2030', 'extreme_risk_2030',\n",
    "    'high_risk_2050', 'very_high_risk_2050', 'extreme_risk_2050',\n",
    "    'high_risk_2080', 'very_high_risk_2080', 'extreme_risk_2080'\n",
    "]\n",
    "\n",
    "fu_summary_85_long = summary_85.melt(\n",
    "    id_vars=['id', 'Gemeinde', 'Kreis', 'Land'],  # keep these\n",
    "    value_vars=value_vars,\n",
    "    var_name='risk_year',\n",
    "    value_name='fu_diff_2020'\n",
    ")\n",
    "\n",
    "# Split the combined column into risk_level and year\n",
    "fu_summary_85_long[['risk_level', 'year']] = fu_summary_85_long['risk_year'].str.extract(r'(.*)_(\\d{4})')\n",
    "fu_summary_85_long['year'] = fu_summary_85_long['year'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjhYSFAo_Uxq"
   },
   "source": [
    "### Please store the pre-processed files as pickle files, since you’ll need to load them directly later—especially if the session needs to be restarted due to running out of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKYXgVLX_Uxr"
   },
   "outputs": [],
   "source": [
    "fu_summary_85_long.to_pickle(\"fu_summary_85_long.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWqvSg2pusnn"
   },
   "outputs": [],
   "source": [
    "with open('fu_summary_85_long.pkl', 'rb') as f:\n",
    "    fu_summary_85_long = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9OixaToPaNb"
   },
   "source": [
    "## Pluvial flooding defended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZ9l2S52PaNc"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qP3DfnArPaNc"
   },
   "outputs": [],
   "source": [
    "# List of years to process\n",
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "# Columns you want to select (CU columns and Land)\n",
    "pd_columns = ['id', 'Gemeinde', 'Kreis', 'Land'] + [col for col in all_columns if col.startswith('PD')]\n",
    "\n",
    "# Loop through each year and modify the corresponding DataFrame\n",
    "for year in years:\n",
    "    if year == 2020:\n",
    "        df_name = f'Gemeinde_{year}_100_raw'  # 2020 uses '_100_raw'\n",
    "    else:\n",
    "        df_name = f'Gemeinde_{year}_100_85_raw'  # 2030, 2050, and 2080 use '_100_85_raw'\n",
    "\n",
    "    # Check if the DataFrame exists before proceeding\n",
    "    if df_name in globals():\n",
    "        # Get the raw DataFrame and select the relevant columns for max flooding data\n",
    "        df_raw = globals()[df_name]\n",
    "        globals()[f'Gemeinde_{year}_100_85_PD'] = df_raw[pd_columns]\n",
    "    else:\n",
    "        print(f'{df_name} does not exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQQpV6j9PaNc"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    globals()[f'Gemeinde_{y}_100_85_PD_long'] = globals()[f'Gemeinde_{y}_100_85_PD'].melt(\n",
    "        id_vars=['id', 'Gemeinde', 'Kreis', 'Land'], var_name='month_year_depth', value_name='PD_area'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "YUB6QeI4PaNc",
    "outputId": "b1bc7f08-5dd9-4dbc-eda8-99ee36589a0f"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2020_100_85_PD_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fsyOIqagPaNc"
   },
   "outputs": [],
   "source": [
    "for df in [Gemeinde_2020_100_85_PD_long, Gemeinde_2030_100_85_PD_long, Gemeinde_2050_100_85_PD_long, Gemeinde_2080_100_85_PD_long]:\n",
    "\n",
    "    # Replace 'CU' with 'MAX' if needed (optional step if not already 'MAX')\n",
    "    # df['month_year_depth'] = df['month_year_depth'].str.replace('CU', 'MAX')\n",
    "\n",
    "    # Split the 'month_year_depth' column into parts\n",
    "    df[['prefix', 'month_year', 'depth_cat']] = df['month_year_depth'].str.extract(r'^(PD)-([A-Za-z]{3}-\\d{2})-(.+)$')\n",
    "\n",
    "    # Drop 'prefix' if not needed\n",
    "    df.drop(columns='prefix', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6A3Ho8HUPaNc",
    "outputId": "353b3501-8795-4791-e32a-9687b941d03b"
   },
   "outputs": [],
   "source": [
    "Gemeinde_2080_100_85_PD_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FK0jezS9PaNd"
   },
   "outputs": [],
   "source": [
    "# Define your DataFrames with their corresponding years\n",
    "year_df_map = {\n",
    "    2020: Gemeinde_2020_100_85_PD_long,\n",
    "    2030: Gemeinde_2030_100_85_PD_long,\n",
    "    2050: Gemeinde_2050_100_85_PD_long,\n",
    "    2080: Gemeinde_2080_100_85_PD_long\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame\n",
    "for year, df in year_df_map.items():\n",
    "\n",
    "    # Filter to only Jan-25\n",
    "    jan25_df = df[df['month_year'] == 'Jan-25'].reset_index(drop=True)\n",
    "\n",
    "    # Set the new global variable\n",
    "    globals()[f'Gemeinde_{year}_100_85_PD_Jan25'] = jan25_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sC20BbDhPaNd"
   },
   "outputs": [],
   "source": [
    "# Second part: group and create *_settle DataFrames\n",
    "for year in [2020, 2030, 2050, 2080]:\n",
    "    jan25_var_name = f'Gemeinde_{year}_100_85_PD_Jan25'\n",
    "\n",
    "    if jan25_var_name in globals():\n",
    "        jan25_df = globals()[jan25_var_name]\n",
    "\n",
    "        # Group and sum by 'id' and 'month_year'\n",
    "        settle_df = jan25_df.groupby(['id', 'month_year'], as_index=False)['PD_area'].sum()\n",
    "        settle_df.columns = ['id', 'month_year', 'settle_area']\n",
    "        settle_df['settle_area'] = settle_df['settle_area'].astype(float)\n",
    "\n",
    "        # Save to a new global variable\n",
    "        globals()[f'{jan25_var_name}_settle'] = settle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QC4K1HWxPaNd"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    long_df = globals()[f'Gemeinde_{y}_100_85_PD_Jan25']\n",
    "    settle_df = globals()[f'Gemeinde_{y}_100_85_PD_Jan25_settle']\n",
    "\n",
    "    merged_df = pd.merge(long_df, settle_df, on=['id', 'month_year'], how='left')\n",
    "\n",
    "    globals()[f'Gemeinde_{y}_100_85_PD_Jan25'] = merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8Ga1e_XPaNd"
   },
   "outputs": [],
   "source": [
    "years = [2020, 2030, 2050, 2080]\n",
    "\n",
    "for y in years:\n",
    "    df = globals()[f'Gemeinde_{y}_100_85_PD_Jan25']\n",
    "    globals()[f'Gemeinde_{y}_100_85_PD_Jan25'] = df\n",
    "    df['settle_area_non_zero'] = df.settle_area.replace(0, 1e-6)\n",
    "    globals()[f'Gemeinde_{y}_100_85_PD_Jan25']['settle_area_non_zero'] = df.settle_area_non_zero\n",
    "    globals()[f'Gemeinde_{y}_100_85_PD_Jan25']['depth_cat_area_pct'] = (df.PD_area / df.settle_area_non_zero) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0pHw3HtPaNd",
    "outputId": "82afc3a6-6ead-4b8b-8993-a34b77e2c514"
   },
   "outputs": [],
   "source": [
    "cols = ['id', 'Gemeinde', 'Kreis', 'Land']\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "base_df = Gemeinde_2020_100_85_PD_Jan25\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_85_PD_Jan25']\n",
    "\n",
    "    # Direct comparison of the column values (row by row, in order)\n",
    "    is_same = base_df[cols].equals(current_df[cols])\n",
    "\n",
    "    if is_same:\n",
    "        print(f\"✅ Columns match exactly for year {year}\")\n",
    "    else:\n",
    "        print(f\"❌ Columns differ for year {year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xu27Hp9ePaNd"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    current_df = globals()[f'Gemeinde_{year}_100_85_PD_Jan25']\n",
    "    base_df = Gemeinde_2020_100_85_PD_Jan25\n",
    "    base_df[f'diff_{year}'] = current_df.PD_area - base_df.PD_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h05no7iGPaNd"
   },
   "outputs": [],
   "source": [
    "summary = Gemeinde_2020_100_85_PD_Jan25[['id', 'Gemeinde', 'Kreis', 'Land']].drop_duplicates().copy()\n",
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    diff_col = f'diff_{year}'\n",
    "\n",
    "    pivot = (\n",
    "        Gemeinde_2020_100_85_PD_Jan25\n",
    "        .groupby(['id', 'depth_cat'])[diff_col]\n",
    "        .sum()\n",
    "        .unstack(fill_value=0)\n",
    "        .add_prefix(f'sum_diff_{year}_')  # e.g., sum_diff_2030_lt0.5\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    summary = summary.merge(pivot, on='id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t89npekKPaNe"
   },
   "outputs": [],
   "source": [
    "# Convert all newly added columns to float\n",
    "new_cols = [col for col in summary.columns if col.startswith('sum_diff_')]\n",
    "summary[new_cols] = summary[new_cols].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBTearfEPaNe"
   },
   "outputs": [],
   "source": [
    "summary_85 = summary.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hbg6DlSDPaNe"
   },
   "outputs": [],
   "source": [
    "years = [2030, 2050, 2080]\n",
    "\n",
    "for year in years:\n",
    "    summary_85[f'high_risk_{year}'] = (\n",
    "        summary_85.get(f'sum_diff_{year}_lt0.5', 0) +\n",
    "        summary_85.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_85.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_85[f'very_high_risk_{year}'] = (\n",
    "        summary_85.get(f'sum_diff_{year}_lt1.5', 0) +\n",
    "        summary_85.get(f'sum_diff_{year}_gt1.5', 0)\n",
    "    )\n",
    "\n",
    "    summary_85[f'extreme_risk_{year}'] = summary_85.get(f'sum_diff_{year}_gt1.5', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JNsNBNFlPaNe",
    "outputId": "13480f46-6784-4d99-8598-e42ab942c697"
   },
   "outputs": [],
   "source": [
    "summary_85.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xBnTRNDPaNe"
   },
   "outputs": [],
   "source": [
    "value_vars = [\n",
    "    'high_risk_2030', 'very_high_risk_2030', 'extreme_risk_2030',\n",
    "    'high_risk_2050', 'very_high_risk_2050', 'extreme_risk_2050',\n",
    "    'high_risk_2080', 'very_high_risk_2080', 'extreme_risk_2080'\n",
    "]\n",
    "\n",
    "pd_summary_85_long = summary_85.melt(\n",
    "    id_vars=['id', 'Gemeinde', 'Kreis', 'Land'],  # keep these\n",
    "    value_vars=value_vars,\n",
    "    var_name='risk_year',\n",
    "    value_name='pd_diff_2020'\n",
    ")\n",
    "\n",
    "# Split the combined column into risk_level and year\n",
    "pd_summary_85_long[['risk_level', 'year']] = pd_summary_85_long['risk_year'].str.extract(r'(.*)_(\\d{4})')\n",
    "pd_summary_85_long['year'] = pd_summary_85_long['year'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHzMNaePPaNe"
   },
   "source": [
    "### Please store the pre-processed files as pickle files, since you’ll need to load them directly later—especially if the session needs to be restarted due to running out of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikVLA8cmPaNe"
   },
   "outputs": [],
   "source": [
    "pd_summary_85_long.to_pickle(\"pd_summary_85_long.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlRg2FR0PaNe"
   },
   "outputs": [],
   "source": [
    "with open(\"pd_summary_85_long.pkl\", \"rb\") as f:\n",
    "    pd_summary_85_long = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TE9R5vsigN1L"
   },
   "source": [
    "# Combined Gemeinden Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 905
    },
    "id": "9EbKAPbRve_R",
    "outputId": "e117b5ee-0614-4737-a3ba-d8afec49d5e1"
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "# Only the first and last scenarios\n",
    "scenarios = ['SSP1-RCP2.6', 'SSP5-RCP8.5']\n",
    "scenario_files = ['summary_26_long', 'summary_85_long']\n",
    "\n",
    "# Setup\n",
    "save_path = \"/content/drive/MyDrive/Germany_Flood_Study/Gemeinde_climate_scenario_plots/final_adjustments\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "file_name = \"separate_risk_level_future_exposure_diff_Jan25_stacked_extreme_scenarios_strip.png\"\n",
    "full_path = os.path.join(save_path, file_name)\n",
    "\n",
    "# Color and labels\n",
    "red_shades = ['#ff9999', '#ff1a1a', '#800000']\n",
    "risk_order = ['high_risk', 'very_high_risk', 'extreme_risk']\n",
    "risk_label_map = {\n",
    "    'high_risk': 'Flood depth > 0.15 m',\n",
    "    'very_high_risk': 'Flood depth > 0.5 m',\n",
    "    'extreme_risk': 'Flood depth > 1.5 m'\n",
    "}\n",
    "color_map = dict(zip(risk_order, red_shades))\n",
    "\n",
    "# 2 rows x 3 cols\n",
    "fig, axs = plt.subplots(2, 3, figsize=(18, 12), sharey=True)\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "for row_idx, (scenario, data_name) in enumerate(zip(scenarios, scenario_files)):\n",
    "    data = globals()[data_name]\n",
    "    for col_idx, risk in enumerate(risk_order):\n",
    "        ax = axs[row_idx, col_idx]\n",
    "        subset = data[(data['risk_level'] == risk) & (data['Gemeinde'] != 'Germany')]\n",
    "\n",
    "        sns.stripplot(\n",
    "            data=subset,\n",
    "            x='year',\n",
    "            y='max_diff_2020',\n",
    "            color=color_map[risk],\n",
    "            s=2,\n",
    "            alpha=0.1,\n",
    "            ax=ax,\n",
    "            zorder=-1\n",
    "        )\n",
    "\n",
    "        # Percentiles\n",
    "        grouped = subset.groupby('year')['max_diff_2020']\n",
    "        for rp, group in grouped:\n",
    "            percentiles = {\n",
    "                '50': group.median(),\n",
    "                '75': group.quantile(0.75),\n",
    "                '90': group.quantile(0.90)\n",
    "            }\n",
    "            xpos = list(subset['year'].unique()).index(rp)\n",
    "            ax.hlines(percentiles['50'], xpos - 0.15, xpos + 0.15, color='black', linewidth=1.5, linestyles='solid')\n",
    "            ax.hlines(percentiles['75'], xpos - 0.2, xpos + 0.2, color='black', linewidth=1.2, linestyles='dashed')\n",
    "            ax.hlines(percentiles['90'], xpos - 0.3, xpos + 0.3, color='black', linewidth=1, linestyles='dotted')\n",
    "\n",
    "        # Titles & Labels\n",
    "        if row_idx == 0:\n",
    "            ax.set_title(risk_label_map[risk], fontsize=16)\n",
    "        if col_idx == 0:\n",
    "            ax.set_ylabel(f\"{scenario}\\n\\nFlood exposure change (km$^2$)\", fontsize=16)\n",
    "        else:\n",
    "            ax.set_ylabel(\"\")\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylim(-0.01, 0.05)\n",
    "        ax.tick_params(axis='x', rotation=45, labelsize=14)\n",
    "        ax.tick_params(axis='y', labelsize=14)\n",
    "\n",
    "# Custom legend at bottom middle\n",
    "legend_lines = [\n",
    "    Line2D([0], [0], color='black', linewidth=1.5, linestyle='solid', label='50th percentile'),\n",
    "    Line2D([0], [0], color='black', linewidth=1.2, linestyle='dashed', label='75th percentile'),\n",
    "    Line2D([0], [0], color='black', linewidth=1, linestyle='dotted', label='90th percentile')\n",
    "]\n",
    "\n",
    "fig.legend(\n",
    "    handles=legend_lines,\n",
    "    # title='Percentile',\n",
    "    loc='lower center',\n",
    "    bbox_to_anchor=(0.5, -0.05),\n",
    "    ncol=3,\n",
    "    fontsize=16,\n",
    "    title_fontsize=18\n",
    ")\n",
    "\n",
    "# fig.supxlabel(\"Projected Years\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.12)  # make space for the legend\n",
    "\n",
    "# plt.savefig(full_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 905
    },
    "id": "49J1HPh8wbnL",
    "outputId": "917ccd0f-7c16-4e56-cdbe-96c06b0ab3a1"
   },
   "outputs": [],
   "source": [
    "# Only take the first and last scenario\n",
    "scenarios_subset = ['SSP1-RCP2.6', 'SSP5-RCP8.5']\n",
    "scenario_files_subset = ['summary_26_long', 'summary_85_long']\n",
    "\n",
    "# Setup\n",
    "save_path = \"/content/drive/MyDrive/Germany_Flood_Study/Gemeinde_climate_scenario_plots/final_adjustments\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "file_name = \"separate_risk_level_future_exposure_pct_diff_Jan25_stacked_extreme_scenarios_strip.png\"\n",
    "full_path = os.path.join(save_path, file_name)\n",
    "\n",
    "# Color and labels\n",
    "red_shades = ['#ff9999', '#ff1a1a', '#800000']\n",
    "risk_order = ['high_risk', 'very_high_risk', 'extreme_risk']\n",
    "risk_label_map = {\n",
    "    'high_risk': 'Flood depth > 0.15 m',\n",
    "    'very_high_risk': 'Flood depth > 0.5 m',\n",
    "    'extreme_risk': 'Flood depth > 1.5 m'\n",
    "}\n",
    "color_map = dict(zip(risk_order, red_shades))\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(18, 12), sharey=True)  # 2 rows now\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "for row_idx, (scenario, data_name) in enumerate(zip(scenarios_subset, scenario_files_subset)):\n",
    "    data = globals()[data_name]\n",
    "    for col_idx, risk in enumerate(risk_order):\n",
    "        ax = axs[row_idx, col_idx]\n",
    "        subset = data[\n",
    "            (data['risk_level'] == risk) &\n",
    "            (data['Gemeinde'] != 'Germany')\n",
    "        ]\n",
    "\n",
    "        sns.stripplot(\n",
    "            data=subset,\n",
    "            x='year',\n",
    "            y='max_pct_diff_2020',\n",
    "            color=color_map[risk],\n",
    "            s=2,\n",
    "            alpha=0.1,\n",
    "            ax=ax,\n",
    "            zorder=-1\n",
    "        )\n",
    "\n",
    "        # Draw percentiles\n",
    "        grouped = subset.groupby('year')['max_pct_diff_2020']\n",
    "        for rp, group in grouped:\n",
    "            percentiles = {\n",
    "                '50': group.median(),\n",
    "                '75': group.quantile(0.75),\n",
    "                '90': group.quantile(0.90)\n",
    "            }\n",
    "            xpos = list(subset['year'].unique()).index(rp)\n",
    "            ax.hlines(percentiles['50'], xpos - 0.15, xpos + 0.15, color='black', linewidth=1.5, linestyles='solid')\n",
    "            ax.hlines(percentiles['75'], xpos - 0.2, xpos + 0.2, color='black', linewidth=1.2, linestyles='dashed')\n",
    "            ax.hlines(percentiles['90'], xpos - 0.3, xpos + 0.3, color='black', linewidth=1, linestyles='dotted')\n",
    "\n",
    "        # Titles & Labels\n",
    "        if row_idx == 0:\n",
    "            ax.set_title(risk_label_map[risk], fontsize=16)\n",
    "        if col_idx == 0:\n",
    "            ax.set_ylabel(f\"{scenario}\\n\\nFlood exposure change (%)\", fontsize=14)\n",
    "        else:\n",
    "            ax.set_ylabel(\"\")\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylim(-25, 175)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Legend at bottom middle\n",
    "legend_lines = [\n",
    "    Line2D([0], [0], color='black', linewidth=1.5, linestyle='solid', label='50th percentile'),\n",
    "    Line2D([0], [0], color='black', linewidth=1.2, linestyle='dashed', label='75th percentile'),\n",
    "    Line2D([0], [0], color='black', linewidth=1, linestyle='dotted', label='90th percentile')\n",
    "]\n",
    "\n",
    "fig.legend(\n",
    "    handles=legend_lines,\n",
    "    # title='Percentile',\n",
    "    loc='lower center',\n",
    "    bbox_to_anchor=(0.5, -0.05),\n",
    "    ncol=3,\n",
    "    fontsize=16,\n",
    "    title_fontsize=18\n",
    ")\n",
    "\n",
    "# fig.suptitle(\"Max Flood Exposure %Change by Depth Across Climate Scenarios (Relative to 2020, Jan-25)\", fontsize=20, y=1.02)\n",
    "# fig.supxlabel(\"Projected Years\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(full_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6Eg3u6wSY5e"
   },
   "source": [
    "# National Level Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "id": "AOjlT6yGx0Zs",
    "outputId": "7d91b7a2-2da6-43cb-ce90-3807586f31db"
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "save_path = \"/content/drive/MyDrive/Germany_Flood_Study/Gemeinde_climate_scenario_plots/final_adjustments\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "file_name = \"separate_risk_level_future_exposure_diff_Jan25_Germany.png\"\n",
    "full_path = os.path.join(save_path, file_name)\n",
    "\n",
    "# Scenarios and corresponding dataframes\n",
    "scenarios = ['26', '45', '70', '85']\n",
    "scenario_titles = {\n",
    "    '26': 'SSP1-RCP2.6',\n",
    "    '45': 'SSP2-RCP4.5',\n",
    "    '70': 'SSP3-RCP7.0',\n",
    "    '85': 'SSP5-RCP8.5'\n",
    "}\n",
    "\n",
    "# Risk levels and colors\n",
    "risk_order = ['high_risk', 'very_high_risk', 'extreme_risk']\n",
    "risk_label_map = {\n",
    "    'high_risk': '> 0.15 m',\n",
    "    'very_high_risk': '> 0.5 m',\n",
    "    'extreme_risk': '> 1.5 m'\n",
    "}\n",
    "red_shades = ['#ff9999', '#ff1a1a', '#800000']\n",
    "color_map = dict(zip(risk_order, red_shades))\n",
    "\n",
    "# Create 1x4 subplots\n",
    "fig, axs = plt.subplots(1, 4, figsize=(22, 6), sharey=True)\n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    df = globals()[f'summary_{scenario}_long']\n",
    "    df_germany = df[df['Gemeinde'] == 'Germany'].copy()\n",
    "\n",
    "    # Add a 2020 zero point for each risk level\n",
    "    zero_df = pd.DataFrame({\n",
    "        'year': [2020] * len(risk_order),\n",
    "        'max_diff_2020': [0] * len(risk_order),\n",
    "        'risk_level': risk_order,\n",
    "        'Gemeinde': ['Germany'] * len(risk_order)\n",
    "    })\n",
    "\n",
    "    df_germany = pd.concat([zero_df, df_germany], ignore_index=True)\n",
    "\n",
    "    sns.lineplot(\n",
    "        data=df_germany,\n",
    "        x='year',\n",
    "        y='max_diff_2020',\n",
    "        hue='risk_level',\n",
    "        hue_order=risk_order,\n",
    "        palette=color_map,\n",
    "        marker='o',\n",
    "        ax=axs[i]\n",
    "    )\n",
    "\n",
    "    axs[i].set_title(scenario_titles[scenario], fontsize=16)\n",
    "    axs[i].set_xlabel('Year', fontsize=14)\n",
    "    axs[i].set_xticks([2020, 2030, 2050, 2080])\n",
    "    axs[i].set_ylim(-10, 240)\n",
    "    axs[i].grid(True)\n",
    "\n",
    "    if i == 0:\n",
    "        axs[i].set_ylabel('Flood exposure increase (km$^2$)', fontsize=16, rotation=90)\n",
    "    else:\n",
    "        axs[i].set_ylabel('')\n",
    "\n",
    "    axs[i].get_legend().remove()\n",
    "\n",
    "# Create a single legend from the first axis\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "new_labels = [risk_label_map.get(label, label) for label in labels]\n",
    "\n",
    "# Add legend at bottom center\n",
    "fig.legend(\n",
    "    handles,\n",
    "    new_labels,\n",
    "    title='Flood depth',\n",
    "    loc='lower center',\n",
    "    bbox_to_anchor=(0.5, -0.08),\n",
    "    ncol=len(risk_order),\n",
    "    fontsize=16,\n",
    "    title_fontsize=18\n",
    ")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95])  # leave space for bottom legend\n",
    "\n",
    "# plt.savefig(full_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "EPF24haayfQr",
    "outputId": "d7680c0a-60e6-4083-d4f0-282754b826da"
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "save_path = \"/content/drive/MyDrive/Germany_Flood_Study/Gemeinde_climate_scenario_plots/final_adjustments\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "file_name = \"separate_risk_level_future_cu_exposure_diff_Jan25_Germany.png\"\n",
    "full_path = os.path.join(save_path, file_name)\n",
    "\n",
    "# Scenarios and corresponding dataframes\n",
    "scenarios = ['26', '45', '70', '85']\n",
    "scenario_titles = {\n",
    "    '26': 'SSP1-RCP2.6',\n",
    "    '45': 'SSP2-RCP4.5',\n",
    "    '70': 'SSP3-RCP7.0',\n",
    "    '85': 'SSP5-RCP8.5'\n",
    "}\n",
    "\n",
    "# Risk levels and colors\n",
    "risk_order = ['high_risk', 'very_high_risk', 'extreme_risk']\n",
    "risk_label_map = {\n",
    "    'high_risk': '> 0.15 m',\n",
    "    'very_high_risk': '> 0.5 m',\n",
    "    'extreme_risk': '> 1.5 m'\n",
    "}\n",
    "blue_shades = ['#99ccff', '#1a75ff', '#003366']\n",
    "color_map = dict(zip(risk_order, blue_shades))\n",
    "\n",
    "# Create 1x4 subplots\n",
    "fig, axs = plt.subplots(1, 4, figsize=(22, 6), sharey=True)\n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    df = globals()[f'cu_summary_{scenario}_long']\n",
    "    df_germany = df[df['Gemeinde'] == 'Germany'].copy()\n",
    "\n",
    "    # Add a 2020 zero point for each risk level\n",
    "    zero_df = pd.DataFrame({\n",
    "        'year': [2020] * len(risk_order),\n",
    "        'cu_diff_2020': [0] * len(risk_order),\n",
    "        'risk_level': risk_order,\n",
    "        'Gemeinde': ['Germany'] * len(risk_order)\n",
    "    })\n",
    "\n",
    "    # Combine with the original data\n",
    "    df_germany = pd.concat([zero_df, df_germany], ignore_index=True)\n",
    "\n",
    "    sns.lineplot(\n",
    "        data=df_germany,\n",
    "        x='year',\n",
    "        y='cu_diff_2020',\n",
    "        hue='risk_level',\n",
    "        hue_order=risk_order,\n",
    "        palette=color_map,\n",
    "        marker='o',\n",
    "        ax=axs[i]\n",
    "    )\n",
    "\n",
    "    axs[i].set_title(scenario_titles[scenario])\n",
    "    axs[i].set_xlabel('Year', fontsize=14)\n",
    "    axs[i].set_xticks([2020, 2030, 2050, 2080])\n",
    "    # axs[i].set_ylim(-10, 240)\n",
    "    axs[i].grid(True)\n",
    "\n",
    "    if i == 0:\n",
    "        axs[i].set_ylabel('Flood exposure increase (km$^2$)', fontsize=16, rotation=90)\n",
    "    else:\n",
    "        axs[i].set_ylabel('')\n",
    "\n",
    "    axs[i].get_legend().remove()\n",
    "\n",
    "# Create a single legend from the first axis\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "# Map the risk levels to custom labels\n",
    "new_labels = [risk_label_map.get(label, label) for label in labels]\n",
    "\n",
    "# Add legend outside the plot\n",
    "fig.legend(\n",
    "    handles,\n",
    "    new_labels,\n",
    "    title='Flood depth',\n",
    "    loc='lower center',\n",
    "    bbox_to_anchor=(0.5, -0.12),\n",
    "    ncol=len(risk_order),\n",
    "    fontsize=16,\n",
    "    title_fontsize=18  # smaller x value = closer to the plot\n",
    ")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "\n",
    "# Save or show the plot\n",
    "# plt.savefig(full_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "NgMhHsplzAgy",
    "outputId": "6d4ac750-12af-41e3-a610-c4fe58209640"
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "save_path = \"/content/drive/MyDrive/Germany_Flood_Study/Gemeinde_climate_scenario_plots/final_adjustments\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "file_name = \"separate_risk_level_future_fu_exposure_diff_Jan25_Germany.png\"\n",
    "full_path = os.path.join(save_path, file_name)\n",
    "\n",
    "# Scenarios and corresponding dataframes\n",
    "scenarios = ['26', '45', '70', '85']\n",
    "scenario_titles = {\n",
    "    '26': 'SSP1-RCP2.6',\n",
    "    '45': 'SSP2-RCP4.5',\n",
    "    '70': 'SSP3-RCP7.0',\n",
    "    '85': 'SSP5-RCP8.5'\n",
    "}\n",
    "\n",
    "# Risk levels and colors\n",
    "risk_order = ['high_risk', 'very_high_risk', 'extreme_risk']\n",
    "risk_label_map = {\n",
    "    'high_risk': '> 0.15 m',\n",
    "    'very_high_risk': '> 0.5 m',\n",
    "    'extreme_risk': '> 1.5 m'\n",
    "}\n",
    "green_shades = ['#a8e6a3', '#4caf50', '#1b5e20']\n",
    "color_map = dict(zip(risk_order, green_shades))\n",
    "\n",
    "# Create 1x4 subplots\n",
    "fig, axs = plt.subplots(1, 4, figsize=(22, 6), sharey=True)\n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    df = globals()[f'fu_summary_{scenario}_long']\n",
    "    df_germany = df[df['Gemeinde'] == 'Germany'].copy()\n",
    "\n",
    "    # Add a 2020 zero point for each risk level\n",
    "    zero_df = pd.DataFrame({\n",
    "        'year': [2020] * len(risk_order),\n",
    "        'fu_diff_2020': [0] * len(risk_order),\n",
    "        'risk_level': risk_order,\n",
    "        'Gemeinde': ['Germany'] * len(risk_order)\n",
    "    })\n",
    "\n",
    "    # Combine with the original data\n",
    "    df_germany = pd.concat([zero_df, df_germany], ignore_index=True)\n",
    "\n",
    "    sns.lineplot(\n",
    "        data=df_germany,\n",
    "        x='year',\n",
    "        y='fu_diff_2020',\n",
    "        hue='risk_level',\n",
    "        hue_order=risk_order,\n",
    "        palette=color_map,\n",
    "        marker='o',\n",
    "        ax=axs[i]\n",
    "    )\n",
    "\n",
    "    axs[i].set_title(scenario_titles[scenario])\n",
    "    axs[i].set_xlabel('Year', fontsize=14)\n",
    "    axs[i].set_xticks([2020, 2030, 2050, 2080])\n",
    "    # axs[i].set_ylim(-10, 240)\n",
    "    axs[i].grid(True)\n",
    "\n",
    "    if i == 0:\n",
    "        axs[i].set_ylabel('Flood exposure increase (km$^2$)', fontsize=16, rotation=90)\n",
    "    else:\n",
    "        axs[i].set_ylabel('')\n",
    "\n",
    "    axs[i].get_legend().remove()\n",
    "\n",
    "# Create a single legend from the first axis\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "# Map the risk levels to custom labels\n",
    "new_labels = [risk_label_map.get(label, label) for label in labels]\n",
    "\n",
    "# Add legend outside the plot\n",
    "fig.legend(\n",
    "    handles,\n",
    "    new_labels,\n",
    "    title='Flood depth',\n",
    "    loc='lower center',\n",
    "    bbox_to_anchor=(0.5, -0.12),\n",
    "    ncol=len(risk_order),\n",
    "    fontsize=16,\n",
    "    title_fontsize=18\n",
    ")\n",
    "\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "\n",
    "# Save or show the plot\n",
    "# plt.savefig(full_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "sAf5tLoa0l4c",
    "outputId": "bdaf0230-8b77-4f87-d983-5e61d81636a2"
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "save_path = \"/content/drive/MyDrive/Germany_Flood_Study/Gemeinde_climate_scenario_plots/final_adjustments\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "file_name = \"separate_risk_level_future_pd_exposure_diff_Jan25_Germany.png\"\n",
    "full_path = os.path.join(save_path, file_name)\n",
    "\n",
    "# Scenarios and corresponding dataframes\n",
    "scenarios = ['26', '45', '70', '85']\n",
    "scenario_titles = {\n",
    "    '26': 'SSP1-RCP2.6',\n",
    "    '45': 'SSP2-RCP4.5',\n",
    "    '70': 'SSP3-RCP7.0',\n",
    "    '85': 'SSP5-RCP8.5'\n",
    "}\n",
    "\n",
    "# Risk levels and colors\n",
    "risk_order = ['high_risk', 'very_high_risk', 'extreme_risk']\n",
    "risk_label_map = {\n",
    "    'high_risk': '> 0.15 m',\n",
    "    'very_high_risk': '> 0.5 m',\n",
    "    'extreme_risk': '> 1.5 m'\n",
    "}\n",
    "purple_shades = ['#d1b3ff', '#9c27b0', '#4a0072']\n",
    "color_map = dict(zip(risk_order, purple_shades))\n",
    "\n",
    "# Create 1x4 subplots\n",
    "fig, axs = plt.subplots(1, 4, figsize=(22, 6), sharey=True)\n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    df = globals()[f'pd_summary_{scenario}_long']\n",
    "    df_germany = df[df['Gemeinde'] == 'Germany'].copy()\n",
    "\n",
    "    # Add a 2020 zero point for each risk level\n",
    "    zero_df = pd.DataFrame({\n",
    "        'year': [2020] * len(risk_order),\n",
    "        'pd_diff_2020': [0] * len(risk_order),\n",
    "        'risk_level': risk_order,\n",
    "        'Gemeinde': ['Germany'] * len(risk_order)\n",
    "    })\n",
    "\n",
    "    # Combine with the original data\n",
    "    df_germany = pd.concat([zero_df, df_germany], ignore_index=True)\n",
    "\n",
    "    sns.lineplot(\n",
    "        data=df_germany,\n",
    "        x='year',\n",
    "        y='pd_diff_2020',\n",
    "        hue='risk_level',\n",
    "        hue_order=risk_order,\n",
    "        palette=color_map,\n",
    "        marker='o',\n",
    "        ax=axs[i]\n",
    "    )\n",
    "\n",
    "    axs[i].set_title(scenario_titles[scenario])\n",
    "    axs[i].set_xlabel('Year', fontsize=14)\n",
    "    axs[i].set_xticks([2020, 2030, 2050, 2080])\n",
    "    # axs[i].set_ylim(-10, 240)\n",
    "    axs[i].grid(True)\n",
    "\n",
    "    if i == 0:\n",
    "        axs[i].set_ylabel('Flood exposure increase (km$^2$)', fontsize=16, rotation=90)\n",
    "    else:\n",
    "        axs[i].set_ylabel('')\n",
    "\n",
    "    axs[i].get_legend().remove()\n",
    "\n",
    "# Create a single legend from the first axis\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "# Map the risk levels to custom labels\n",
    "new_labels = [risk_label_map.get(label, label) for label in labels]\n",
    "\n",
    "# Add legend outside the plot\n",
    "fig.legend(\n",
    "    handles,\n",
    "    new_labels,\n",
    "    title='Flood depth',\n",
    "    loc='lower center',\n",
    "    bbox_to_anchor=(0.5, -0.12),\n",
    "    ncol=len(risk_order),\n",
    "    fontsize=16,\n",
    "    title_fontsize=18\n",
    ")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "# Save or show the plot\n",
    "# plt.savefig(full_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "d_nOA6O2-ZZP",
    "j2PYbfOmZWlF",
    "qOxKGiWjh_LL",
    "6BGGkcn0gitE",
    "CU_AriicnP7A",
    "vyDSIs3cGNqh",
    "VXIRYIyThIRh",
    "uTdHi3_UzmP4",
    "ti4KlZuDKPEz",
    "bBkuzrNfa5wC",
    "JDkskPana5wH",
    "uzpXLsfd4ZCr",
    "tKZOyBui4ZCt",
    "ves2q3poJYhK",
    "nPCghViuJYhM",
    "SFp-9TuhaFFN",
    "CPqU6G9tZ8Fb",
    "kbAGzYuIvTix",
    "-MwBYKUAYnV3",
    "1WaewbysWgZC",
    "GDsMwRfxK7CW",
    "WuUsHaYpf-f4",
    "BU535nyof-f7",
    "YzasnAYf6oQS",
    "E6APOSD0MaoY",
    "z6HfgN4baJxt",
    "lvIAoHXCaOJR",
    "KbRgplAJ4qoL",
    "Hu0rK9fPYwvC",
    "VerCJXrHZawU",
    "mrhXMbQtPOx8",
    "607Cn7NSuWXH",
    "pYzX9flw9H3U",
    "ugbn-lazN8VR",
    "3GByCfhuqSmz",
    "QHzfrN7hqOvu",
    "-6nnauaP0vFn",
    "-KKl-l4JbRYQ",
    "4kk-iEdaP_Lo",
    "4KKG4lne0n3r",
    "HdzizhHT_UxZ",
    "gZ9l2S52PaNc",
    "TE9R5vsigN1L"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
